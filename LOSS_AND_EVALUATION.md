# DeepCFR æŸå¤±è®¡ç®—ä¸è®­ç»ƒæ•ˆæœè¯„ä¼°

## ğŸ“Š åŸå§‹é¡¹ç›®çš„æŸå¤±è®¡ç®—æ–¹å¼

### 1. ä¼˜åŠ¿ç½‘ç»œæŸå¤±ï¼ˆAdvantage Network Lossï¼‰

```python
# ä»£ç ä½ç½®ï¼šopen_spiel/python/pytorch/deep_cfr.py:493-494
iters = sqrt(iteration)  # è¿­ä»£æ¬¡æ•°çš„å¹³æ–¹æ ¹
loss_advantages = MSE(iters * outputs, iters * advantages)
```

**å…¬å¼**ï¼š
```
loss = MSE(âˆšt * network_output, âˆšt * target_advantage)
```

**è¯´æ˜**ï¼š
- `iters = sqrt(iteration)`ï¼šéšç€è¿­ä»£å¢åŠ ï¼Œæƒé‡ä¼šå¢å¤§
- `outputs`ï¼šç½‘ç»œé¢„æµ‹çš„ä¼˜åŠ¿å€¼
- `advantages`ï¼šå®é™…è®¡ç®—çš„ä¼˜åŠ¿å€¼ï¼ˆregretï¼‰
- ä½¿ç”¨ `iters` åŠ æƒæ˜¯ä¸ºäº†ç»™åæœŸè¿­ä»£æ›´å¤§çš„æƒé‡

**ä¸ºä»€ä¹ˆæŸå¤±ä¼šå¢é•¿ï¼Ÿ**
- `iters` ä¼šéšç€è¿­ä»£å¢åŠ è€Œå¢å¤§ï¼ˆâˆš1=1, âˆš100=10ï¼‰
- å³ä½¿ç½‘ç»œé¢„æµ‹å’Œç›®æ ‡çš„ç›¸å¯¹è¯¯å·®ä¸å˜ï¼ŒåŠ æƒåçš„æŸå¤±ä¹Ÿä¼šå¢é•¿
- **è¿™æ˜¯æ­£å¸¸çš„ï¼Œä¸æ˜¯é—®é¢˜ï¼**

### 2. ç­–ç•¥ç½‘ç»œæŸå¤±ï¼ˆStrategy Network Lossï¼‰

```python
# ä»£ç ä½ç½®ï¼šopen_spiel/python/pytorch/deep_cfr.py:527
iters = sqrt(iteration)
loss_strategy = MSE(iters * outputs, iters * target_probs)
```

**å…¬å¼**ï¼š
```
loss = MSE(âˆšt * network_output_probs, âˆšt * target_probs)
```

**è¯´æ˜**ï¼š
- `outputs`ï¼šç½‘ç»œé¢„æµ‹çš„åŠ¨ä½œæ¦‚ç‡
- `target_probs`ï¼šä»éå†ä¸­æ”¶é›†çš„å¹³å‡ç­–ç•¥æ¦‚ç‡
- åŒæ ·ä½¿ç”¨ `iters` åŠ æƒ

## ğŸ¯ å¦‚ä½•åˆ¤æ–­è®­ç»ƒæ•ˆæœ

### æ–¹æ³•1: ç­–ç•¥ç†µï¼ˆPolicy Entropyï¼‰

**å«ä¹‰**ï¼šè¡¡é‡ç­–ç•¥çš„éšæœºæ€§
- **é«˜ç†µ**ï¼šç­–ç•¥æ›´éšæœºï¼Œæ¢ç´¢æ›´å¤š
- **ä½ç†µ**ï¼šç­–ç•¥æ›´ç¡®å®šï¼Œæ”¶æ•›åˆ°ç‰¹å®šåŠ¨ä½œ
- **æ­£å¸¸è¶‹åŠ¿**ï¼šè®­ç»ƒåˆæœŸç†µè¾ƒé«˜ï¼ŒåæœŸé€æ¸é™ä½

**ä»£ç **ï¼š
```python
from training_evaluator import quick_evaluate

eval_result = quick_evaluate(game, deep_cfr_solver)
entropy = eval_result['metrics']['avg_entropy']
print(f"ç­–ç•¥ç†µ: {entropy:.4f}")
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- ç†µåœ¨åˆç†èŒƒå›´å†…ï¼ˆé€šå¸¸0.5-2.0ï¼‰
- ç†µé€æ¸é™ä½è¯´æ˜ç­–ç•¥åœ¨æ”¶æ•›
- ç†µè¿‡ä½ï¼ˆ<0.1ï¼‰å¯èƒ½è¿‡æ‹Ÿåˆ

### æ–¹æ³•2: ç¼“å†²åŒºå¤§å°ï¼ˆBuffer Sizeï¼‰

**å«ä¹‰**ï¼šæ¢ç´¢äº†å¤šå°‘ä¸åŒçš„ä¿¡æ¯çŠ¶æ€
- **ç­–ç•¥ç¼“å†²åŒº**ï¼šæ”¶é›†äº†å¤šå°‘ç­–ç•¥æ ·æœ¬
- **ä¼˜åŠ¿ç¼“å†²åŒº**ï¼šæ”¶é›†äº†å¤šå°‘ä¼˜åŠ¿æ ·æœ¬
- **æ­£å¸¸è¶‹åŠ¿**ï¼šéšç€è®­ç»ƒè¿›è¡Œï¼Œç¼“å†²åŒºé€æ¸å¢å¤§

**ä»£ç **ï¼š
```python
strategy_buffer_size = len(deep_cfr_solver.strategy_buffer)
advantage_buffer_size = len(deep_cfr_solver.advantage_buffers[0])
print(f"ç­–ç•¥ç¼“å†²åŒº: {strategy_buffer_size:,}")
print(f"ä¼˜åŠ¿ç¼“å†²åŒº: {advantage_buffer_size:,}")
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- ç¼“å†²åŒºåº”è¯¥æŒç»­å¢é•¿
- å¦‚æœç¼“å†²åŒºå¢é•¿å¾ˆæ…¢ï¼Œå¯èƒ½æ¢ç´¢ä¸è¶³
- å¦‚æœç¼“å†²åŒºæ»¡äº†ï¼Œå¯èƒ½éœ€è¦å¢åŠ å®¹é‡

### æ–¹æ³•3: æµ‹è¯•å¯¹å±€ï¼ˆTest Gamesï¼‰

**å«ä¹‰**ï¼šä¸éšæœºç­–ç•¥å¯¹å±€ï¼Œçœ‹å®é™…è¡¨ç°
- **å¹³å‡æ”¶ç›Š**ï¼šç©å®¶0çš„å¹³å‡æ”¶ç›Š
- **èƒœç‡**ï¼šç©å®¶0çš„èƒœç‡
- **æ­£å¸¸è¶‹åŠ¿**ï¼šéšç€è®­ç»ƒè¿›è¡Œï¼Œæ”¶ç›Šå’Œèƒœç‡åº”è¯¥æé«˜

**ä»£ç **ï¼š
```python
eval_result = quick_evaluate(
    game, 
    deep_cfr_solver,
    include_test_games=True,
    num_test_games=100
)
test = eval_result['test_results']
print(f"å¹³å‡æ”¶ç›Š: {test['player0_avg_return']:.4f}")
print(f"èƒœç‡: {test['player0_win_rate']*100:.1f}%")
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- æ”¶ç›Šåº”è¯¥é€æ¸æé«˜ï¼ˆä»è´Ÿå€¼åˆ°æ­£å€¼ï¼‰
- èƒœç‡åº”è¯¥è¶…è¿‡50%ï¼ˆå¯¹éšæœºç­–ç•¥ï¼‰
- å¦‚æœæ”¶ç›Šå’Œèƒœç‡ä¸æé«˜ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°

### æ–¹æ³•4: æŸå¤±è¶‹åŠ¿ï¼ˆLoss Trendï¼‰

**æ³¨æ„**ï¼šæŸå¤±å€¼æœ¬èº«ä¸èƒ½ç›´æ¥åˆ¤æ–­è®­ç»ƒæ•ˆæœï¼

**åŸå› **ï¼š
- æŸå¤±ä½¿ç”¨äº† `sqrt(iteration)` åŠ æƒï¼Œä¼šè‡ªç„¶å¢é•¿
- æ›´å…³æ³¨æŸå¤±çš„**ç›¸å¯¹å˜åŒ–**ï¼Œè€Œä¸æ˜¯ç»å¯¹å€¼

**å¦‚ä½•çœ‹æŸå¤±è¶‹åŠ¿**ï¼š
```python
# æŸ¥çœ‹æŸå¤±çš„å˜åŒ–ç‡
losses = advantage_losses[0]
if len(losses) > 10:
    recent_avg = np.mean(losses[-10:])
    earlier_avg = np.mean(losses[-20:-10])
    trend = (recent_avg - earlier_avg) / earlier_avg
    print(f"æŸå¤±è¶‹åŠ¿: {trend*100:.2f}%")
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- æŸå¤±å¢é•¿æ˜¯æ­£å¸¸çš„ï¼ˆå› ä¸ºåŠ æƒï¼‰
- æ›´å…³æ³¨ç­–ç•¥ç†µã€ç¼“å†²åŒºå¤§å°ã€æµ‹è¯•å¯¹å±€ç»“æœ

### æ–¹æ³•5: NashConvï¼ˆæœ€å‡†ç¡®ï¼Œä½†è€—æ—¶ï¼‰

**å«ä¹‰**ï¼šè·ç¦»çº³ä»€å‡è¡¡çš„è·ç¦»
- **å€¼è¶Šå°è¶Šå¥½**ï¼š0è¡¨ç¤ºè¾¾åˆ°çº³ä»€å‡è¡¡
- **è®¡ç®—è€—æ—¶**ï¼šå¯¹äºå¤§è§„æ¨¡æ¸¸æˆå¯èƒ½å¾ˆæ…¢

**ä»£ç **ï¼š
```python
# éœ€è¦ä¿®å¤è¯„ä¼°ä»£ç åæ‰èƒ½ä½¿ç”¨
conv = pyspiel.nash_conv(game, policy)
print(f"NashConv: {conv:.6f}")
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- NashConv < 1.0ï¼šç­–ç•¥å¾ˆå¥½
- NashConv < 0.1ï¼šç­–ç•¥éå¸¸å¥½
- NashConv = 0ï¼šè¾¾åˆ°çº³ä»€å‡è¡¡

## ğŸ”§ ä¿®å¤è¯„ä¼°ä»£ç 

**å·²ä¿®å¤**ï¼šè¯„ä¼°ä»£ç çš„é€»è¾‘é”™è¯¯å·²ä¿®å¤ã€‚ç°åœ¨è¯„ä¼°ä¼šåœ¨æ¯æ¬¡è¿­ä»£æ—¶è‡ªåŠ¨è¿è¡Œï¼ˆå¦‚æœ `training_evaluator` æ¨¡å—å¯ç”¨ï¼‰ï¼Œä¸å†ä¾èµ– `skip_nashconv` å‚æ•°ã€‚

**ä¿®å¤å†…å®¹**ï¼š
- ç§»é™¤äº† `if skip_nashconv:` æ¡ä»¶åˆ¤æ–­
- è¯„ä¼°ç°åœ¨æ€»æ˜¯è¿è¡Œï¼ˆå¦‚æœæ¨¡å—å¯ç”¨ï¼‰
- NashConv è®¡ç®—ä»ç„¶æ˜¯å¯é€‰çš„ï¼ˆé€šè¿‡ `--skip_nashconv` æ§åˆ¶ï¼‰

## ğŸ“ ä½¿ç”¨å»ºè®®

### è®­ç»ƒæ—¶å¯ç”¨è¯„ä¼°

```bash
# å¯ç”¨è¯„ä¼°ï¼ˆåŒ…æ‹¬æµ‹è¯•å¯¹å±€ï¼‰
python train_deep_cfr_texas.py \
    --num_players 6 \
    --num_iterations 100 \
    --eval_interval 10 \
    --eval_with_games \
    --skip_nashconv

# åªå¯ç”¨è½»é‡çº§è¯„ä¼°ï¼ˆä¸åŒ…å«æµ‹è¯•å¯¹å±€ï¼Œæ›´å¿«ï¼‰
python train_deep_cfr_texas.py \
    --num_players 6 \
    --num_iterations 100 \
    --eval_interval 10 \
    --skip_nashconv
```

### æŸ¥çœ‹è®­ç»ƒå†å²

è®­ç»ƒå†å²ä¿å­˜åœ¨ `models/deepcfr_texas/deepcfr_texas_training_history.json`ï¼ŒåŒ…å«ï¼š
- æ¯æ¬¡è¿­ä»£çš„æŸå¤±å€¼
- ç­–ç•¥ç†µ
- ç¼“å†²åŒºå¤§å°
- æµ‹è¯•å¯¹å±€ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰

```python
import json

with open('models/deepcfr_texas/deepcfr_texas_training_history.json', 'r') as f:
    history = json.load(f)

# æŸ¥çœ‹ç­–ç•¥ç†µè¶‹åŠ¿
entropies = [it['metrics']['avg_entropy'] for it in history['iterations']]
print(f"ç­–ç•¥ç†µ: {entropies}")

# æŸ¥çœ‹æµ‹è¯•å¯¹å±€ç»“æœ
if 'test_results' in history['iterations'][-1]:
    test = history['iterations'][-1]['test_results']
    print(f"å¹³å‡æ”¶ç›Š: {test['player0_avg_return']:.4f}")
    print(f"èƒœç‡: {test['player0_win_rate']*100:.1f}%")
```

## ğŸ“ æ€»ç»“

### æŸå¤±å€¼
- **ä¼˜åŠ¿æŸå¤±**å’Œ**ç­–ç•¥æŸå¤±**ä½¿ç”¨ `sqrt(iteration)` åŠ æƒ
- **æŸå¤±å¢é•¿æ˜¯æ­£å¸¸çš„**ï¼Œå› ä¸ºåŠ æƒå› å­ä¼šå¢å¤§
- **ä¸è¦ç”¨æŸå¤±ç»å¯¹å€¼åˆ¤æ–­è®­ç»ƒæ•ˆæœ**

### è®­ç»ƒæ•ˆæœåˆ¤æ–­
1. **ç­–ç•¥ç†µ**ï¼šåº”è¯¥é€æ¸é™ä½ï¼ˆä½†ä¸è¦è¿‡ä½ï¼‰
2. **ç¼“å†²åŒºå¤§å°**ï¼šåº”è¯¥æŒç»­å¢é•¿
3. **æµ‹è¯•å¯¹å±€**ï¼šæ”¶ç›Šå’Œèƒœç‡åº”è¯¥æé«˜
4. **NashConv**ï¼šæœ€å‡†ç¡®ä½†è€—æ—¶ï¼ˆå¯é€‰ï¼‰

### æ¨èåšæ³•
- è®­ç»ƒæ—¶å¯ç”¨è¯„ä¼°ï¼ˆ`--eval_interval 10`ï¼‰
- å®šæœŸæŸ¥çœ‹è®­ç»ƒå†å²æ–‡ä»¶
- ä½¿ç”¨ `evaluate_model.py` è¿›è¡Œå®Œæ•´è¯„ä¼°
- è·³è¿‡ NashConv è®¡ç®—ï¼ˆ`--skip_nashconv`ï¼‰é™¤ééœ€è¦ç²¾ç¡®æŒ‡æ ‡

