# 交互式对局指南

## 📖 简介

`play_interactive.py` 是一个交互式对局脚本，让你可以与训练好的 DeepCFR 模型进行德州扑克对局。

## 🚀 快速开始

### 基本使用

```bash
# 使用默认模型目录
python play_interactive.py

# 指定模型目录
python play_interactive.py --model_dir models/deepcfr_texas_20251121_113543

# 指定玩家数量（必须与训练时一致）
python play_interactive.py --num_players 6

# 选择人类玩家编号（0 或 1，默认是 0）
python play_interactive.py --human_player 0
```

### 完整参数

```bash
python play_interactive.py \
    --model_dir models/deepcfr_texas_20251121_113543 \
    --num_players 2 \
    --human_player 0 \
    --use_gpu
```

## 📋 参数说明

| 参数 | 说明 | 默认值 |
|------|------|--------|
| `--model_dir` | 模型目录路径 | `models/deepcfr_texas_20251121_113543` |
| `--num_players` | 玩家数量（必须与训练时一致） | `2` |
| `--human_player` | 人类玩家编号（0 或 1） | `0` |
| `--use_gpu` | 使用 GPU（如果可用） | 默认启用 |

## 🎮 游戏流程

### 1. 启动游戏

运行脚本后，会显示：
- 使用的设备（CPU/GPU）
- 模型加载状态
- 人类玩家和模型玩家的编号

### 2. 游戏进行中

每轮会显示：
- **当前轮次**：Preflop / Flop / Turn / River
- **公共牌**：已发的公共牌（如果有）
- **你的手牌**：你的两张底牌
- **底池**：当前底池大小
- **可选动作**：当前可执行的动作列表

### 3. 选择动作

当轮到你行动时，会显示可选动作：
```
可选动作:
  1. 弃牌 (Fold)
  2. 跟注/过牌 (Call/Check)
  3. 加注 (Raise)

请选择动作 (输入数字): 
```

输入对应的数字即可。

### 4. 游戏结束

游戏结束后会显示：
- 最终公共牌
- 你的收益和模型收益
- 胜负结果

## 💡 使用示例

### 示例 1: 基本对局

```bash
$ python play_interactive.py

使用设备: cuda

[1/2] 加载模型: models/deepcfr_texas_20251121_113543
  ✓ 模型加载成功（简单特征版本）

======================================================================
交互式对局
======================================================================
人类玩家: 0
模型玩家: 1

提示: 输入 Ctrl+C 可以退出游戏

======================================================================
开始新游戏
======================================================================

======================================================================
当前轮次: Preflop
======================================================================

公共牌: (未发牌)

你的手牌: A♠ K♠

底池: 200

轮到你行动

可选动作:
  1. 弃牌 (Fold)
  2. 跟注/过牌 (Call/Check)
  3. 加注 (Raise)

请选择动作 (输入数字): 3

你选择了: 3

模型选择了: 1

...

======================================================================
游戏结束
======================================================================

最终公共牌: A♥ K♦ Q♠ J♠ 10♠

结果:
  你的收益: 150.00
  模型收益: -150.00

🎉 你赢了！

----------------------------------------------------------------------
是否继续下一局? (y/n): n
```

## 🎯 功能特点

### ✅ 已实现功能

1. **模型加载**
   - 支持简单特征版本（`DeepCFRSimpleFeature`）
   - 支持标准版本（`MLP`）
   - 自动检测模型类型

2. **游戏状态显示**
   - 显示当前轮次（Preflop/Flop/Turn/River）
   - 显示公共牌和手牌
   - 显示底池大小
   - 显示当前行动玩家

3. **交互式操作**
   - 人类玩家选择动作
   - 模型自动选择动作
   - 显示模型的动作概率（如果有）

4. **游戏结果**
   - 显示最终收益
   - 显示胜负结果
   - 支持多局游戏

### ⚠️ 注意事项

1. **游戏配置必须一致**
   - `--num_players` 必须与训练时一致
   - 游戏配置（盲注、筹码等）必须与训练时一致

2. **模型文件**
   - 需要 `deepcfr_texas_policy_network.pt` 文件
   - 模型目录必须包含此文件

3. **动作编号**
   - 动作编号可能因游戏配置而异
   - 脚本会显示当前可用的动作

## 🔧 故障排除

### 问题 1: 模型目录不存在

**错误信息**:
```
错误: 模型目录不存在: models/deepcfr_texas_xxx
```

**解决方法**:
- 检查模型目录路径是否正确
- 脚本会列出可用的模型目录

### 问题 2: 模型加载失败

**错误信息**:
```
模型加载失败
```

**解决方法**:
- 检查模型文件是否存在
- 检查 `--num_players` 是否与训练时一致
- 检查网络结构是否匹配

### 问题 3: 动作选择无效

**错误信息**:
```
无效选择，请输入 1-3 之间的数字
```

**解决方法**:
- 输入数字，不要输入其他字符
- 确保输入的数字在可选范围内

## 📝 提示

1. **观察模型行为**
   - 注意模型在不同情况下的动作选择
   - 观察模型的动作概率分布

2. **学习策略**
   - 通过观察模型的行为学习策略
   - 尝试不同的动作组合

3. **多局游戏**
   - 进行多局游戏以获得更准确的评估
   - 观察长期收益趋势

## 🎓 下一步

- 使用 `evaluate_model.py` 进行批量评估
- 使用 `analyze_training_results.py` 分析训练效果
- 调整训练参数并重新训练

