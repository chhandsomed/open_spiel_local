#!/usr/bin/env python3
"""
å¤šè¿›ç¨‹å¹¶è¡Œ DeepCFR è®­ç»ƒå™¨

æ¶æ„ï¼š
- å¤šä¸ª Worker è¿›ç¨‹å¹¶è¡Œéå†æ¸¸æˆæ ‘ï¼Œæ”¶é›†æ ·æœ¬
- ä¸»è¿›ç¨‹ä»å…±äº«ç¼“å†²åŒºé‡‡æ ·ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œ
- ä½¿ç”¨å…±äº«å†…å­˜å®ç°è¿›ç¨‹é—´é«˜æ•ˆé€šä¿¡

ä¼˜åŠ¿ï¼š
- çœŸæ­£çš„å¹¶è¡ŒåŒ–ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸ CPU
- æ¸¸æˆæ ‘éå†ï¼ˆCPU å¯†é›†ï¼‰å’Œç½‘ç»œè®­ç»ƒï¼ˆGPU å¯†é›†ï¼‰å¯ä»¥åŒæ—¶è¿›è¡Œ
- çº¿æ€§æ‰©å±•ï¼šN ä¸ª Worker å¯ä»¥è·å¾—æ¥è¿‘ N å€çš„éå†é€Ÿåº¦
"""

import os
os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')

import time
import signal
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.multiprocessing as mp
from multiprocessing import Process, Queue, Event, Value, Manager
from collections import namedtuple
import queue

import pyspiel
from open_spiel.python.pytorch import deep_cfr
from deep_cfr_simple_feature import SimpleFeatureMLP


# æ ·æœ¬æ•°æ®ç»“æ„
AdvantageMemory = namedtuple("AdvantageMemory", "info_state iteration advantage action")
StrategyMemory = namedtuple("StrategyMemory", "info_state iteration strategy_action_probs")


class SharedBuffer:
    """å…±äº«å†…å­˜ç¼“å†²åŒºï¼Œç”¨äºè¿›ç¨‹é—´é€šä¿¡
    
    ä½¿ç”¨ Manager å®ç°è·¨è¿›ç¨‹å…±äº«çš„åˆ—è¡¨
    """
    
    def __init__(self, manager, capacity=1000000):
        self.capacity = capacity
        self._data = manager.list()
        self._add_calls = Value('i', 0)
        self._lock = manager.Lock()
    
    def add(self, element):
        """æ·»åŠ æ ·æœ¬ï¼ˆReservoir Samplingï¼‰"""
        with self._lock:
            if len(self._data) < self.capacity:
                self._data.append(element)
            else:
                idx = np.random.randint(0, self._add_calls.value + 1)
                if idx < self.capacity:
                    self._data[idx] = element
            self._add_calls.value += 1
    
    def sample(self, num_samples):
        """é‡‡æ ·"""
        with self._lock:
            if len(self._data) < num_samples:
                return list(self._data)
            indices = np.random.choice(len(self._data), num_samples, replace=False)
            return [self._data[i] for i in indices]
    
    def __len__(self):
        return len(self._data)
    
    def clear(self):
        with self._lock:
            while len(self._data) > 0:
                self._data.pop()
            self._add_calls.value = 0


class NetworkWrapper:
    """ç½‘ç»œåŒ…è£…å™¨ï¼Œæ”¯æŒè·¨è¿›ç¨‹å…±äº«
    
    ä½¿ç”¨å…±äº«å†…å­˜å­˜å‚¨ç½‘ç»œå‚æ•°ï¼ŒWorker å¯ä»¥è¯»å–æœ€æ–°å‚æ•°
    """
    
    def __init__(self, network, device='cpu'):
        self.network = network
        self.device = device
        self._state_dict = None
    
    def get_state_dict(self):
        """è·å–ç½‘ç»œå‚æ•°ï¼ˆç”¨äº Worker åŒæ­¥ï¼‰"""
        return {k: v.cpu().numpy() for k, v in self.network.state_dict().items()}
    
    def load_state_dict_from_numpy(self, numpy_dict):
        """ä» numpy å­—å…¸åŠ è½½å‚æ•°"""
        state_dict = {k: torch.from_numpy(v) for k, v in numpy_dict.items()}
        self.network.load_state_dict(state_dict)
        self.network = self.network.to(self.device)


def worker_process(
    worker_id,
    game_string,
    num_players,
    embedding_size,
    num_actions,
    advantage_network_layers,
    advantage_queues,  # æ¯ä¸ªç©å®¶ä¸€ä¸ªé˜Ÿåˆ—
    strategy_queue,
    network_params_queue,  # æ¥æ”¶æœ€æ–°ç½‘ç»œå‚æ•°
    stop_event,
    iteration_counter,
    num_traversals_per_batch,
    device='cpu'
):
    """Worker è¿›ç¨‹ï¼šå¹¶è¡Œéå†æ¸¸æˆæ ‘
    
    Args:
        worker_id: Worker ID
        game_string: æ¸¸æˆé…ç½®å­—ç¬¦ä¸²
        num_players: ç©å®¶æ•°é‡
        embedding_size: ä¿¡æ¯çŠ¶æ€ç»´åº¦
        num_actions: åŠ¨ä½œæ•°é‡
        advantage_network_layers: ä¼˜åŠ¿ç½‘ç»œå±‚é…ç½®
        advantage_queues: ä¼˜åŠ¿æ ·æœ¬é˜Ÿåˆ—ï¼ˆæ¯ä¸ªç©å®¶ä¸€ä¸ªï¼‰
        strategy_queue: ç­–ç•¥æ ·æœ¬é˜Ÿåˆ—
        network_params_queue: ç½‘ç»œå‚æ•°é˜Ÿåˆ—
        stop_event: åœæ­¢ä¿¡å·
        iteration_counter: å½“å‰è¿­ä»£è®¡æ•°å™¨
        num_traversals_per_batch: æ¯æ‰¹éå†æ¬¡æ•°
        device: è®¡ç®—è®¾å¤‡
    """
    # è®¾ç½®è¿›ç¨‹åç§°
    import setproctitle
    try:
        setproctitle.setproctitle(f"deepcfr_worker_{worker_id}")
    except:
        pass
    
    print(f"[Worker {worker_id}] å¯åŠ¨ï¼Œè®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¸¸æˆ
    game = pyspiel.load_game(game_string)
    root_node = game.new_initial_state()
    
    # åˆ›å»ºæœ¬åœ°ä¼˜åŠ¿ç½‘ç»œï¼ˆç”¨äºé‡‡æ ·åŠ¨ä½œï¼‰
    advantage_networks = []
    for _ in range(num_players):
        net = SimpleFeatureMLP(
            embedding_size,
            list(advantage_network_layers),
            num_actions,
            num_players=num_players,
            max_game_length=game.max_game_length()
        )
        net = net.to(device)
        net.eval()
        advantage_networks.append(net)
    
    def sample_action_from_advantage(state, player):
        """ä½¿ç”¨ä¼˜åŠ¿ç½‘ç»œé‡‡æ ·åŠ¨ä½œ"""
        info_state = state.information_state_tensor(player)
        legal_actions = state.legal_actions(player)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(np.expand_dims(info_state, axis=0)).to(device)
            raw_advantages = advantage_networks[player](state_tensor)[0].cpu().numpy()
        
        advantages = [max(0., advantage) for advantage in raw_advantages]
        cumulative_regret = sum(advantages[action] for action in legal_actions)
        
        matched_regrets = np.array([0.] * num_actions)
        if cumulative_regret > 0.:
            for action in legal_actions:
                matched_regrets[action] = advantages[action] / cumulative_regret
        else:
            matched_regrets[max(legal_actions, key=lambda a: raw_advantages[a])] = 1
        
        return advantages, matched_regrets
    
    def traverse_game_tree(state, player, iteration):
        """éå†æ¸¸æˆæ ‘ï¼Œæ”¶é›†æ ·æœ¬"""
        if state.is_terminal():
            return state.returns()[player]
        
        if state.is_chance_node():
            chance_outcome, chance_proba = zip(*state.chance_outcomes())
            action = np.random.choice(chance_outcome, p=chance_proba)
            return traverse_game_tree(state.child(action), player, iteration)
        
        if state.current_player() == player:
            expected_payoff = {}
            sampled_regret = {}
            
            _, strategy = sample_action_from_advantage(state, player)
            
            for action in state.legal_actions():
                expected_payoff[action] = traverse_game_tree(
                    state.child(action), player, iteration
                )
            
            cfv = sum(strategy[a] * expected_payoff[a] for a in state.legal_actions())
            
            for action in state.legal_actions():
                sampled_regret[action] = expected_payoff[action] - cfv
            
            sampled_regret_arr = [0] * num_actions
            for action in sampled_regret:
                sampled_regret_arr[action] = sampled_regret[action]
            
            # å‘é€ä¼˜åŠ¿æ ·æœ¬
            sample = AdvantageMemory(
                state.information_state_tensor(),
                iteration,
                sampled_regret_arr,
                action
            )
            try:
                advantage_queues[player].put_nowait(sample)
            except queue.Full:
                pass  # é˜Ÿåˆ—æ»¡äº†å°±ä¸¢å¼ƒ
            
            return cfv
        else:
            other_player = state.current_player()
            _, strategy = sample_action_from_advantage(state, other_player)
            
            probs = np.array(strategy)
            probs /= probs.sum()
            sampled_action = np.random.choice(range(num_actions), p=probs)
            
            # å‘é€ç­–ç•¥æ ·æœ¬
            sample = StrategyMemory(
                state.information_state_tensor(other_player),
                iteration,
                strategy
            )
            try:
                strategy_queue.put_nowait(sample)
            except queue.Full:
                pass
            
            return traverse_game_tree(state.child(sampled_action), player, iteration)
    
    # ä¸»å¾ªç¯
    last_sync_iteration = 0
    
    while not stop_event.is_set():
        current_iteration = iteration_counter.value
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åŒæ­¥ç½‘ç»œå‚æ•°
        try:
            while True:
                params = network_params_queue.get_nowait()
                for player in range(num_players):
                    if player in params:
                        numpy_dict = params[player]
                        state_dict = {k: torch.from_numpy(v) for k, v in numpy_dict.items()}
                        advantage_networks[player].load_state_dict(state_dict)
                        advantage_networks[player] = advantage_networks[player].to(device)
                last_sync_iteration = current_iteration
        except queue.Empty:
            pass
        
        # éå†æ¸¸æˆæ ‘
        for player in range(num_players):
            for _ in range(num_traversals_per_batch):
                if stop_event.is_set():
                    break
                traverse_game_tree(root_node.clone(), player, current_iteration)
    
    print(f"[Worker {worker_id}] åœæ­¢")


class ParallelDeepCFRSolver:
    """å¤šè¿›ç¨‹å¹¶è¡Œ DeepCFR æ±‚è§£å™¨
    
    ä½¿ç”¨å¤šä¸ª Worker è¿›ç¨‹å¹¶è¡Œéå†æ¸¸æˆæ ‘ï¼Œä¸»è¿›ç¨‹è®­ç»ƒç½‘ç»œã€‚
    """
    
    def __init__(
        self,
        game,
        num_workers=4,
        policy_network_layers=(128, 128),
        advantage_network_layers=(128, 128),
        num_iterations=100,
        num_traversals=20,
        learning_rate=1e-4,
        batch_size_advantage=2048,
        batch_size_strategy=2048,
        memory_capacity=1000000,
        device='cuda',
        gpu_ids=None,  # å¤š GPU æ”¯æŒ
        sync_interval=1,  # æ¯å¤šå°‘æ¬¡è¿­ä»£åŒæ­¥ä¸€æ¬¡ç½‘ç»œå‚æ•°
    ):
        self.game = game
        self.num_workers = num_workers
        self.num_players = game.num_players()
        self.num_iterations = num_iterations
        self.num_traversals = num_traversals
        self.learning_rate = learning_rate
        self.batch_size_advantage = batch_size_advantage
        self.batch_size_strategy = batch_size_strategy
        self.memory_capacity = memory_capacity
        self.sync_interval = sync_interval
        
        # å¤š GPU è®¾ç½®
        self.gpu_ids = gpu_ids
        self.use_multi_gpu = gpu_ids is not None and len(gpu_ids) > 1 and torch.cuda.is_available()
        
        if self.use_multi_gpu:
            self.device = torch.device(f"cuda:{gpu_ids[0]}")
            print(f"  å¤š GPU æ¨¡å¼: {gpu_ids}")
        else:
            self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # æ¸¸æˆä¿¡æ¯
        self._root_node = game.new_initial_state()
        self._embedding_size = len(self._root_node.information_state_tensor(0))
        self._num_actions = game.num_distinct_actions()
        
        # æ¸¸æˆå­—ç¬¦ä¸²ï¼ˆç”¨äº Worker åˆ›å»ºæ¸¸æˆï¼‰
        self._game_string = str(game)
        
        # ç½‘ç»œå±‚é…ç½®
        self._policy_network_layers = policy_network_layers
        self._advantage_network_layers = advantage_network_layers
        
        # åˆ›å»ºç½‘ç»œ
        self._create_networks()
        
        # å¤šè¿›ç¨‹ç»„ä»¶
        self._manager = None
        self._workers = []
        self._advantage_queues = []
        self._strategy_queue = None
        self._network_params_queues = []
        self._stop_event = None
        self._iteration_counter = None
        
        # æœ¬åœ°ç¼“å†²åŒº
        self._advantage_memories = [
            deep_cfr.ReservoirBuffer(memory_capacity) for _ in range(self.num_players)
        ]
        self._strategy_memories = deep_cfr.ReservoirBuffer(memory_capacity)
        
        self._iteration = 1
    
    def _create_networks(self):
        """åˆ›å»ºç¥ç»ç½‘ç»œ"""
        # ç­–ç•¥ç½‘ç»œ
        policy_net = SimpleFeatureMLP(
            self._embedding_size,
            list(self._policy_network_layers),
            self._num_actions,
            num_players=self.num_players,
            max_game_length=self.game.max_game_length()
        )
        
        # å¤š GPU åŒ…è£…
        if self.use_multi_gpu:
            self._policy_network = nn.DataParallel(policy_net, device_ids=self.gpu_ids)
            self._policy_network = self._policy_network.to(self.device)
        else:
            self._policy_network = policy_net.to(self.device)
        
        self._policy_sm = nn.Softmax(dim=-1)
        self._loss_policy = nn.MSELoss()
        self._optimizer_policy = torch.optim.Adam(
            self._policy_network.parameters(), lr=self.learning_rate
        )
        
        # ä¼˜åŠ¿ç½‘ç»œï¼ˆæ¯ä¸ªç©å®¶ä¸€ä¸ªï¼‰
        self._advantage_networks = []
        self._optimizer_advantages = []
        for _ in range(self.num_players):
            net = SimpleFeatureMLP(
                self._embedding_size,
                list(self._advantage_network_layers),
                self._num_actions,
                num_players=self.num_players,
                max_game_length=self.game.max_game_length()
            )
            
            # å¤š GPU åŒ…è£…
            if self.use_multi_gpu:
                net = nn.DataParallel(net, device_ids=self.gpu_ids)
                net = net.to(self.device)
            else:
                net = net.to(self.device)
            
            self._advantage_networks.append(net)
            self._optimizer_advantages.append(
                torch.optim.Adam(net.parameters(), lr=self.learning_rate)
            )
        
        self._loss_advantages = nn.MSELoss(reduction="mean")
    
    def _start_workers(self):
        """å¯åŠ¨ Worker è¿›ç¨‹"""
        mp.set_start_method('spawn', force=True)
        
        self._manager = Manager()
        self._stop_event = Event()
        self._iteration_counter = Value('i', 1)
        
        # åˆ›å»ºé˜Ÿåˆ—
        self._advantage_queues = [Queue(maxsize=100000) for _ in range(self.num_players)]
        self._strategy_queue = Queue(maxsize=100000)
        self._network_params_queues = [Queue(maxsize=10) for _ in range(self.num_workers)]
        
        # è®¡ç®—æ¯ä¸ª Worker çš„éå†æ¬¡æ•°
        traversals_per_worker = max(1, self.num_traversals // self.num_workers)
        
        # å¯åŠ¨ Worker
        for i in range(self.num_workers):
            p = Process(
                target=worker_process,
                args=(
                    i,
                    self._game_string,
                    self.num_players,
                    self._embedding_size,
                    self._num_actions,
                    self._advantage_network_layers,
                    self._advantage_queues,
                    self._strategy_queue,
                    self._network_params_queues[i],
                    self._stop_event,
                    self._iteration_counter,
                    traversals_per_worker,
                    'cpu',  # Worker åœ¨ CPU ä¸Šè¿è¡Œ
                )
            )
            p.start()
            self._workers.append(p)
        
        print(f"å·²å¯åŠ¨ {self.num_workers} ä¸ª Worker è¿›ç¨‹")
    
    def _stop_workers(self):
        """åœæ­¢ Worker è¿›ç¨‹"""
        self._stop_event.set()
        for p in self._workers:
            p.join(timeout=5)
            if p.is_alive():
                p.terminate()
        self._workers = []
        print("æ‰€æœ‰ Worker å·²åœæ­¢")
    
    def _sync_network_params(self):
        """åŒæ­¥ç½‘ç»œå‚æ•°åˆ°æ‰€æœ‰ Worker"""
        params = {}
        for player in range(self.num_players):
            # å¤„ç† DataParallel åŒ…è£…
            net = self._advantage_networks[player]
            if isinstance(net, nn.DataParallel):
                state_dict = net.module.state_dict()
            else:
                state_dict = net.state_dict()
            
            params[player] = {
                k: v.cpu().numpy() 
                for k, v in state_dict.items()
            }
        
        for q in self._network_params_queues:
            try:
                q.put_nowait(params)
            except queue.Full:
                pass
    
    def _collect_samples(self, timeout=0.1):
        """ä»é˜Ÿåˆ—æ”¶é›†æ ·æœ¬"""
        # æ”¶é›†ä¼˜åŠ¿æ ·æœ¬
        for player in range(self.num_players):
            while True:
                try:
                    sample = self._advantage_queues[player].get_nowait()
                    self._advantage_memories[player].add(sample)
                except queue.Empty:
                    break
        
        # æ”¶é›†ç­–ç•¥æ ·æœ¬
        while True:
            try:
                sample = self._strategy_queue.get_nowait()
                self._strategy_memories.add(sample)
            except queue.Empty:
                break
    
    def _learn_advantage_network(self, player):
        """è®­ç»ƒä¼˜åŠ¿ç½‘ç»œ"""
        num_samples = len(self._advantage_memories[player])
        if num_samples < 32:  # æœ€å°‘éœ€è¦ 32 ä¸ªæ ·æœ¬æ‰è®­ç»ƒ
            return None
        
        # ä½¿ç”¨å®é™…æ ·æœ¬æ•°å’Œ batch_size çš„è¾ƒå°å€¼
        actual_batch_size = min(num_samples, self.batch_size_advantage)
        samples = self._advantage_memories[player].sample(actual_batch_size)
        
        info_states = []
        advantages = []
        iterations = []
        for s in samples:
            info_states.append(s.info_state)
            advantages.append(s.advantage)
            iterations.append([s.iteration])
        
        self._optimizer_advantages[player].zero_grad()
        advantages_tensor = torch.FloatTensor(np.array(advantages)).to(self.device)
        iters = torch.FloatTensor(np.sqrt(np.array(iterations))).to(self.device)
        outputs = self._advantage_networks[player](
            torch.FloatTensor(np.array(info_states)).to(self.device)
        )
        loss = self._loss_advantages(iters * outputs, iters * advantages_tensor)
        loss.backward()
        self._optimizer_advantages[player].step()
        
        return loss.detach().cpu().numpy()
    
    def _learn_strategy_network(self):
        """è®­ç»ƒç­–ç•¥ç½‘ç»œ"""
        num_samples = len(self._strategy_memories)
        if num_samples < 32:  # æœ€å°‘éœ€è¦ 32 ä¸ªæ ·æœ¬æ‰è®­ç»ƒ
            return None
        
        # ä½¿ç”¨å®é™…æ ·æœ¬æ•°å’Œ batch_size çš„è¾ƒå°å€¼
        actual_batch_size = min(num_samples, self.batch_size_strategy)
        samples = self._strategy_memories.sample(actual_batch_size)
        
        info_states = []
        action_probs = []
        iterations = []
        for s in samples:
            info_states.append(s.info_state)
            action_probs.append(s.strategy_action_probs)
            iterations.append([s.iteration])
        
        self._optimizer_policy.zero_grad()
        iters = torch.FloatTensor(np.sqrt(np.array(iterations))).to(self.device)
        ac_probs = torch.FloatTensor(np.array(np.squeeze(action_probs))).to(self.device)
        logits = self._policy_network(
            torch.FloatTensor(np.array(info_states)).to(self.device)
        )
        outputs = self._policy_sm(logits)
        loss = self._loss_policy(iters * outputs, iters * ac_probs)
        loss.backward()
        self._optimizer_policy.step()
        
        return loss.detach().cpu().numpy()
    
    def solve(self, verbose=True, eval_interval=10, checkpoint_interval=0, 
              model_dir=None, save_prefix=None, game=None):
        """è¿è¡Œå¹¶è¡Œ DeepCFR è®­ç»ƒ
        
        Args:
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            eval_interval: è¯„ä¼°é—´éš”
            checkpoint_interval: checkpoint ä¿å­˜é—´éš”ï¼ˆ0=ä¸ä¿å­˜ï¼‰
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            save_prefix: ä¿å­˜æ–‡ä»¶å‰ç¼€
            game: æ¸¸æˆå®ä¾‹ï¼ˆç”¨äºä¿å­˜ checkpointï¼‰
        
        Returns:
            policy_network: è®­ç»ƒå¥½çš„ç­–ç•¥ç½‘ç»œ
            advantage_losses: ä¼˜åŠ¿ç½‘ç»œæŸå¤±å†å²
            policy_loss: ç­–ç•¥ç½‘ç»œæœ€ç»ˆæŸå¤±
        """
        print("=" * 70)
        print("å¹¶è¡Œ DeepCFR è®­ç»ƒ")
        print("=" * 70)
        print(f"  Worker æ•°é‡: {self.num_workers}")
        print(f"  è¿­ä»£æ¬¡æ•°: {self.num_iterations}")
        print(f"  æ¯æ¬¡è¿­ä»£éå†æ¬¡æ•°: {self.num_traversals}")
        print(f"  è®¾å¤‡: {self.device}")
        print()
        
        # å¯åŠ¨ Worker
        self._start_workers()
        
        advantage_losses = {p: [] for p in range(self.num_players)}
        start_time = time.time()
        
        try:
            # ç­‰å¾… Worker å¯åŠ¨å¹¶å¼€å§‹äº§ç”Ÿæ ·æœ¬
            print("  ç­‰å¾… Worker å¯åŠ¨...", end="", flush=True)
            warmup_time = 0
            max_warmup = 30  # æœ€å¤šç­‰å¾… 30 ç§’
            while warmup_time < max_warmup:
                time.sleep(1)
                warmup_time += 1
                self._collect_samples()
                total_samples = sum(len(m) for m in self._advantage_memories)
                if total_samples > 0:
                    print(f" å°±ç»ª (è€—æ—¶ {warmup_time} ç§’ï¼Œå·²æ”¶é›† {total_samples} ä¸ªæ ·æœ¬)")
                    break
                print(".", end="", flush=True)
            else:
                print(f" è­¦å‘Š: Worker å¯åŠ¨è¶…æ—¶ï¼Œç»§ç»­è®­ç»ƒ...")
            
            for iteration in range(self.num_iterations):
                iter_start = time.time()
                
                # æ›´æ–°è¿­ä»£è®¡æ•°å™¨
                self._iteration_counter.value = iteration + 1
                
                # ç­‰å¾… Worker æ”¶é›†æ ·æœ¬ï¼ˆæ ¹æ®éå†æ¬¡æ•°åŠ¨æ€è°ƒæ•´ï¼‰
                # æ¯æ¬¡éå†å¤§çº¦éœ€è¦ 0.2-0.5 ç§’ï¼Œæ€»å…±éœ€è¦ num_traversals / num_workers æ¬¡
                wait_time = max(0.5, (self.num_traversals / self.num_workers) * 0.3)
                time.sleep(wait_time)
                
                # æ”¶é›†æ ·æœ¬
                self._collect_samples()
                
                # è®­ç»ƒä¼˜åŠ¿ç½‘ç»œ
                for player in range(self.num_players):
                    loss = self._learn_advantage_network(player)
                    if loss is not None:
                        advantage_losses[player].append(loss)
                
                # åŒæ­¥ç½‘ç»œå‚æ•°åˆ° Worker
                if (iteration + 1) % self.sync_interval == 0:
                    self._sync_network_params()
                
                self._iteration += 1
                
                iter_time = time.time() - iter_start
                
                if verbose:
                    print(f"\r  è¿­ä»£ {iteration + 1}/{self.num_iterations} "
                          f"(è€—æ—¶: {iter_time:.2f}ç§’) | "
                          f"ä¼˜åŠ¿æ ·æœ¬: {sum(len(m) for m in self._advantage_memories):,} | "
                          f"ç­–ç•¥æ ·æœ¬: {len(self._strategy_memories):,}", end="")
                
                if (iteration + 1) % eval_interval == 0:
                    print()
                    for player, losses in advantage_losses.items():
                        if losses:
                            print(f"    ç©å®¶ {player} æŸå¤±: {losses[-1]:.6f}")
                
                # ä¿å­˜ checkpoint
                if checkpoint_interval > 0 and (iteration + 1) % checkpoint_interval == 0:
                    if model_dir and save_prefix and game:
                        print(f"\n  ğŸ’¾ ä¿å­˜ checkpoint (è¿­ä»£ {iteration + 1})...", end="", flush=True)
                        try:
                            save_checkpoint(self, game, model_dir, save_prefix, iteration + 1)
                            print(" å®Œæˆ")
                        except Exception as e:
                            print(f" å¤±è´¥: {e}")
            
            print()
            
            # è®­ç»ƒç­–ç•¥ç½‘ç»œ
            print("  è®­ç»ƒç­–ç•¥ç½‘ç»œ...")
            policy_loss = self._learn_strategy_network()
            
            total_time = time.time() - start_time
            print(f"\n  âœ“ è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f} ç§’")
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            if model_dir and save_prefix and game:
                print(f"  ğŸ’¾ ä¿å­˜ä¸­æ–­æ—¶çš„ checkpoint (è¿­ä»£ {self._iteration})...")
                try:
                    save_checkpoint(self, game, model_dir, save_prefix, self._iteration)
                    print(f"  âœ“ Checkpoint å·²ä¿å­˜")
                except Exception as e:
                    print(f"  âœ— ä¿å­˜å¤±è´¥: {e}")
        finally:
            # åœæ­¢ Worker
            self._stop_workers()
        
        return self._policy_network, advantage_losses, policy_loss
    
    def action_probabilities(self, state, player_id=None):
        """è®¡ç®—åŠ¨ä½œæ¦‚ç‡ï¼ˆç”¨äºæ¨ç†ï¼‰"""
        del player_id
        cur_player = state.current_player()
        legal_actions = state.legal_actions(cur_player)
        info_state_vector = np.array(state.information_state_tensor())
        if len(info_state_vector.shape) == 1:
            info_state_vector = np.expand_dims(info_state_vector, axis=0)
        with torch.no_grad():
            logits = self._policy_network(
                torch.FloatTensor(info_state_vector).to(self.device)
            )
            probs = self._policy_sm(logits).cpu().numpy()
        return {action: probs[0][action] for action in legal_actions}


def create_save_directory(save_prefix, save_dir="models"):
    """åˆ›å»ºä¿å­˜ç›®å½•"""
    import time as time_module
    base_dir = save_dir
    if not os.path.exists(base_dir):
        os.makedirs(base_dir)
    
    model_dir = os.path.join(base_dir, save_prefix)
    if os.path.exists(model_dir):
        timestamp = time_module.strftime("%Y%m%d_%H%M%S")
        model_dir = f"{model_dir}_{timestamp}"
    
    os.makedirs(model_dir, exist_ok=True)
    return model_dir


def save_checkpoint(solver, game, model_dir, save_prefix, iteration, is_final=False):
    """ä¿å­˜ checkpoint"""
    if is_final:
        suffix = ""
        checkpoint_dir = model_dir
    else:
        suffix = f"_iter{iteration}"
        checkpoint_dir = os.path.join(model_dir, "checkpoints")
        os.makedirs(checkpoint_dir, exist_ok=True)
    
    # ä¿å­˜ç­–ç•¥ç½‘ç»œï¼ˆå¤„ç† DataParallelï¼‰
    policy_path = os.path.join(checkpoint_dir, f"{save_prefix}_policy_network{suffix}.pt")
    policy_net = solver._policy_network
    if isinstance(policy_net, nn.DataParallel):
        torch.save(policy_net.module.state_dict(), policy_path)
    else:
        torch.save(policy_net.state_dict(), policy_path)
    
    # ä¿å­˜ä¼˜åŠ¿ç½‘ç»œï¼ˆå¤„ç† DataParallelï¼‰
    for player in range(game.num_players()):
        advantage_path = os.path.join(checkpoint_dir, f"{save_prefix}_advantage_player_{player}{suffix}.pt")
        adv_net = solver._advantage_networks[player]
        if isinstance(adv_net, nn.DataParallel):
            torch.save(adv_net.module.state_dict(), advantage_path)
        else:
            torch.save(adv_net.state_dict(), advantage_path)
    
    return checkpoint_dir


def main():
    parser = argparse.ArgumentParser(description="å¤šè¿›ç¨‹å¹¶è¡Œ DeepCFR è®­ç»ƒ")
    parser.add_argument("--num_players", type=int, default=2, help="ç©å®¶æ•°é‡")
    parser.add_argument("--num_workers", type=int, default=4, help="Worker è¿›ç¨‹æ•°é‡")
    parser.add_argument("--num_iterations", type=int, default=100, help="è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--num_traversals", type=int, default=40, help="æ¯æ¬¡è¿­ä»£éå†æ¬¡æ•°")
    parser.add_argument("--policy_layers", type=int, nargs="+", default=[128, 128])
    parser.add_argument("--advantage_layers", type=int, nargs="+", default=[128, 128])
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--batch_size", type=int, default=2048)
    parser.add_argument("--memory_capacity", type=int, default=1000000)
    parser.add_argument("--betting_abstraction", type=str, default="fcpa")
    parser.add_argument("--save_prefix", type=str, default="deepcfr_parallel")
    parser.add_argument("--save_dir", type=str, default="models")
    parser.add_argument("--eval_interval", type=int, default=10)
    parser.add_argument("--checkpoint_interval", type=int, default=0, 
                        help="Checkpoint ä¿å­˜é—´éš”ï¼ˆ0=ä¸ä¿å­˜ä¸­é—´checkpointï¼‰")
    parser.add_argument("--skip_nashconv", action="store_true", 
                        help="è·³è¿‡ NashConv è®¡ç®—ï¼ˆ6äººå±€å¼ºçƒˆå»ºè®®ï¼‰")
    parser.add_argument("--use_gpu", action="store_true", default=True,
                        help="ä½¿ç”¨ GPU è®­ç»ƒ")
    parser.add_argument("--gpu_ids", type=int, nargs="+", default=None,
                        help="ä½¿ç”¨çš„ GPU ID åˆ—è¡¨ï¼ˆä¾‹å¦‚ --gpu_ids 0 1 2 3ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¸¸æˆ
    num_players = args.num_players
    if num_players == 2:
        blinds_str = "100 50"
        first_player_str = "2 1 1 1"
    else:
        blinds_list = ["50", "100"] + ["0"] * (num_players - 2)
        blinds_str = " ".join(blinds_list)
        first_player_str = " ".join(["3"] + ["1"] * 3)
    
    stacks_str = " ".join(["2000"] * num_players)
    
    game_string = (
        f"universal_poker("
        f"betting=nolimit,"
        f"numPlayers={num_players},"
        f"numRounds=4,"
        f"blind={blinds_str},"
        f"stack={stacks_str},"
        f"numHoleCards=2,"
        f"numBoardCards=0 3 1 1,"
        f"firstPlayer={first_player_str},"
        f"numSuits=4,"
        f"numRanks=13,"
        f"bettingAbstraction={args.betting_abstraction}"
        f")"
    )
    
    # è®¾ç½®è®¾å¤‡
    gpu_ids = None
    if args.use_gpu and torch.cuda.is_available():
        if args.gpu_ids is not None and len(args.gpu_ids) > 0:
            gpu_ids = args.gpu_ids
            device = f"cuda:{gpu_ids[0]}"
            if len(gpu_ids) > 1:
                print(f"ä½¿ç”¨å¤š GPU: {gpu_ids}")
                for gid in gpu_ids:
                    print(f"  GPU {gid}: {torch.cuda.get_device_name(gid)}")
            else:
                print(f"ä½¿ç”¨ GPU: {torch.cuda.get_device_name(gpu_ids[0])}")
        else:
            device = "cuda:0"
            print(f"ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        print("ä½¿ç”¨ CPU")
    
    print(f"åˆ›å»ºæ¸¸æˆ: {game_string}")
    game = pyspiel.load_game(game_string)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    model_dir = create_save_directory(args.save_prefix, args.save_dir)
    print(f"æ¨¡å‹ä¿å­˜ç›®å½•: {model_dir}")
    if args.checkpoint_interval > 0:
        print(f"Checkpoint ä¿å­˜é—´éš”: æ¯ {args.checkpoint_interval} æ¬¡è¿­ä»£")
    
    # åˆ›å»ºæ±‚è§£å™¨
    solver = ParallelDeepCFRSolver(
        game,
        num_workers=args.num_workers,
        policy_network_layers=tuple(args.policy_layers),
        advantage_network_layers=tuple(args.advantage_layers),
        num_iterations=args.num_iterations,
        num_traversals=args.num_traversals,
        learning_rate=args.learning_rate,
        batch_size_advantage=args.batch_size,
        batch_size_strategy=args.batch_size,
        memory_capacity=args.memory_capacity,
        device=device,
        gpu_ids=gpu_ids,
    )
    
    # è®­ç»ƒï¼ˆå¸¦ checkpoint æ”¯æŒï¼‰
    policy_network, advantage_losses, policy_loss = solver.solve(
        verbose=True,
        eval_interval=args.eval_interval,
        checkpoint_interval=args.checkpoint_interval,
        model_dir=model_dir,
        save_prefix=args.save_prefix,
        game=game,
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    save_checkpoint(solver, game, model_dir, args.save_prefix, args.num_iterations, is_final=True)
    print(f"  âœ“ ç­–ç•¥ç½‘ç»œå·²ä¿å­˜: {os.path.join(model_dir, f'{args.save_prefix}_policy_network.pt')}")
    for player in range(num_players):
        print(f"  âœ“ ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œå·²ä¿å­˜")
    
    # ä¿å­˜é…ç½®
    import json
    config_path = os.path.join(model_dir, "config.json")
    config = {
        'num_players': num_players,
        'num_workers': args.num_workers,
        'num_iterations': args.num_iterations,
        'num_traversals': args.num_traversals,
        'policy_layers': args.policy_layers,
        'advantage_layers': args.advantage_layers,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size,
        'memory_capacity': args.memory_capacity,
        'betting_abstraction': args.betting_abstraction,
        'device': device,
        'gpu_ids': gpu_ids,
        'game_string': game_string,
        'multi_gpu': gpu_ids is not None and len(gpu_ids) > 1,
        'parallel': True,
    }
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"  âœ“ é…ç½®å·²ä¿å­˜: {config_path}")
    
    # NashConv è®¡ç®—ï¼ˆå¯é€‰ï¼‰
    if not args.skip_nashconv:
        print(f"\nè®¡ç®— NashConv...")
        if num_players > 2:
            print(f"  âš ï¸ è­¦å‘Š: {num_players} äººæ¸¸æˆçš„ NashConv è®¡ç®—å¯èƒ½éå¸¸æ…¢æˆ–ä¸å¯è¡Œ")
            print(f"  å»ºè®®: ä½¿ç”¨ --skip_nashconv è·³è¿‡")
        try:
            from open_spiel.python import policy
            
            average_policy = policy.tabular_policy_from_callable(
                game, solver.action_probabilities
            )
            pyspiel_policy = policy.python_policy_to_pyspiel_policy(average_policy)
            conv = pyspiel.nash_conv(game, pyspiel_policy, use_cpp_br=True)
            print(f"  âœ“ NashConv: {conv:.6f}")
        except Exception as e:
            print(f"  âœ— NashConv è®¡ç®—å¤±è´¥: {e}")
            print(f"  å»ºè®®: ä½¿ç”¨ --skip_nashconv è·³è¿‡")
    else:
        print(f"\n  â­ï¸ è·³è¿‡ NashConv è®¡ç®—")
    
    print("\n" + "=" * 70)
    print("âœ“ å¹¶è¡Œ DeepCFR è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)


if __name__ == "__main__":
    main()

