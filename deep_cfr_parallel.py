#!/usr/bin/env python3
"""
å¤šè¿›ç¨‹å¹¶è¡Œ DeepCFR è®­ç»ƒå™¨

æ¶æ„ï¼š
- å¤šä¸ª Worker è¿›ç¨‹å¹¶è¡Œéå†æ¸¸æˆæ ‘ï¼Œæ”¶é›†æ ·æœ¬
- ä¸»è¿›ç¨‹ä»å…±äº«ç¼“å†²åŒºé‡‡æ ·ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œ
- ä½¿ç”¨å…±äº«å†…å­˜å®ç°è¿›ç¨‹é—´é«˜æ•ˆé€šä¿¡

ä¼˜åŠ¿ï¼š
- çœŸæ­£çš„å¹¶è¡ŒåŒ–ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸ CPU
- æ¸¸æˆæ ‘éå†ï¼ˆCPU å¯†é›†ï¼‰å’Œç½‘ç»œè®­ç»ƒï¼ˆGPU å¯†é›†ï¼‰å¯ä»¥åŒæ—¶è¿›è¡Œ
- çº¿æ€§æ‰©å±•ï¼šN ä¸ª Worker å¯ä»¥è·å¾—æ¥è¿‘ N å€çš„éå†é€Ÿåº¦
"""

import os
os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')

import time
import signal
import argparse
import logging
import sys
import re
import numpy as np
import torch
import torch.nn as nn
import torch.multiprocessing as mp
from multiprocessing import Process, Queue, Event, Value, Manager
from collections import namedtuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional
import queue
import resource

# é…ç½®logging
def setup_logging(log_file=None):
    """é…ç½®loggingï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    handlers = [logging.StreamHandler(sys.stdout)]
    if log_file:
        handlers.append(logging.FileHandler(log_file, mode='a', encoding='utf-8'))
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=handlers,
        force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
    )
    return logging.getLogger(__name__)

# å…¨å±€loggerï¼ˆä¼šåœ¨mainå‡½æ•°ä¸­åˆå§‹åŒ–ï¼‰
logger = None

def get_logger():
    """è·å–loggerï¼Œå¦‚æœæœªåˆå§‹åŒ–åˆ™è¿”å›ä¸€ä¸ªç®€å•çš„logger"""
    global logger
    if logger is None:
        # å¦‚æœloggeræœªåˆå§‹åŒ–ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„loggerï¼ˆç”¨äºWorkerè¿›ç¨‹ï¼‰
        import logging
        logging.basicConfig(level=logging.INFO, format='%(message)s', force=True)
        logger = logging.getLogger(__name__)
    return logger

try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

import pyspiel
from open_spiel.python.pytorch import deep_cfr
from deep_cfr_simple_feature import SimpleFeatureMLP


# æ ·æœ¬æ•°æ®ç»“æ„
AdvantageMemory = namedtuple("AdvantageMemory", "info_state iteration advantage action")
StrategyMemory = namedtuple("StrategyMemory", "info_state iteration strategy_action_probs")


class SharedBuffer:
    """å…±äº«å†…å­˜ç¼“å†²åŒºï¼Œç”¨äºè¿›ç¨‹é—´é€šä¿¡
    
    ä½¿ç”¨ Manager å®ç°è·¨è¿›ç¨‹å…±äº«çš„åˆ—è¡¨
    """
    
    def __init__(self, manager, capacity=1000000):
        self.capacity = capacity
        self._data = manager.list()
        self._add_calls = Value('i', 0)
        self._lock = manager.Lock()
    
    def add(self, element):
        """æ·»åŠ æ ·æœ¬ï¼ˆReservoir Samplingï¼‰"""
        with self._lock:
            if len(self._data) < self.capacity:
                self._data.append(element)
            else:
                idx = np.random.randint(0, self._add_calls.value + 1)
                if idx < self.capacity:
                    self._data[idx] = element
            self._add_calls.value += 1
    
    def sample(self, num_samples):
        """é‡‡æ ·"""
        with self._lock:
            if len(self._data) < num_samples:
                return list(self._data)
            indices = np.random.choice(len(self._data), num_samples, replace=False)
            return [self._data[i] for i in indices]
    
    def __len__(self):
        return len(self._data)
    
    def clear(self):
        with self._lock:
            while len(self._data) > 0:
                self._data.pop()
            self._add_calls.value = 0


class RandomReplacementBuffer:
    """Reservoir Sampling ç¼“å†²åŒºï¼ˆNumPy ä¼˜åŒ–ç‰ˆï¼‰
    
    ä½¿ç”¨ NumPy æ•°ç»„å­˜å‚¨æ•°æ®ï¼Œå½»åº•é¿å… Python å¯¹è±¡è®¿é—®ç“¶é¢ˆã€‚
    - info_states: [capacity, info_state_size] float32
    - iterations: [capacity] int32
    - advantages/action_probs: [capacity, num_actions] float32
    
    ä¼˜åŒ–æ•ˆæœï¼šæ•°æ®å‡†å¤‡æ—¶é—´ä» ~700ms é™åˆ° ~5msï¼ˆ140x åŠ é€Ÿï¼‰
    """
    
    def __init__(self, capacity, info_state_size=None, num_actions=None, buffer_type='advantage'):
        """åˆå§‹åŒ–ç¼“å†²åŒº
        
        Args:
            capacity: ç¼“å†²åŒºå®¹é‡
            info_state_size: ä¿¡æ¯çŠ¶æ€ç»´åº¦ï¼ˆé¦–æ¬¡æ·»åŠ æ—¶è‡ªåŠ¨æ£€æµ‹ï¼‰
            num_actions: åŠ¨ä½œæ•°é‡ï¼ˆé¦–æ¬¡æ·»åŠ æ—¶è‡ªåŠ¨æ£€æµ‹ï¼‰
            buffer_type: 'advantage' æˆ– 'strategy'
        """
        self._capacity = capacity
        self._buffer_type = buffer_type
        self._size = 0  # å½“å‰æœ‰æ•ˆæ ·æœ¬æ•°
        self._add_calls = 0  # æ·»åŠ è®¡æ•°å™¨ï¼ˆReservoir Samplingå¿…éœ€ï¼‰
        
        # å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆé¦–æ¬¡æ·»åŠ æ—¶ç¡®å®šç»´åº¦ï¼‰
        self._info_state_size = info_state_size
        self._num_actions = num_actions
        self._initialized = False
        
        # NumPy æ•°ç»„ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._info_states = None
        self._iterations = None
        self._advantages = None  # ç”¨äº advantage buffer
        self._action_probs = None  # ç”¨äº strategy buffer
    
    def _lazy_init(self, element):
        """å»¶è¿Ÿåˆå§‹åŒ–ï¼šæ ¹æ®ç¬¬ä¸€ä¸ªæ ·æœ¬ç¡®å®šç»´åº¦"""
        if self._initialized:
            return
        
        # è‡ªåŠ¨æ£€æµ‹ç»´åº¦
        if self._info_state_size is None:
            self._info_state_size = len(element.info_state)
        
        if self._buffer_type == 'advantage':
            if self._num_actions is None:
                self._num_actions = len(element.advantage)
            # é¢„åˆ†é…æ•°ç»„
            self._info_states = np.zeros((self._capacity, self._info_state_size), dtype=np.float32)
            self._iterations = np.zeros(self._capacity, dtype=np.int32)
            self._advantages = np.zeros((self._capacity, self._num_actions), dtype=np.float32)
        else:  # strategy
            if self._num_actions is None:
                self._num_actions = len(element.strategy_action_probs)
            # é¢„åˆ†é…æ•°ç»„
            self._info_states = np.zeros((self._capacity, self._info_state_size), dtype=np.float32)
            self._iterations = np.zeros(self._capacity, dtype=np.int32)
            self._action_probs = np.zeros((self._capacity, self._num_actions), dtype=np.float32)
        
        self._initialized = True
    
    def add(self, element):
        """æ·»åŠ æ ·æœ¬ï¼ˆFIFOæ›¿æ¢ï¼Œç¡®ä¿æ–°æ ·æœ¬ä¸ä¼šè¢«éšæœºæ›¿æ¢æ‰ï¼‰"""
        if self._capacity == 0:
            return
        
        # å»¶è¿Ÿåˆå§‹åŒ–
        self._lazy_init(element)
        
        # ç¡®å®šå†™å…¥ä½ç½®
        if self._size < self._capacity:
            idx = self._size
            self._size += 1
        else:
            # FIFOæ›¿æ¢ï¼šå¾ªç¯æ›¿æ¢ï¼Œç¡®ä¿æ–°æ ·æœ¬ä¸ä¼šè¢«éšæœºæ›¿æ¢æ‰
            # è¿™æ ·å¯ä»¥ä¿æŒæ ·æœ¬çš„æ—¶æ•ˆæ€§ï¼Œæ–°æ ·æœ¬ä¸ä¼šè¢«è¿‡æ—©æ›¿æ¢
            idx = self._add_calls % self._capacity
        
        # å†™å…¥æ•°æ®ï¼ˆNumPy æ•°ç»„æ“ä½œï¼Œæ—  GILï¼‰
        self._info_states[idx] = element.info_state
        self._iterations[idx] = element.iteration
        
        if self._buffer_type == 'advantage':
            adv = element.advantage
            if len(adv) >= self._num_actions:
                self._advantages[idx] = adv[:self._num_actions]
            else:
                self._advantages[idx, :len(adv)] = adv
                self._advantages[idx, len(adv):] = 0
        else:  # strategy
            probs = element.strategy_action_probs
            if len(probs) >= self._num_actions:
                self._action_probs[idx] = probs[:self._num_actions]
            else:
                self._action_probs[idx, :len(probs)] = probs
                self._action_probs[idx, len(probs):] = 0
        
        self._add_calls += 1
    
    def sample(self, num_samples, current_iteration=None, new_sample_ratio=0.5, new_sample_window=0):
        """é‡‡æ ·ï¼ˆè¿”å› NumPy æ•°ç»„ï¼Œè€Œä¸æ˜¯ Python å¯¹è±¡åˆ—è¡¨ï¼‰
        
        ä¿®æ”¹ï¼šæ”¯æŒæŒ‰æ¯”ä¾‹åˆ†å±‚é‡‡æ ·ï¼ˆæ–°æ ·æœ¬/è€æ ·æœ¬ï¼‰ï¼Œç”¨äºæå‡â€œæ–°æ ·æœ¬å æ¯”â€ï¼Œé™ä½åˆ†å¸ƒæ¼‚ç§»å¸¦æ¥çš„è®­ç»ƒæŠ–åŠ¨ã€‚
        - æ–°æ ·æœ¬ï¼šiteration âˆˆ [current_iteration - new_sample_window, current_iteration]
          - new_sample_window=0 æ—¶é€€åŒ–ä¸º iteration == current_iterationï¼ˆæ—§è¡Œä¸ºï¼‰
        - è€æ ·æœ¬ï¼šå…¶ä»– iteration
        
        è¿”å›:
            dict: {
                'info_states': [batch, info_state_size],
                'iterations': [batch],
                'advantages': [batch, num_actions] (ä»… advantage buffer),
                'action_probs': [batch, num_actions] (ä»… strategy buffer),
                'new_sample_count': int
            }
        """
        if self._size == 0 or not self._initialized:
            return self._empty_result()
        
        actual_num_samples = min(num_samples, self._size)
        
        # å¦‚æœæä¾› current_iterationï¼Œåˆ™ä¼˜å…ˆåˆ†å±‚é‡‡æ ·ï¼Œç¡®ä¿æ–°æ ·æœ¬å æ¯”
        if current_iteration is not None and new_sample_ratio is not None:
            try:
                ratio = float(new_sample_ratio)
            except Exception:
                ratio = 0.5
            ratio = max(0.0, min(1.0, ratio))
            try:
                window = int(new_sample_window) if new_sample_window is not None else 0
            except Exception:
                window = 0
            window = max(0, window)
            return self._stratified_sample_numpy(actual_num_samples, int(current_iteration), ratio, window)
        
        # å¦åˆ™éšæœºé‡‡æ ·
        return self._random_sample_numpy(actual_num_samples, current_iteration, new_sample_window=new_sample_window)
    
    def _empty_result(self):
        """è¿”å›ç©ºç»“æœ"""
        if self._buffer_type == 'advantage':
            return {
                'info_states': np.array([], dtype=np.float32).reshape(0, self._info_state_size or 1),
                'iterations': np.array([], dtype=np.int32),
                'advantages': np.array([], dtype=np.float32).reshape(0, self._num_actions or 1),
                'new_sample_count': 0
            }
        else:
            return {
                'info_states': np.array([], dtype=np.float32).reshape(0, self._info_state_size or 1),
                'iterations': np.array([], dtype=np.int32),
                'action_probs': np.array([], dtype=np.float32).reshape(0, self._num_actions or 1),
                'new_sample_count': 0
            }
    
    def _random_sample_numpy(self, num_samples, current_iteration=None, new_sample_window=0):
        """éšæœºé‡‡æ ·ï¼ˆçº¯ NumPyï¼Œæ—  GILï¼‰
        
        ä¿®æ”¹ï¼šç»Ÿè®¡æ–°æ ·æœ¬æ•°é‡ï¼Œç”¨äºæ—¥å¿—è¾“å‡º
        """
        indices = np.random.choice(self._size, num_samples, replace=False)
        
        # ç»Ÿè®¡æ–°æ ·æœ¬æ•°é‡ï¼ˆç”¨äºæ—¥å¿—è¾“å‡ºï¼‰
        new_sample_count = 0
        if current_iteration is not None:
            try:
                window = int(new_sample_window) if new_sample_window is not None else 0
            except Exception:
                window = 0
            window = max(0, window)
            sampled_iterations = self._iterations[indices]
            if window <= 0:
                new_sample_count = np.sum(sampled_iterations == current_iteration)
            else:
                low = current_iteration - window
                new_sample_count = np.sum((sampled_iterations >= low) & (sampled_iterations <= current_iteration))
        
        return self._gather_by_indices(indices, new_sample_count=new_sample_count)
    
    def _stratified_sample_numpy(self, num_samples, current_iteration, new_sample_ratio, new_sample_window=0):
        """åˆ†å±‚é‡‡æ ·ï¼ˆçº¯ NumPy å‘é‡åŒ–ï¼Œæ—  GILï¼‰"""
        # å‘é‡åŒ–åˆ†ç¦»æ–°æ—§æ ·æœ¬ï¼ˆçº¯ NumPyï¼Œé‡Šæ”¾ GILï¼‰
        valid_iterations = self._iterations[:self._size]
        if new_sample_window is None:
            new_sample_window = 0
        try:
            window = int(new_sample_window)
        except Exception:
            window = 0
        window = max(0, window)
        if window <= 0:
            new_mask = valid_iterations == current_iteration
        else:
            low = current_iteration - window
            new_mask = (valid_iterations >= low) & (valid_iterations <= current_iteration)
        new_indices = np.where(new_mask)[0]
        old_indices = np.where(~new_mask)[0]
        
        num_new_available = len(new_indices)
        num_old_available = len(old_indices)
        
        # è®¡ç®—ç›®æ ‡é‡‡æ ·æ•°é‡ï¼šä¼˜å…ˆæ»¡è¶³ new_sample_ratioï¼Œä½†ä¿è¯å°½å¯èƒ½å‡‘æ»¡ num_samples
        desired_new = int(num_samples * new_sample_ratio)
        desired_new = max(0, min(desired_new, num_new_available))
        desired_old = num_samples - desired_new
        desired_old = max(0, min(desired_old, num_old_available))
        
        # å¦‚æœè€æ ·æœ¬ä¸è¶³ï¼Œç”¨å‰©ä½™çš„æ–°æ ·æœ¬è¡¥é½ï¼ˆå…è®¸è¶…è¿‡ new_sample_ratioï¼‰
        remaining = num_samples - (desired_new + desired_old)
        if remaining > 0 and num_new_available > desired_new:
            extra_new = min(remaining, num_new_available - desired_new)
            desired_new += extra_new
            remaining -= extra_new
        
        # å¦‚æœæ–°æ ·æœ¬ä¸è¶³ï¼Œå†ç”¨å‰©ä½™çš„è€æ ·æœ¬è¡¥é½
        if remaining > 0 and num_old_available > desired_old:
            extra_old = min(remaining, num_old_available - desired_old)
            desired_old += extra_old
            remaining -= extra_old
        
        # NumPy éšæœºé‡‡æ ·ï¼ˆé‡Šæ”¾ GILï¼‰
        sampled_indices = []
        
        if desired_new > 0 and num_new_available > 0:
            if desired_new >= num_new_available:
                sampled_indices.append(new_indices)
            else:
                sampled_indices.append(np.random.choice(new_indices, desired_new, replace=False))
        
        if desired_old > 0 and num_old_available > 0:
            if desired_old >= num_old_available:
                sampled_indices.append(old_indices)
            else:
                sampled_indices.append(np.random.choice(old_indices, desired_old, replace=False))
        
        if len(sampled_indices) == 0:
            return self._empty_result()
        
        all_indices = np.concatenate(sampled_indices)
        # æ‰“æ•£ç´¢å¼•ï¼Œé¿å…â€œæ–°æ ·æœ¬/è€æ ·æœ¬åˆ†å—â€å¸¦æ¥çš„æ½œåœ¨åºåˆ—åå·®
        if all_indices.shape[0] > 1:
            np.random.shuffle(all_indices)
        return self._gather_by_indices(all_indices, new_sample_count=desired_new)
    
    def _gather_by_indices(self, indices, new_sample_count=0):
        """æ ¹æ®ç´¢å¼•æ”¶é›†æ•°æ®ï¼ˆçº¯ NumPy åˆ‡ç‰‡ï¼Œæ—  GILï¼‰"""
        result = {
            'info_states': self._info_states[indices].copy(),
            'iterations': self._iterations[indices].copy(),
            'new_sample_count': new_sample_count
        }
        
        if self._buffer_type == 'advantage':
            result['advantages'] = self._advantages[indices].copy()
        else:
            result['action_probs'] = self._action_probs[indices].copy()
        
        return result
    
    def sample_legacy(self, num_samples, current_iteration=None, new_sample_ratio=0.5, new_sample_window=0):
        """å…¼å®¹æ—§æ¥å£ï¼šè¿”å› namedtuple åˆ—è¡¨ï¼ˆæ…¢ï¼Œä»…ç”¨äºå…¼å®¹ï¼‰"""
        result = self.sample(num_samples, current_iteration, new_sample_ratio, new_sample_window)
        
        if len(result['info_states']) == 0:
            return []
        
        samples = []
        for i in range(len(result['info_states'])):
            if self._buffer_type == 'advantage':
                samples.append(AdvantageMemory(
                    info_state=result['info_states'][i],
                    iteration=int(result['iterations'][i]),
                    advantage=result['advantages'][i],
                    action=0
                ))
            else:
                samples.append(StrategyMemory(
                    info_state=result['info_states'][i],
                    iteration=int(result['iterations'][i]),
                    strategy_action_probs=result['action_probs'][i]
                ))
        
        return samples
    
    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self._size = 0
        self._add_calls = 0
        # ä¸éœ€è¦é‡æ–°åˆ†é…æ•°ç»„ï¼Œåªéœ€é‡ç½®å¤§å°
    
    def __len__(self):
        return self._size
    
    def __iter__(self):
        """å…¼å®¹æ—§æ¥å£ï¼šè¿­ä»£å™¨ï¼ˆæ…¢ï¼Œä»…ç”¨äºå…¼å®¹ï¼‰"""
        for i in range(self._size):
            if self._buffer_type == 'advantage':
                yield AdvantageMemory(
                    info_state=self._info_states[i],
                    iteration=int(self._iterations[i]),
                    advantage=self._advantages[i],
                    action=0
                )
            else:
                yield StrategyMemory(
                    info_state=self._info_states[i],
                    iteration=int(self._iterations[i]),
                    strategy_action_probs=self._action_probs[i]
                )


class NetworkWrapper:
    """ç½‘ç»œåŒ…è£…å™¨ï¼Œæ”¯æŒè·¨è¿›ç¨‹å…±äº«
    
    ä½¿ç”¨å…±äº«å†…å­˜å­˜å‚¨ç½‘ç»œå‚æ•°ï¼ŒWorker å¯ä»¥è¯»å–æœ€æ–°å‚æ•°
    """
    
    def __init__(self, network, device='cpu'):
        self.network = network
        self.device = device
        self._state_dict = None
    
    def get_state_dict(self):
        """è·å–ç½‘ç»œå‚æ•°ï¼ˆç”¨äº Worker åŒæ­¥ï¼‰"""
        return {k: v.cpu().numpy() for k, v in self.network.state_dict().items()}
    
    def load_state_dict_from_numpy(self, numpy_dict):
        """ä» numpy å­—å…¸åŠ è½½å‚æ•°"""
        state_dict = {k: torch.from_numpy(v) for k, v in numpy_dict.items()}
        self.network.load_state_dict(state_dict)
        self.network = self.network.to(self.device)


def worker_process(
    worker_id,
    game_string,
    num_players,
    embedding_size,
    num_actions,
    advantage_network_layers,
    advantage_queues,  # æ¯ä¸ªç©å®¶ä¸€ä¸ªé˜Ÿåˆ—
    strategy_queue,
    network_params_queue,  # æ¥æ”¶æœ€æ–°ç½‘ç»œå‚æ•°
    stop_event,
    iteration_counter,
    num_traversals_per_batch,
    device='cpu',
    max_memory_gb=None,  # Worker å†…å­˜é™åˆ¶
    parent_pid=None,  # ä¸»è¿›ç¨‹PIDï¼Œç”¨äºæ£€æŸ¥ä¸»è¿›ç¨‹æ˜¯å¦å­˜æ´»
    # ğŸ§¯ advantage/regret ç›®æ ‡ç¨³å¥åŒ–å‚æ•°ï¼ˆä¸»è¿›ç¨‹ä¼ å…¥ï¼‰
    advantage_target_scale: float = 1.0,           # å½’ä¸€åŒ–å°ºåº¦ï¼ˆé€šå¸¸=max_stackï¼‰
    advantage_target_clip: Optional[float] = 5.0   # å½’ä¸€åŒ–åè£å‰ªé˜ˆå€¼ï¼ˆNoneè¡¨ç¤ºä¸è£å‰ªï¼‰
):
    """Worker è¿›ç¨‹ï¼šå¹¶è¡Œéå†æ¸¸æˆæ ‘
    
    Args:
        worker_id: Worker ID
        game_string: æ¸¸æˆé…ç½®å­—ç¬¦ä¸²
        num_players: ç©å®¶æ•°é‡
        embedding_size: ä¿¡æ¯çŠ¶æ€ç»´åº¦
        num_actions: åŠ¨ä½œæ•°é‡
        advantage_network_layers: ä¼˜åŠ¿ç½‘ç»œå±‚é…ç½®
        advantage_queues: ä¼˜åŠ¿æ ·æœ¬é˜Ÿåˆ—ï¼ˆæ¯ä¸ªç©å®¶ä¸€ä¸ªï¼‰
        strategy_queue: ç­–ç•¥æ ·æœ¬é˜Ÿåˆ—
        network_params_queue: ç½‘ç»œå‚æ•°é˜Ÿåˆ—
        stop_event: åœæ­¢ä¿¡å·
        iteration_counter: å½“å‰è¿­ä»£è®¡æ•°å™¨
        num_traversals_per_batch: æ¯æ‰¹éå†æ¬¡æ•°
        device: è®¡ç®—è®¾å¤‡
    """
    # è®¾ç½®è¿›ç¨‹åç§°
    try:
        import setproctitle
        setproctitle.setproctitle(f"deepcfr_worker_{worker_id}")
    except ImportError:
        pass
    except:
        pass
    
    # è®¾ç½®å†…å­˜é™åˆ¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if max_memory_gb:
        try:
            max_bytes = int(max_memory_gb * 1024 * 1024 * 1024)
            # è®¾ç½®è™šæ‹Ÿå†…å­˜é™åˆ¶ï¼ˆRLIMIT_ASï¼‰
            resource.setrlimit(resource.RLIMIT_AS, (max_bytes, max_bytes))
            print(f"[Worker {worker_id}] å·²è®¾ç½®å†…å­˜é™åˆ¶: {max_memory_gb}GB")
        except (ValueError, OSError) as e:
            print(f"[Worker {worker_id}] âš ï¸ æ— æ³•è®¾ç½®å†…å­˜é™åˆ¶: {e}")
    
    # è®¾ç½®å¼‚å¸¸å¤„ç†
    try:
        # è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
        if HAS_PSUTIL:
            try:
                process = psutil.Process()
                mem_info = process.memory_info()
                mem_mb = mem_info.rss / 1024 / 1024
                print(f"[Worker {worker_id}] å¯åŠ¨ï¼Œè®¾å¤‡: {device}ï¼Œåˆå§‹å†…å­˜: {mem_mb:.1f}MB")
            except:
                print(f"[Worker {worker_id}] å¯åŠ¨ï¼Œè®¾å¤‡: {device}")
        else:
            print(f"[Worker {worker_id}] å¯åŠ¨ï¼Œè®¾å¤‡: {device}")
        
        # åˆ›å»ºæ¸¸æˆ
        try:
            game = pyspiel.load_game(game_string)
            root_node = game.new_initial_state()
        except Exception as e:
            print(f"[Worker {worker_id}] âŒ åˆ›å»ºæ¸¸æˆå¤±è´¥: {e}")
            raise
        
        # åˆ›å»ºæœ¬åœ°ä¼˜åŠ¿ç½‘ç»œï¼ˆç”¨äºé‡‡æ ·åŠ¨ä½œï¼‰
        advantage_networks = []
        try:
            for _ in range(num_players):
                # ä»æ¸¸æˆå­—ç¬¦ä¸²ä¸­è§£æmax_stack
                import re
                game_string = str(game)
                match = re.search(r'stack=([\d\s]+)', game_string)
                max_stack = 2000  # é»˜è®¤å€¼
                if match:
                    stack_str = match.group(1).strip()
                    stack_values = stack_str.split()
                    if stack_values:
                        try:
                            max_stack = int(stack_values[0])
                        except ValueError:
                            pass
                
                net = SimpleFeatureMLP(
                    embedding_size,
                    list(advantage_network_layers),
                    num_actions,
                    num_players=num_players,
                    max_game_length=game.max_game_length(),
                    max_stack=max_stack
                )
                net = net.to(device)
                net.eval()
                advantage_networks.append(net)
        except Exception as e:
            print(f"[Worker {worker_id}] âŒ åˆ›å»ºä¼˜åŠ¿ç½‘ç»œå¤±è´¥: {e}")
            raise
        
        def sample_action_from_advantage(state, player):
            """ä½¿ç”¨ä¼˜åŠ¿ç½‘ç»œé‡‡æ ·åŠ¨ä½œï¼ˆRegret Matchingï¼‰
            
            æ³¨æ„ï¼šä¼˜åŠ¿ç½‘ç»œåœ¨åˆå§‹åŒ–æ—¶åˆ›å»ºï¼Œä¹‹åä¼šæŒç»­å­¦ä¹ ï¼ˆé»˜è®¤ä¸ä¼šé‡æ–°åˆå§‹åŒ–ï¼‰ã€‚
            å³ä½¿ç½‘ç»œæƒé‡åœ¨åˆå§‹æ—¶æ˜¯éšæœºçš„ï¼Œä¹Ÿä¼šä½¿ç”¨å®ƒè¿›è¡Œé¢„æµ‹ï¼Œè¿™ä¸åŸå§‹open_spielçš„å®ç°ä¸€è‡´ã€‚
            
            Args:
                state: æ¸¸æˆçŠ¶æ€
                player: ç©å®¶ID
            
            Returns:
                advantages: ä¼˜åŠ¿å€¼åˆ—è¡¨
                matched_regrets: ç­–ç•¥æ¦‚ç‡åˆ†å¸ƒ
            """
            info_state = state.information_state_tensor(player)
            legal_actions = state.legal_actions(player)
            
            # æ€»æ˜¯ä½¿ç”¨ä¼˜åŠ¿ç½‘ç»œï¼ˆå³ä½¿æƒé‡æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼‰
            # è¿™ä¸åŸå§‹open_spielçš„å®ç°ä¸€è‡´
            with torch.no_grad():
                state_tensor = torch.FloatTensor(np.expand_dims(info_state, axis=0)).to(device)
                raw_advantages = advantage_networks[player](state_tensor)[0].cpu().numpy()
            
            advantages = [max(0., advantage) for advantage in raw_advantages]
            cumulative_regret = sum(advantages[action] for action in legal_actions)
            
            matched_regrets = np.array([0.] * num_actions)
            if cumulative_regret > 0.:
                for action in legal_actions:
                    matched_regrets[action] = advantages[action] / cumulative_regret
            else:
                matched_regrets[max(legal_actions, key=lambda a: raw_advantages[a])] = 1
            
            return advantages, matched_regrets
        
        def traverse_game_tree(state, player, iteration):
            """éå†æ¸¸æˆæ ‘ï¼Œæ”¶é›†æ ·æœ¬
            
            Args:
                state: æ¸¸æˆçŠ¶æ€
                player: ç©å®¶ID
                iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            """
            nonlocal local_strategy_batch
            if state.is_terminal():
                return state.returns()[player]
            
            if state.is_chance_node():
                chance_outcome, chance_proba = zip(*state.chance_outcomes())
                action = np.random.choice(chance_outcome, p=chance_proba)
                return traverse_game_tree(state.child(action), player, iteration)
            
            if state.current_player() == player:
                expected_payoff = {}
                sampled_regret = {}
                
                _, strategy = sample_action_from_advantage(state, player)
                
                for action in state.legal_actions():
                    expected_payoff[action] = traverse_game_tree(
                        state.child(action), player, iteration
                    )
                
                cfv = sum(strategy[a] * expected_payoff[a] for a in state.legal_actions())
                
                for action in state.legal_actions():
                    sampled_regret[action] = expected_payoff[action] - cfv
                
                sampled_regret_arr = [0] * num_actions
                for action in sampled_regret:
                    # ä¿®å¤ï¼šå­¦ä¹ å®Œæ•´çš„é—æ†¾å€¼ï¼ˆåŒ…æ‹¬è´Ÿå€¼ï¼‰ï¼Œä¸åŸå§‹OpenSpielä¸€è‡´
                    # ç†ç”±ï¼šä¼˜åŠ¿ç½‘ç»œéœ€è¦å­¦ä¹ å®Œæ•´çš„é—æ†¾å€¼ï¼Œé‡‡æ ·æ—¶å†æˆªæ–­ä¸º0ï¼ˆRegret Matchingæ ‡å‡†åšæ³•ï¼‰
                    # å¦‚æœåªå­¦ä¹ æ­£é—æ†¾å€¼ï¼ŒFoldç­‰åŠ¨ä½œçš„è´Ÿé—æ†¾å€¼ä¼šè¢«å¿½ç•¥ï¼Œå¯¼è‡´æ— æ³•å­¦ä¹ åˆ°çœŸå®ä»·å€¼
                    sampled_regret_arr[action] = sampled_regret[action]

                # ==========================
                # ğŸ§¯ Advantage/Regret ç›®æ ‡ç¨³å¥åŒ–
                # ==========================
                # regret/advantage ä»¥ç­¹ç å•ä½è®¡é€šå¸¸é‡å°¾ä¸”é‡çº§å¤§ï¼Œç›´æ¥ç”¨ MSE ä¼šå¯¼è‡´ loss æå¤§ä¸”è®­ç»ƒéœ‡è¡/å¹³å°ã€‚
                # æ¨èï¼šæŒ‰ max_stack å½’ä¸€åŒ–åˆ° O(1) çš„å°ºåº¦ï¼Œå¹¶å¯¹å½’ä¸€åŒ–åçš„ç›®æ ‡åšè£å‰ªã€‚
                try:
                    if advantage_target_scale and float(advantage_target_scale) > 0:
                        inv_scale = 1.0 / float(advantage_target_scale)
                        sampled_regret_arr = [float(x) * inv_scale for x in sampled_regret_arr]
                        if advantage_target_clip is not None:
                            c = float(advantage_target_clip)
                            if c > 0:
                                sampled_regret_arr = [max(-c, min(c, float(x))) for x in sampled_regret_arr]
                except Exception:
                    # å½’ä¸€åŒ–å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼ˆä½†ä¼šå›é€€ä¸ºåŸå§‹å°ºåº¦ï¼‰
                    pass
                
                # è°ƒè¯•è¾“å‡ºï¼šæ”¶é›†æ ·æœ¬æ•°æ®ï¼ˆåªæ”¶é›†å‰20ä¸ªæ ·æœ¬ï¼‰
                if hasattr(traverse_game_tree, '_debug_count'):
                    traverse_game_tree._debug_count += 1
                else:
                    traverse_game_tree._debug_count = 1
                
                if traverse_game_tree._debug_count <= 20:
                    try:
                        import json
                        info_state_vec = state.information_state_tensor()
                        if isinstance(info_state_vec, np.ndarray):
                            info_state_list = info_state_vec.tolist()[:10]
                        else:
                            info_state_list = list(info_state_vec)[:10]
                        
                        debug_data = {
                            'sample_id': traverse_game_tree._debug_count,
                            'info_state': info_state_list,  # åªä¿å­˜å‰10ç»´
                            'sampled_regret': {str(k): float(v) for k, v in sampled_regret.items()},
                            'sampled_regret_arr_old': [float(sampled_regret.get(a, 0)) for a in range(num_actions)],
                            'sampled_regret_arr_new': [float(x) for x in sampled_regret_arr],
                            'strategy': strategy.tolist() if hasattr(strategy, 'tolist') else list(strategy),
                            'cfv': float(cfv),
                            'expected_payoff': {str(k): float(v) for k, v in expected_payoff.items()},
                        }
                        debug_file = '/tmp/deepcfr_debug_samples.jsonl'
                        with open(debug_file, 'a') as f:
                            f.write(json.dumps(debug_data) + '\n')
                    except Exception as e:
                        pass  # å¿½ç•¥è°ƒè¯•è¾“å‡ºé”™è¯¯ï¼Œä¸å½±å“è®­ç»ƒ
                
                # å‘é€ä¼˜åŠ¿æ ·æœ¬
                sample = AdvantageMemory(
                    state.information_state_tensor(),
                    iteration,
                    sampled_regret_arr,
                    action
                )
                
                # --- ä¿®æ”¹å¼€å§‹ ---
                # ç´¯ç§¯åˆ°æ‰¹æ¬¡
                if player not in local_advantage_batches:
                    local_advantage_batches[player] = []
                local_advantage_batches[player].append(sample)
                
                # å¦‚æœæ‰¹æ¬¡æ»¡äº†ï¼Œå‘é€
                if len(local_advantage_batches[player]) >= batch_size_limit:
                    try:
                        advantage_queues[player].put(local_advantage_batches[player], timeout=0.01)
                        local_advantage_batches[player] = []  # æ¸…ç©ºæ‰¹æ¬¡
                    except queue.Full:
                        # é˜Ÿåˆ—æ»¡äº†ï¼Œå®ç°FIFOæ›¿æ¢ï¼šå…ˆä¸¢å¼ƒæœ€æ—§çš„æ ·æœ¬ï¼Œå†æ·»åŠ æ–°æ ·æœ¬
                        try:
                            # å°è¯•ä¸¢å¼ƒä¸€ä¸ªæ—§æ ·æœ¬ï¼ˆFIFOï¼‰
                            advantage_queues[player].get_nowait()
                            # ä¸¢å¼ƒæˆåŠŸåï¼Œå†å°è¯•putæ–°æ ·æœ¬
                            try:
                                advantage_queues[player].put(local_advantage_batches[player], timeout=0.01)
                                local_advantage_batches[player] = []  # æ¸…ç©ºæ‰¹æ¬¡
                            except queue.Full:
                                # å¦‚æœè¿˜æ˜¯æ»¡äº†ï¼Œè¯´æ˜é˜Ÿåˆ—å¤§å°å¯èƒ½æœ‰é—®é¢˜ï¼Œç›´æ¥ä¸¢å¼ƒæ–°æ ·æœ¬
                                pass
                        except queue.Empty:
                            # é˜Ÿåˆ—ä¸ºç©ºï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†å¤„ç†ä¸€ä¸‹ï¼‰
                            pass


                return cfv
            else:
                other_player = state.current_player()
                _, strategy = sample_action_from_advantage(state, other_player)
                
                probs = np.array(strategy)
                probs /= probs.sum()
                sampled_action = np.random.choice(range(num_actions), p=probs)
                
                sample = StrategyMemory(
                    state.information_state_tensor(other_player),
                    iteration,
                    strategy
                )
                
                # --- ä¿®æ”¹å¼€å§‹ ---
                # ç´¯ç§¯åˆ°æ‰¹æ¬¡
                local_strategy_batch.append(sample)
                
                # å¦‚æœæ‰¹æ¬¡æ»¡äº†ï¼Œå‘é€
                if len(local_strategy_batch) >= batch_size_limit:
                    try:
                        strategy_queue.put(local_strategy_batch, timeout=0.01)
                        local_strategy_batch = []  # æ¸…ç©ºæ‰¹æ¬¡
                    except queue.Full:
                        # é˜Ÿåˆ—æ»¡äº†ï¼Œå®ç°FIFOæ›¿æ¢ï¼šå…ˆä¸¢å¼ƒæœ€æ—§çš„æ ·æœ¬ï¼Œå†æ·»åŠ æ–°æ ·æœ¬
                        try:
                            # å°è¯•ä¸¢å¼ƒä¸€ä¸ªæ—§æ ·æœ¬ï¼ˆFIFOï¼‰
                            strategy_queue.get_nowait()
                            # ä¸¢å¼ƒæˆåŠŸåï¼Œå†å°è¯•putæ–°æ ·æœ¬
                            try:
                                strategy_queue.put(local_strategy_batch, timeout=0.01)
                                local_strategy_batch = []  # æ¸…ç©ºæ‰¹æ¬¡
                            except queue.Full:
                                # å¦‚æœè¿˜æ˜¯æ»¡äº†ï¼Œè¯´æ˜é˜Ÿåˆ—å¤§å°å¯èƒ½æœ‰é—®é¢˜ï¼Œç›´æ¥ä¸¢å¼ƒæ–°æ ·æœ¬
                                pass
                        except queue.Empty:
                            # é˜Ÿåˆ—ä¸ºç©ºï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†å¤„ç†ä¸€ä¸‹ï¼‰
                            pass
                
                return traverse_game_tree(state.child(sampled_action), player, iteration)
        
        # ä¸»å¾ªç¯
        last_sync_iteration = 0
        # æœ¬åœ°æ‰¹æ¬¡ç¼“å†²åŒºï¼ˆå‡å°‘ Queue.put è°ƒç”¨é¢‘ç‡ï¼‰
        local_advantage_batches = {}  # {player_id: [samples]}
        local_strategy_batch = []
        batch_size_limit = 200  # æ¯ç§¯ç´¯ 100 ä¸ªæ ·æœ¬å‘é€ä¸€æ¬¡
        
        # ä¸»è¿›ç¨‹å­˜æ´»æ£€æŸ¥ï¼šæ¯10æ¬¡å¾ªç¯æ£€æŸ¥ä¸€æ¬¡ï¼ˆé¿å…é¢‘ç¹æ£€æŸ¥å½±å“æ€§èƒ½ï¼‰
        parent_check_counter = 0
        parent_check_interval = 10
        
        def check_parent_alive():
            """æ£€æŸ¥ä¸»è¿›ç¨‹æ˜¯å¦å­˜æ´»"""
            if parent_pid is None:
                return True  # å¦‚æœæ²¡æœ‰ä¼ é€’parent_pidï¼Œè·³è¿‡æ£€æŸ¥
            
            try:
                # æ–¹æ³•1: ä½¿ç”¨os.getppid()æ£€æŸ¥çˆ¶è¿›ç¨‹ID
                # å¦‚æœçˆ¶è¿›ç¨‹æ˜¯init (PID 1)ï¼Œè¯´æ˜ä¸»è¿›ç¨‹å·²é€€å‡º
                ppid = os.getppid()
                if ppid == 1:
                    return False
                
                # æ–¹æ³•2: ä½¿ç”¨psutilæ£€æŸ¥æŒ‡å®šçš„ä¸»è¿›ç¨‹æ˜¯å¦å­˜æ´»
                if HAS_PSUTIL:
                    try:
                        parent_process = psutil.Process(parent_pid)
                        # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨ä¸”çŠ¶æ€ä¸æ˜¯zombie
                        if parent_process.status() == psutil.STATUS_ZOMBIE:
                            return False
                    except psutil.NoSuchProcess:
                        # ä¸»è¿›ç¨‹ä¸å­˜åœ¨
                        return False
                    except psutil.AccessDenied:
                        # æ— æ³•è®¿é—®ï¼Œå‡è®¾å­˜æ´»ï¼ˆå¯èƒ½æ˜¯æƒé™é—®é¢˜ï¼‰
                        pass
                
                return True
            except Exception:
                # æ£€æŸ¥å¤±è´¥ï¼Œå‡è®¾å­˜æ´»ï¼ˆé¿å…è¯¯æ€ï¼‰
                return True
        
        while not stop_event.is_set():
            # å®šæœŸæ£€æŸ¥ä¸»è¿›ç¨‹æ˜¯å¦å­˜æ´»
            parent_check_counter += 1
            if parent_check_counter >= parent_check_interval:
                parent_check_counter = 0
                if not check_parent_alive():
                    print(f"\n[Worker {worker_id}] æ£€æµ‹åˆ°ä¸»è¿›ç¨‹å·²é€€å‡ºï¼Œè‡ªåŠ¨é€€å‡º...")
                    stop_event.set()  # è®¾ç½®åœæ­¢äº‹ä»¶
                    break
            
            # å…³é”®ä¿®å¤ï¼šå…ˆåŒæ­¥ç½‘ç»œå‚æ•°ï¼Œå†è¯»å–è¿­ä»£è®¡æ•°å™¨
            # è¿™æ ·å¯ä»¥ç¡®ä¿Workerä½¿ç”¨çš„ç½‘ç»œå‚æ•°å’Œè¿­ä»£æ ‡è®°æ˜¯åŒ¹é…çš„
            network_synced = False
            try:
                while True:
                    params = network_params_queue.get_nowait()
                    for player in range(num_players):
                        if player in params:
                            numpy_dict = params[player]
                            state_dict = {k: torch.from_numpy(v) for k, v in numpy_dict.items()}
                            advantage_networks[player].load_state_dict(state_dict)
                            advantage_networks[player] = advantage_networks[player].to(device)
                    network_synced = True
                    last_sync_iteration = iteration_counter.value
            except queue.Empty:
                pass
            
            # éå†æ¸¸æˆæ ‘
            for player in range(num_players):
                for _ in range(num_traversals_per_batch):
                    if stop_event.is_set():
                        break
                    current_iteration = iteration_counter.value
                    traverse_game_tree(root_node.clone(), player, current_iteration)
                
            # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒºï¼šæ— è®ºæ˜¯å¦è¾¾åˆ° batch_limitï¼Œéƒ½å°†æ‰‹ä¸­çš„æ ·æœ¬å‘é€å‡ºå»
            # è¿™é˜²æ­¢äº†åœ¨å¤šç©å®¶æ¸¸æˆä¸­ï¼ŒæŸäº›ç©å®¶çš„æ ·æœ¬ç§¯ç´¯å¤ªæ…¢å¯¼è‡´çš„ä¸»è¿›ç¨‹é¥¥é¥¿
            for p in list(local_advantage_batches.keys()):
                batch = local_advantage_batches[p]
                if batch:
                    try:
                        advantage_queues[p].put(batch, timeout=0.01)
                    except queue.Full:
                        # é˜Ÿåˆ—æ»¡äº†ï¼Œå®ç°FIFOæ›¿æ¢ï¼šå…ˆä¸¢å¼ƒæœ€æ—§çš„æ ·æœ¬ï¼Œå†æ·»åŠ æ–°æ ·æœ¬
                        try:
                            # å°è¯•ä¸¢å¼ƒä¸€ä¸ªæ—§æ ·æœ¬ï¼ˆFIFOï¼‰
                            advantage_queues[p].get_nowait()
                            # ä¸¢å¼ƒæˆåŠŸåï¼Œå†å°è¯•putæ–°æ ·æœ¬
                            try:
                                advantage_queues[p].put(batch, timeout=0.01)
                            except queue.Full:
                                # å¦‚æœè¿˜æ˜¯æ»¡äº†ï¼Œè¯´æ˜é˜Ÿåˆ—å¤§å°å¯èƒ½æœ‰é—®é¢˜ï¼Œç›´æ¥ä¸¢å¼ƒæ–°æ ·æœ¬
                                pass
                        except queue.Empty:
                            # é˜Ÿåˆ—ä¸ºç©ºï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†å¤„ç†ä¸€ä¸‹ï¼‰
                            pass
                # æ¸…ç©ºè¯¥ç©å®¶çš„ç¼“å†²åŒº
                local_advantage_batches[p] = []
            
            if local_strategy_batch:
                try:
                    strategy_queue.put(local_strategy_batch, timeout=0.01)
                    local_strategy_batch = []
                except queue.Full:
                    # é˜Ÿåˆ—æ»¡äº†ï¼Œå®ç°FIFOæ›¿æ¢ï¼šå…ˆä¸¢å¼ƒæœ€æ—§çš„æ ·æœ¬ï¼Œå†æ·»åŠ æ–°æ ·æœ¬
                    try:
                        # å°è¯•ä¸¢å¼ƒä¸€ä¸ªæ—§æ ·æœ¬ï¼ˆFIFOï¼‰
                        strategy_queue.get_nowait()
                        # ä¸¢å¼ƒæˆåŠŸåï¼Œå†å°è¯•putæ–°æ ·æœ¬
                        try:
                            strategy_queue.put(local_strategy_batch, timeout=0.01)
                            local_strategy_batch = []
                        except queue.Full:
                            # å¦‚æœè¿˜æ˜¯æ»¡äº†ï¼Œè¯´æ˜é˜Ÿåˆ—å¤§å°å¯èƒ½æœ‰é—®é¢˜ï¼Œç›´æ¥ä¸¢å¼ƒæ–°æ ·æœ¬
                            pass
                    except queue.Empty:
                        # é˜Ÿåˆ—ä¸ºç©ºï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†å¤„ç†ä¸€ä¸‹ï¼‰
                        pass
        
    except KeyboardInterrupt:
        print(f"\n[Worker {worker_id}] æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œé€€å‡º...")
    except Exception as e:
        print(f"\n[Worker {worker_id}] å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        # ä¸é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è¿›ç¨‹æ­£å¸¸é€€å‡º
        # è¿™æ ·å¯ä»¥é¿å…å¼‚å¸¸ä¼ æ’­å¯¼è‡´å…¶ä»–é—®é¢˜
    finally:
        print(f"[Worker {worker_id}] åœæ­¢")


class ParallelDeepCFRSolver:
    """å¤šè¿›ç¨‹å¹¶è¡Œ DeepCFR æ±‚è§£å™¨
    
    ä½¿ç”¨å¤šä¸ª Worker è¿›ç¨‹å¹¶è¡Œéå†æ¸¸æˆæ ‘ï¼Œä¸»è¿›ç¨‹è®­ç»ƒç½‘ç»œã€‚
    """
    
    def __init__(
        self,
        game,
        num_workers=4,
        policy_network_layers=(128, 128),
        advantage_network_layers=(128, 128),
        num_iterations=100,
        num_traversals=20,
        learning_rate=1e-4,
        batch_size_advantage=2048,
        batch_size_strategy=2048,
        memory_capacity=1000000,
        strategy_memory_capacity=None,  # ç­–ç•¥ç½‘ç»œç¼“å†²åŒºå®¹é‡ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨memory_capacityï¼‰
        device='cuda',
        gpu_ids=None,  # å¤š GPU æ”¯æŒ
        sync_interval=1,  # æ¯å¤šå°‘æ¬¡è¿­ä»£åŒæ­¥ä¸€æ¬¡ç½‘ç»œå‚æ•°
        max_memory_gb=None,  # æœ€å¤§å†…å­˜é™åˆ¶ï¼ˆGBï¼‰ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
        queue_maxsize=50000,  # é˜Ÿåˆ—æœ€å¤§å¤§å°ï¼ˆé™ä½ä»¥å‡å°‘å†…å­˜å ç”¨ï¼‰
        new_sample_ratio=0.5,  # æ–°æ ·æœ¬å æ¯”ï¼ˆåˆ†å±‚åŠ æƒé‡‡æ ·ï¼Œé»˜è®¤0.5å³50%ï¼‰
        new_sample_window=0,   # æ–°æ ·æœ¬çª—å£ï¼ˆæœ€è¿‘Wè½®ç®—â€œæ–°â€ï¼‰ï¼›0è¡¨ç¤ºä»…å½“å‰è½®
        # åˆ‡æ¢æ¡ä»¶å‚æ•°
        switch_threshold_win_rate_strict=0.25,  # ä¸¥æ ¼èƒœç‡é˜ˆå€¼ï¼ˆ25%ï¼‰
        switch_threshold_win_rate_relaxed=0.20,  # å®½æ¾èƒœç‡é˜ˆå€¼ï¼ˆ20%ï¼‰
        switch_threshold_avg_return_strict=0.0,  # ä¸¥æ ¼å¹³å‡æ”¶ç›Šé˜ˆå€¼ï¼ˆ0 BBï¼‰
        switch_threshold_avg_return_relaxed=10.0,  # å®½æ¾å¹³å‡æ”¶ç›Šé˜ˆå€¼ï¼ˆ10 BBï¼‰
        switch_stable_iterations=10,  # ç¨³å®šæ€§æ£€æŸ¥çš„è¿­ä»£æ¬¡æ•°
        switch_win_rate_std=0.05,  # èƒœç‡æ ‡å‡†å·®é˜ˆå€¼ï¼ˆ5%ï¼‰
        switch_avg_return_std=10.0,  # å¹³å‡æ”¶ç›Šæ ‡å‡†å·®é˜ˆå€¼ï¼ˆ10 BBï¼‰
        transition_iterations=1000,  # è¿‡æ¸¡é˜¶æ®µçš„è¿­ä»£æ¬¡æ•°
        reinitialize_advantage_networks=False,  # æ˜¯å¦é‡æ–°åˆå§‹åŒ–ä¼˜åŠ¿ç½‘ç»œï¼ˆé»˜è®¤å…³é—­ï¼Œé¿å…æ¯æ¬¡è¿­ä»£é‡ç½®å¯¼è‡´å­¦ä¹ ä¸ç¨³å®šï¼‰
        advantage_network_train_steps=1,  # ä¼˜åŠ¿ç½‘ç»œè®­ç»ƒæ­¥æ•°ï¼ˆæ¯æ¬¡è¿­ä»£ï¼‰
        policy_network_train_steps=1,  # ç­–ç•¥ç½‘ç»œè®­ç»ƒæ­¥æ•°ï¼ˆæ¯æ¬¡è¿­ä»£ï¼‰
        # ğŸ§¯ advantage/regret è®­ç»ƒç¨³å¥åŒ–ï¼ˆå»ºè®®å¼€å¯ï¼‰
        advantage_target_scale: Optional[float] = None,   # None: è‡ªåŠ¨å– game.stack çš„æœ€å¤§å€¼
        advantage_target_clip: Optional[float] = None,    # å½’ä¸€åŒ–åè£å‰ªé˜ˆå€¼ï¼ˆNoneä¸è£å‰ªï¼Œé»˜è®¤ä¸è£å‰ªï¼‰
        advantage_loss: str = "mse",                    # "huber" æˆ– "mse"
        huber_delta: float = 1.0                          # Huber deltaï¼ˆå½’ä¸€åŒ–åå•ä½ï¼‰
    ):
        self.game = game
        self.num_workers = num_workers
        self.num_players = game.num_players()
        self.num_iterations = num_iterations
        self.num_traversals = num_traversals
        self.learning_rate = learning_rate
        self.batch_size_advantage = batch_size_advantage
        self.batch_size_strategy = batch_size_strategy
        self.memory_capacity = memory_capacity
        # ç­–ç•¥ç½‘ç»œç¼“å†²åŒºå®¹é‡ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨memory_capacityï¼‰
        self.strategy_memory_capacity = strategy_memory_capacity if strategy_memory_capacity is not None else memory_capacity
        self.sync_interval = sync_interval
        self.max_memory_gb = max_memory_gb
        self.queue_maxsize = queue_maxsize
        self.new_sample_ratio = new_sample_ratio  # æ–°æ ·æœ¬å æ¯”ï¼ˆåˆ†å±‚åŠ æƒé‡‡æ ·ï¼‰
        self.new_sample_window = new_sample_window  # æ–°æ ·æœ¬çª—å£ï¼ˆæœ€è¿‘Wè½®ï¼‰
        
        # å†…å­˜ç›‘æ§
        self._last_memory_check = 0
        self._memory_check_interval = 60  # æ¯60ç§’æ£€æŸ¥ä¸€æ¬¡å†…å­˜
        
        # ç¼“å†²åŒºæ¸…ç†å‚æ•°
        self._buffer_cleanup_threshold = 0.95  # ç¼“å†²åŒºæ¸…ç†é˜ˆå€¼ï¼š95%ï¼ˆæé«˜åˆ°95%ï¼Œé¿å…é¢‘ç¹æ¸…ç†ï¼‰
        self._buffer_keep_ratio = 0.75  # æ¸…ç†åä¿ç•™æ¯”ä¾‹ï¼š75%ï¼ˆåˆ é™¤25%ï¼Œå›ºå®šæ¯”ä¾‹ï¼‰
        
        # é˜Ÿåˆ—ç›‘æ§å’ŒåŠ¨æ€è°ƒæ•´
        self._queue_stats = {
            'last_queue_sizes': {},  # {queue_name: size}
            'queue_growth_rates': {},  # {queue_name: samples/sec}
            'last_check_time': time.time(),
            'collection_counts': {},  # {queue_name: count} ç”¨äºç»Ÿè®¡æ”¶é›†é€Ÿåº¦
        }
        self._adaptive_max_collect = {
            'base': 50000,  # åŸºç¡€å€¼ï¼ˆä»20000å¢åŠ åˆ°50000ï¼Œæé«˜æ¶ˆè´¹é€Ÿåº¦ï¼‰
            'min': 25000,   # æœ€å°å€¼ï¼ˆä»10000æé«˜åˆ°25000ï¼Œé¿å…max_collectè¿‡ä½å¯¼è‡´æ ·æœ¬æ”¶é›†å˜æ…¢ï¼‰
            'max': 200000,  # æœ€å¤§å€¼ï¼ˆä»100000å¢åŠ åˆ°200000ï¼Œå…è®¸æ›´å¿«æ¶ˆè´¹ï¼‰
            'current': 50000,  # å½“å‰å€¼
        }
        
        # å¤š GPU è®¾ç½®
        self.gpu_ids = gpu_ids
        self.use_multi_gpu = gpu_ids is not None and len(gpu_ids) > 1 and torch.cuda.is_available()
        
        if self.use_multi_gpu:
            self.device = torch.device(f"cuda:{gpu_ids[0]}")
            print(f"  å¤š GPU æ¨¡å¼: {gpu_ids}")
        else:
            self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # æ¸¸æˆä¿¡æ¯
        self._root_node = game.new_initial_state()
        self._embedding_size = len(self._root_node.information_state_tensor(0))
        self._num_actions = game.num_distinct_actions()
        
        # æ¸¸æˆå­—ç¬¦ä¸²ï¼ˆç”¨äº Worker åˆ›å»ºæ¸¸æˆï¼‰
        self._game_string = str(game)
        
        # ä»æ¸¸æˆé…ç½®ä¸­è§£æmax_stackï¼ˆç”¨äºå½’ä¸€åŒ–ä¸‹æ³¨ç»Ÿè®¡ç‰¹å¾ï¼‰
        self._max_stack = self._parse_max_stack_from_game_string(self._game_string)

        # ğŸ§¯ advantage/regret ç›®æ ‡ç¨³å¥åŒ–é…ç½®
        self._advantage_target_scale = float(advantage_target_scale) if advantage_target_scale is not None else float(self._max_stack)
        if self._advantage_target_scale <= 0:
            self._advantage_target_scale = 1.0
        self._advantage_target_clip = advantage_target_clip
        self._advantage_loss_type = str(advantage_loss).lower()
        self._huber_delta = float(huber_delta) if huber_delta is not None else 1.0
        if self._huber_delta <= 0:
            self._huber_delta = 1.0
        
        # ç½‘ç»œå±‚é…ç½®
        self._policy_network_layers = policy_network_layers
        self._advantage_network_layers = advantage_network_layers
        
        # åˆ›å»ºç½‘ç»œ
        self._create_networks()
        
        # å¤šè¿›ç¨‹ç»„ä»¶
        self._manager = None
        self._workers = []
        self._advantage_queues = []
        self._strategy_queue = None
        self._network_params_queues = []
        self._stop_event = None
        self._iteration_counter = None
        
        # æœ¬åœ°ç¼“å†²åŒºï¼ˆä½¿ç”¨éšæœºæ›¿æ¢ç­–ç•¥ï¼ŒNumPy ä¼˜åŒ–ç‰ˆï¼‰
        # ä¼˜åŒ–ï¼šä½¿ç”¨ NumPy æ•°ç»„å­˜å‚¨ï¼Œé¿å… Python å¯¹è±¡è®¿é—®ç“¶é¢ˆ
        self._advantage_memories = [
            RandomReplacementBuffer(
                memory_capacity, 
                info_state_size=self._embedding_size,
                num_actions=self._num_actions,
                buffer_type='advantage'
            ) for _ in range(self.num_players)
        ]
        self._strategy_memories = RandomReplacementBuffer(
            self.strategy_memory_capacity,
            info_state_size=self._embedding_size,
            num_actions=self._num_actions,
            buffer_type='strategy'
        )

        self._iteration = 1
        
        # åˆ‡æ¢æ¡ä»¶å‚æ•°
        self.expected_win_rate_random = 1.0 / self.num_players  # éšæœºç­–ç•¥æœŸæœ›èƒœç‡
        self.switch_threshold_win_rate_strict = switch_threshold_win_rate_strict
        self.switch_threshold_win_rate_relaxed = switch_threshold_win_rate_relaxed
        self.switch_threshold_avg_return_strict = switch_threshold_avg_return_strict
        self.switch_threshold_avg_return_relaxed = switch_threshold_avg_return_relaxed
        self.switch_stable_iterations = switch_stable_iterations
        self.switch_win_rate_std = switch_win_rate_std
        self.switch_avg_return_std = switch_avg_return_std
        self.transition_iterations = transition_iterations
        
        # åˆ‡æ¢çŠ¶æ€ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰
        self.switch_start_iteration = None
        self.win_rate_history = []
        self.avg_return_history = []
        
        # ä¼˜åŠ¿ç½‘ç»œé‡æ–°åˆå§‹åŒ–ï¼ˆä¸å•è¿›ç¨‹ä¸€è‡´ï¼‰
        self._reinitialize_advantage_networks = reinitialize_advantage_networks
        
        # ç½‘ç»œè®­ç»ƒæ­¥æ•°
        self._advantage_network_train_steps = advantage_network_train_steps
        self._policy_network_train_steps = policy_network_train_steps
    
    def _parse_max_stack_from_game_string(self, game_string):
        """ä»æ¸¸æˆå­—ç¬¦ä¸²ä¸­è§£æmax_stackå€¼
        
        Args:
            game_string: æ¸¸æˆé…ç½®å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "universal_poker(...,stack=2000 2000 2000,...)"
        
        Returns:
            max_stack: å•ä¸ªç©å®¶çš„æœ€å¤§ç­¹ç é‡ï¼ˆé»˜è®¤2000ï¼‰
        """
        import re
        # åŒ¹é… stack=åé¢çš„å€¼
        match = re.search(r'stack=([\d\s]+)', game_string)
        if match:
            stack_str = match.group(1).strip()
            stack_values = stack_str.split()
            if stack_values:
                try:
                    # å–æœ€å¤§å€¼ï¼Œå…¼å®¹ä¸ç­‰æ·±åº¦çš„é…ç½®
                    max_stack = max(int(x) for x in stack_values)
                    return max_stack
                except ValueError:
                    pass
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼2000
        return 2000
    
    def _create_networks(self):
        """åˆ›å»ºç¥ç»ç½‘ç»œ"""
        # ç­–ç•¥ç½‘ç»œ
        policy_net = SimpleFeatureMLP(
            self._embedding_size,
            list(self._policy_network_layers),
            self._num_actions,
            num_players=self.num_players,
            max_game_length=self.game.max_game_length(),
            max_stack=self._max_stack
        )
        
        # éªŒè¯ç­–ç•¥ç½‘ç»œè¾“å…¥ç»´åº¦
        actual_input_size = policy_net.mlp.model[0]._weight.shape[1]
        expected_input_size = self._embedding_size + 27  # 23ç»´æ‰‹åŠ¨ç‰¹å¾ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰
        assert actual_input_size == expected_input_size, \
            f"ç­–ç•¥ç½‘ç»œè¾“å…¥ç»´åº¦é”™è¯¯: æœŸæœ› {expected_input_size}ï¼Œå®é™… {actual_input_size}"
        
        # å¤š GPU åˆ†é…ä¼˜åŒ–ï¼šå¦‚æœGPUæ•°é‡ > ç©å®¶æ•°é‡ï¼Œç­–ç•¥ç½‘ç»œå¯ä»¥åˆ†é…åˆ°é¢å¤–çš„GPU
        # è¿™æ ·å¯ä»¥ä¸ä¼˜åŠ¿ç½‘ç»œå¹¶è¡Œè®­ç»ƒï¼Œæé«˜è®­ç»ƒé€Ÿåº¦
        if self.use_multi_gpu and len(self.gpu_ids) > self.num_players:
            # GPUæ•°é‡å……è¶³ï¼Œç­–ç•¥ç½‘ç»œåˆ†é…åˆ°é¢å¤–çš„GPUï¼ˆæœ€åä¸€ä¸ªGPUï¼‰
            policy_gpu_id = self.gpu_ids[-1]
            self._policy_network = policy_net.to(torch.device(f"cuda:{policy_gpu_id}"))
            get_logger().info(f"  ç­–ç•¥ç½‘ç»œåˆ†é…åˆ° GPU {policy_gpu_id}ï¼ˆä¸ä¼˜åŠ¿ç½‘ç»œå¹¶è¡Œè®­ç»ƒï¼‰")
        elif self.use_multi_gpu:
            # GPUæ•°é‡ä¸è¶³ï¼Œä½¿ç”¨DataParallelåœ¨å¤šä¸ªGPUä¸Šå¹¶è¡Œè®­ç»ƒ
            self._policy_network = nn.DataParallel(policy_net, device_ids=self.gpu_ids)
            self._policy_network = self._policy_network.to(self.device)
        else:
            self._policy_network = policy_net.to(self.device)
        
        self._policy_sm = nn.Softmax(dim=-1)
        self._loss_policy = nn.MSELoss(reduction="mean")
        self._optimizer_policy = torch.optim.Adam(
            self._policy_network.parameters(), lr=self.learning_rate
        )
        
        # ä¼˜åŠ¿ç½‘ç»œï¼ˆæ¯ä¸ªç©å®¶ä¸€ä¸ªï¼‰
        self._advantage_networks = []
        self._optimizer_advantages = []
        for player in range(self.num_players):
            net = SimpleFeatureMLP(
                self._embedding_size,
                list(self._advantage_network_layers),
                self._num_actions,
                num_players=self.num_players,
                max_game_length=self.game.max_game_length(),
                max_stack=self._max_stack
            )
            
            # éªŒè¯ä¼˜åŠ¿ç½‘ç»œè¾“å…¥ç»´åº¦
            actual_input_size = net.mlp.model[0]._weight.shape[1]
            expected_input_size = self._embedding_size + 27  # 23ç»´æ‰‹åŠ¨ç‰¹å¾ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰
            assert actual_input_size == expected_input_size, \
                f"ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œè¾“å…¥ç»´åº¦é”™è¯¯: æœŸæœ› {expected_input_size}ï¼Œå®é™… {actual_input_size}"
            
            # å¤š GPU åˆ†é…ä¼˜åŒ–ï¼šå¦‚æœGPUæ•°é‡ >= ç©å®¶æ•°é‡ï¼Œå°†ä¸åŒä¼˜åŠ¿ç½‘ç»œåˆ†é…åˆ°ä¸åŒGPU
            # å¦åˆ™ä½¿ç”¨DataParallelåœ¨å¤šä¸ªGPUä¸Šå¹¶è¡Œè®­ç»ƒå•ä¸ªç½‘ç»œ
            if self.use_multi_gpu and len(self.gpu_ids) >= self.num_players:
                # GPUæ•°é‡å……è¶³ï¼Œæ¯ä¸ªç©å®¶åˆ†é…åˆ°ä¸åŒçš„GPUï¼ˆçœŸæ­£çš„å¹¶è¡Œï¼‰
                gpu_id = self.gpu_ids[player % len(self.gpu_ids)]
                net = net.to(torch.device(f"cuda:{gpu_id}"))
                get_logger().info(f"  ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œåˆ†é…åˆ° GPU {gpu_id}")
            elif self.use_multi_gpu:
                # GPUæ•°é‡ä¸è¶³ï¼Œä½¿ç”¨DataParallelåœ¨å¤šä¸ªGPUä¸Šå¹¶è¡Œè®­ç»ƒ
                # æ³¨æ„ï¼šDataParallelåœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹å¯èƒ½æ­»é”ï¼Œéœ€è¦å°å¿ƒä½¿ç”¨
                # å¦‚æœGPUæ•°é‡ < ç©å®¶æ•°é‡ï¼Œæ‰€æœ‰ç½‘ç»œå…±äº«GPUï¼Œå¤šçº¿ç¨‹è®­ç»ƒå¯èƒ½å¯¼è‡´æ­»é”
                # è§£å†³æ–¹æ¡ˆï¼šä¸ä½¿ç”¨DataParallelï¼Œè€Œæ˜¯å°†ç½‘ç»œåˆ†é…åˆ°ä¸åŒçš„GPUï¼ˆå¾ªç¯åˆ†é…ï¼‰
                # è¿™æ ·å¯ä»¥é¿å…å¤šçº¿ç¨‹æ­»é”ï¼ŒåŒæ—¶å……åˆ†åˆ©ç”¨å¤šGPUèµ„æº
                gpu_id = self.gpu_ids[player % len(self.gpu_ids)]
                net = net.to(torch.device(f"cuda:{gpu_id}"))
                get_logger().info(f"  ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œåˆ†é…åˆ° GPU {gpu_id}ï¼ˆå¾ªç¯åˆ†é…ï¼Œé¿å…DataParallelæ­»é”ï¼‰")
            else:
                net = net.to(self.device)
            
            self._advantage_networks.append(net)
            self._optimizer_advantages.append(
                torch.optim.Adam(net.parameters(), lr=self.learning_rate)
            )
        
        # ä¼˜åŠ¿ç½‘ç»œæŸå¤±ï¼šé»˜è®¤ç”¨ Huberï¼ˆå¯¹é‡å°¾ç›®æ ‡æ›´ç¨³ï¼‰ï¼Œå¯é€‰ MSE
        self._loss_advantages = self._build_advantage_loss()
    
    def _start_workers(self):
        """å¯åŠ¨ Worker è¿›ç¨‹"""
        mp.set_start_method('spawn', force=True)
        
        self._manager = Manager()
        self._stop_event = Event()
        self._iteration_counter = Value('i', 1)
        
        # åˆ›å»ºé˜Ÿåˆ—ï¼ˆé™ä½ maxsize ä»¥å‡å°‘å†…å­˜å ç”¨ï¼‰
        self._advantage_queues = [Queue(maxsize=self.queue_maxsize) for _ in range(self.num_players)]
        self._strategy_queue = Queue(maxsize=self.queue_maxsize)
        self._network_params_queues = [Queue(maxsize=10) for _ in range(self.num_workers)]
        
        # è®¡ç®—æ¯ä¸ª Worker çš„éå†æ¬¡æ•°
        # å…³é”®ä¿®æ­£ï¼šä¸å†ä¸€æ¬¡æ€§åˆ†é… huge numberï¼Œè€Œæ˜¯åˆ†é…ä¸€ä¸ªå°æ‰¹æ¬¡ï¼Œè®© Worker å¿«é€Ÿå“åº”
        # ä¸»è¿›ç¨‹ä¼šé€šè¿‡æ§åˆ¶æ”¶é›†æ ·æœ¬çš„æ•°é‡æ¥ä¿è¯æ€»éå†æ¬¡æ•°
        traversals_per_worker = 100  # æ¯ä¸ª Worker æ¯æ¬¡åªè·‘ 10 æ¬¡éå†ï¼Œç„¶åæ£€æŸ¥åŒæ­¥
        
        # è·å–ä¸»è¿›ç¨‹PIDï¼Œä¼ é€’ç»™workerç”¨äºå­˜æ´»æ£€æŸ¥
        import os
        main_process_pid = os.getpid()
        
        # å¯åŠ¨ Worker
        print(f"  æ­£åœ¨å¹³æ»‘å¯åŠ¨ {self.num_workers} ä¸ª Worker (æ¯ç»„5ä¸ª)...")
        for i in range(self.num_workers):
            p = Process(
                target=worker_process,
                args=(
                    i,
                    self._game_string,
                    self.num_players,
                    self._embedding_size,
                    self._num_actions,
                    self._advantage_network_layers,
                    self._advantage_queues,
                    self._strategy_queue,
                    self._network_params_queues[i],
                    self._stop_event,
                    self._iteration_counter,
                    traversals_per_worker,
                    'cpu',  # Worker åœ¨ CPU ä¸Šè¿è¡Œ
                    self.max_memory_gb,  # Worker å†…å­˜é™åˆ¶
                    main_process_pid,  # ä¸»è¿›ç¨‹PIDï¼Œç”¨äºæ£€æŸ¥ä¸»è¿›ç¨‹æ˜¯å¦å­˜æ´»
                    self._advantage_target_scale,
                    self._advantage_target_clip,
                ),
                daemon=True  # è®¾ç½®ä¸ºå®ˆæŠ¤è¿›ç¨‹ï¼Œä¸»è¿›ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨æ€æ­»
            )
            p.start()
            self._workers.append(p)
            
            # å¹³æ»‘å¯åŠ¨ï¼šæ¯å¯åŠ¨ 5 ä¸ª Worker ç¨å¾®æš‚åœä¸€ä¸‹ï¼Œé¿å…ç¬é—´ IO/CPU æ‹¥å µ
            if (i + 1) % 5 == 0:
                print(f"    å·²å¯åŠ¨ {i + 1}/{self.num_workers}...", end="\r", flush=True)
                time.sleep(1)
        
        print(f"\n  âœ“ å·²å¯åŠ¨ {self.num_workers} ä¸ª Worker è¿›ç¨‹")
        
        # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
        if HAS_PSUTIL:
            try:
                process = psutil.Process()
                mem_info = process.memory_info()
                mem_mb = mem_info.rss / 1024 / 1024
                print(f"  ä¸»è¿›ç¨‹å†…å­˜ä½¿ç”¨: {mem_mb:.1f}MB")
                
                # ä¼°ç®—æ€»å†…å­˜éœ€æ±‚
                # å®é™…å†…å­˜å ç”¨åŒ…æ‹¬ï¼š
                # 1. æ ·æœ¬æ•°æ®æœ¬èº«ï¼šinfo_state (numpyæ•°ç»„ï¼Œå¯èƒ½å‡ ç™¾åˆ°å‡ åƒä¸ªfloat32) + iteration + advantage/strategy
                # 2. Python å¯¹è±¡å¼€é”€ï¼šnamedtupleã€listã€numpyæ•°ç»„å¯¹è±¡ç­‰
                # 3. é˜Ÿåˆ—ç§¯å‹ï¼šå¦‚æœ Worker äº§ç”Ÿé€Ÿåº¦ > æ¶ˆè´¹é€Ÿåº¦
                # 4. Worker è¿›ç¨‹ï¼šæ¯ä¸ª Worker çš„ç½‘ç»œå‰¯æœ¬ã€æ¸¸æˆçŠ¶æ€ç­‰
                # 
                # ä¿å®ˆä¼°ç®—ï¼šæ¯ä¸ªæ ·æœ¬çº¦ 5-10KBï¼ˆåŒ…æ‹¬ Python å¯¹è±¡å¼€é”€ï¼‰
                # å¯¹äº6äººå±€ï¼Œmemory_capacity=1,000,000ï¼š
                #   - ä¼˜åŠ¿æ ·æœ¬ï¼š6 Ã— 1,000,000 Ã— 5KB = 30GB
                #   - ç­–ç•¥æ ·æœ¬ï¼š1 Ã— strategy_memory_capacity Ã— 5KB
                #   - æ€»è®¡ï¼šçº¦ 35GBï¼ˆä¸åŒ…æ‹¬é˜Ÿåˆ—å’Œ Workerï¼‰
                sample_size_kb = 5  # ä¿å®ˆä¼°ç®—ï¼Œå®é™…å¯èƒ½æ›´å¤§
                estimated_memory_gb = (
                    self.memory_capacity * self.num_players * sample_size_kb +  # ä¼˜åŠ¿æ ·æœ¬
                    self.strategy_memory_capacity * sample_size_kb +  # ç­–ç•¥æ ·æœ¬
                    self.queue_maxsize * (self.num_players + 1) * sample_size_kb +  # é˜Ÿåˆ—ç§¯å‹ï¼ˆæœ€åæƒ…å†µï¼‰
                    self.num_workers * 500  # Worker è¿›ç¨‹å¼€é”€ï¼ˆæ¯ä¸ª Worker çº¦ 500MBï¼‰
                ) / 1024 / 1024
                print(f"  ä¼°ç®—æ€»å†…å­˜éœ€æ±‚: {estimated_memory_gb:.2f}GB")
                print(f"    - ä¼˜åŠ¿æ ·æœ¬ç¼“å†²åŒº: {self.memory_capacity * self.num_players * sample_size_kb / 1024 / 1024:.2f}GB")
                print(f"    - ç­–ç•¥æ ·æœ¬ç¼“å†²åŒº: {self.strategy_memory_capacity * sample_size_kb / 1024 / 1024:.2f}GB")
                print(f"    - é˜Ÿåˆ—ç§¯å‹ï¼ˆæœ€åæƒ…å†µï¼‰: {self.queue_maxsize * (self.num_players + 1) * sample_size_kb / 1024 / 1024:.2f}GB")
                print(f"    - Worker è¿›ç¨‹: {self.num_workers * 500 / 1024 / 1024:.2f}GB")
                
                # è·å–ç³»ç»Ÿæ€»å†…å­˜
                try:
                    total_mem = psutil.virtual_memory().total / 1024 / 1024 / 1024
                    available_mem = psutil.virtual_memory().available / 1024 / 1024 / 1024
                    print(f"  ç³»ç»Ÿå†…å­˜: {total_mem:.1f}GB æ€»è®¡, {available_mem:.1f}GB å¯ç”¨")
                    
                    if estimated_memory_gb > available_mem * 0.8:
                        print(f"  âš ï¸ è­¦å‘Š: ä¼°ç®—å†…å­˜éœ€æ±‚ ({estimated_memory_gb:.1f}GB) æ¥è¿‘å¯ç”¨å†…å­˜ ({available_mem:.1f}GB)")
                        print(f"  å»ºè®®: å‡å°‘ --memory_capacity æˆ– --num_workers")
                except:
                    pass
            except Exception as e:
                print(f"  âš ï¸ è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
        else:
            print(f"  âš ï¸ æ— æ³•è·å–å†…å­˜ä¿¡æ¯ï¼ˆpsutil æœªå®‰è£…ï¼Œå»ºè®®å®‰è£…: pip install psutilï¼‰")
    
    def _stop_workers(self):
        """åœæ­¢ Worker è¿›ç¨‹"""
        self._stop_event.set()
        
        # æ¸…ç©ºæ‰€æœ‰é˜Ÿåˆ—ï¼Œé˜²æ­¢ Worker é˜»å¡åœ¨ put æ“ä½œ
        def drain_queue(q):
            try:
                while not q.empty():
                    q.get_nowait()
            except:
                pass
        
        for q in self._advantage_queues:
            drain_queue(q)
        drain_queue(self._strategy_queue)
        for q in self._network_params_queues:
            drain_queue(q)
        
        # ç­‰å¾… Worker é€€å‡º
        for p in self._workers:
            p.join(timeout=5)
            if p.is_alive():
                p.terminate()
                p.join(timeout=1)
        
        self._workers = []
        print("æ‰€æœ‰ Worker å·²åœæ­¢")
    
    def reinitialize_advantage_network(self, player):
        """é‡æ–°åˆå§‹åŒ–ä¼˜åŠ¿ç½‘ç»œï¼ˆä¸å•è¿›ç¨‹ä¸€è‡´ï¼‰"""
        net = self._advantage_networks[player]
        # å¤„ç† DataParallel åŒ…è£…
        if isinstance(net, nn.DataParallel):
            # DataParallelåŒ…è£…çš„ç½‘ç»œï¼Œè°ƒç”¨resetæ–¹æ³•
            net.module.reset()
        else:
            # ç›´æ¥è°ƒç”¨resetæ–¹æ³•ï¼ˆSimpleFeatureMLPæœ‰resetæ–¹æ³•ï¼‰
            net.reset()
        # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
        self._optimizer_advantages[player] = torch.optim.Adam(
            self._advantage_networks[player].parameters(), lr=self.learning_rate)
    
    def _sync_network_params(self):
        """åŒæ­¥ç½‘ç»œå‚æ•°åˆ°æ‰€æœ‰ Workerï¼ˆå§‹ç»ˆåŒæ­¥ï¼Œç§»é™¤ä¸¤é˜¶æ®µæœºåˆ¶ï¼‰"""
        sync_start_time = time.time()
        
        params_start = time.time()
        params = {}
        for player in range(self.num_players):
            # å¤„ç† DataParallel åŒ…è£…
            net = self._advantage_networks[player]
            if isinstance(net, nn.DataParallel):
                state_dict = net.module.state_dict()
            else:
                state_dict = net.state_dict()
            
            params[player] = {
                k: v.cpu().numpy() 
                for k, v in state_dict.items()
            }
        params_time = time.time() - params_start
        
        queue_start = time.time()
        for q in self._network_params_queues:
            try:
                q.put_nowait(params)
            except queue.Full:
                pass
        queue_time = time.time() - queue_start
        
        total_sync_time = time.time() - sync_start_time
        get_logger().info(f"  ç½‘ç»œå‚æ•°åŒæ­¥è€—æ—¶: {total_sync_time*1000:.1f}ms (å‚æ•°æå–: {params_time*1000:.1f}ms, é˜Ÿåˆ—å‘é€: {queue_time*1000:.1f}ms)")
    
    def _cleanup_buffers(self, force=False):
        """æ¸…ç†ç¼“å†²åŒºï¼ˆå·²ç¦ç”¨ï¼‰
        
        ç”±äºä½¿ç”¨éšæœºæ›¿æ¢ç­–ç•¥ï¼Œç¼“å†²åŒºæ»¡äº†ä¹‹åæ–°æ ·æœ¬ä¼šè‡ªåŠ¨éšæœºæ›¿æ¢æ—§æ ·æœ¬ï¼Œ
        å› æ­¤ä¸éœ€è¦ä¸»åŠ¨æ¸…ç†ç¼“å†²åŒºã€‚æ­¤æ–¹æ³•å·²ç¦ç”¨ï¼Œä»…ä¿ç•™æ¥å£ä»¥ä¿æŒå…¼å®¹æ€§ã€‚
        
        Args:
            force: å¼ºåˆ¶æ¸…ç†ï¼Œå³ä½¿å†…å­˜ä½¿ç”¨ä¸é«˜ï¼ˆå·²å¿½ç•¥ï¼‰
        """
        # æ¸…ç†é€»è¾‘å·²ç§»é™¤ï¼Œå®Œå…¨ä¾èµ–éšæœºæ›¿æ¢ç­–ç•¥
        return
    
    def _check_and_cleanup_memory(self, force=False, cleanup_buffers=True):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆé˜Ÿåˆ—æ¸…ç†å·²ç§»é™¤ï¼‰
        
        æ³¨æ„ï¼š
        1. é˜Ÿåˆ—æ¸…ç†å·²å®Œå…¨ç§»é™¤ï¼Œå› ä¸ºWorkerè¿›ç¨‹å·²ç»å®ç°äº†FIFOæ›¿æ¢
        2. FIFOæ›¿æ¢ï¼šå½“é˜Ÿåˆ—æ»¡äº†æ—¶ï¼ŒWorkerè¿›ç¨‹ä¼šå…ˆgetä¸€ä¸ªæ—§æ ·æœ¬ï¼ˆä¸¢å¼ƒï¼‰ï¼Œå†putæ–°æ ·æœ¬
        3. é˜Ÿåˆ—å¤§å°ä¿æŒç¨³å®šï¼ˆmaxsizeï¼‰ï¼Œä¸éœ€è¦ä¸»è¿›ç¨‹æ¸…ç†
        4. ç¼“å†²åŒºæ¸…ç†å·²ç¦ç”¨ï¼Œå®Œå…¨ä¾èµ–éšæœºæ›¿æ¢ç­–ç•¥
        
        Args:
            force: å¼ºåˆ¶æ¸…ç†ï¼Œå³ä½¿å†…å­˜ä½¿ç”¨ä¸é«˜ï¼ˆå·²å¿½ç•¥ï¼Œä»…ä¿ç•™æ¥å£å…¼å®¹æ€§ï¼‰
            cleanup_buffers: æ˜¯å¦æ¸…ç†ç¼“å†²åŒºï¼ˆå·²å¿½ç•¥ï¼Œç¼“å†²åŒºæ¸…ç†å·²ç¦ç”¨ï¼‰
        """
        # é˜Ÿåˆ—æ¸…ç†å·²å®Œå…¨ç§»é™¤ï¼Œå› ä¸ºWorkerè¿›ç¨‹å·²ç»å®ç°äº†FIFOæ›¿æ¢
        # FIFOæ›¿æ¢ï¼šå½“é˜Ÿåˆ—æ»¡äº†æ—¶ï¼ŒWorkerè¿›ç¨‹ä¼šå…ˆgetä¸€ä¸ªæ—§æ ·æœ¬ï¼ˆä¸¢å¼ƒï¼‰ï¼Œå†putæ–°æ ·æœ¬
        # é˜Ÿåˆ—å¤§å°ä¿æŒç¨³å®šï¼ˆmaxsizeï¼‰ï¼Œä¸éœ€è¦ä¸»è¿›ç¨‹æ¸…ç†
        
        # ç¼“å†²åŒºæ¸…ç†å·²ç¦ç”¨ï¼Œå®Œå…¨ä¾èµ–éšæœºæ›¿æ¢ç­–ç•¥
        # if cleanup_buffers:
        #     self._cleanup_buffers(force=force)
    
    def _update_adaptive_max_collect(self):
        """åŠ¨æ€è°ƒæ•´max_collectï¼Œæ ¹æ®é˜Ÿåˆ—ç§¯å‹æƒ…å†µå’ŒCPUä½¿ç”¨ç‡è‡ªé€‚åº”è°ƒæ•´æ¶ˆè´¹é€Ÿåº¦
        
        åˆ¤æ–­é€»è¾‘ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
        1. **é˜Ÿåˆ—ä½¿ç”¨ç‡**ï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰ï¼šé˜Ÿåˆ—ç§¯å‹æƒ…å†µï¼Œåæ˜ æ¶ˆè´¹é€Ÿåº¦æ˜¯å¦è¶³å¤Ÿ
           - ä½¿ç”¨ç‡ > 80%ï¼šæ¶ˆè´¹é€Ÿåº¦ä¸å¤Ÿï¼Œéœ€è¦å¢åŠ max_collect
           - ä½¿ç”¨ç‡ < 30%ï¼šæ¶ˆè´¹é€Ÿåº¦è¿‡å¿«ï¼Œå¯ä»¥å‡å°‘max_collectèŠ‚çœCPU
        2. **é˜Ÿåˆ—å¢é•¿ç‡**ï¼ˆæ¬¡è¦æŒ‡æ ‡ï¼‰ï¼šWorkeräº§ç”Ÿé€Ÿåº¦ï¼Œåæ˜ æ˜¯å¦éœ€è¦æ›´å¿«æ¶ˆè´¹
           - å¢é•¿ç‡ > 100æ ·æœ¬/ç§’ï¼šé˜Ÿåˆ—åœ¨å¿«é€Ÿå¢é•¿ï¼Œéœ€è¦å¢åŠ æ¶ˆè´¹é€Ÿåº¦
        3. **CPUä½¿ç”¨ç‡**ï¼ˆé™åˆ¶æŒ‡æ ‡ï¼‰ï¼šä¸»è¿›ç¨‹CPUä½¿ç”¨æƒ…å†µï¼Œé¿å…è¿‡åº¦æ¶ˆè´¹å¯¼è‡´CPUè¿‡è½½
           - CPUä½¿ç”¨ç‡ > 90%ï¼šå‡å°‘max_collectï¼Œé¿å…CPUè¿‡è½½
           - CPUä½¿ç”¨ç‡ < 50%ï¼šå¯ä»¥é€‚å½“å¢åŠ max_collect
        
        max_collectçš„èŒƒå›´ï¼š[min, max]ï¼Œå…¶ä¸­max = queue_maxsize * 0.5ï¼ˆé¿å…ä¸€æ¬¡æ€§æ¶ˆè´¹å¤ªå¤šï¼‰
        """
        current_time = time.time()
        stats = self._queue_stats
        
        # è®¡ç®—æ‰€æœ‰é˜Ÿåˆ—çš„æœ€å¤§ä½¿ç”¨ç‡
        max_queue_usage = 0.0
        total_queue_size = 0
        total_queue_capacity = 0
        
        for player in range(self.num_players):
            q_size = self._advantage_queues[player].qsize()
            usage = q_size / self.queue_maxsize if self.queue_maxsize > 0 else 0
            max_queue_usage = max(max_queue_usage, usage)
            total_queue_size += q_size
            total_queue_capacity += self.queue_maxsize
        
        strategy_q_size = self._strategy_queue.qsize()
        strategy_usage = strategy_q_size / self.queue_maxsize if self.queue_maxsize > 0 else 0
        max_queue_usage = max(max_queue_usage, strategy_usage)
        total_queue_size += strategy_q_size
        total_queue_capacity += self.queue_maxsize
        
        # è®¡ç®—é˜Ÿåˆ—å¢é•¿ç‡ï¼ˆå¦‚æœé˜Ÿåˆ—å¤§å°åœ¨å¢é•¿ï¼‰
        time_delta = current_time - stats['last_check_time']
        if time_delta > 1.0:  # è‡³å°‘é—´éš”1ç§’æ‰æ›´æ–°
            # æ›´æ–°å¢é•¿ç‡
            for player in range(self.num_players):
                queue_name = f'advantage_{player}'
                current_size = self._advantage_queues[player].qsize()
                if queue_name in stats['last_queue_sizes']:
                    old_size = stats['last_queue_sizes'][queue_name]
                    growth = (current_size - old_size) / time_delta
                    stats['queue_growth_rates'][queue_name] = growth
                stats['last_queue_sizes'][queue_name] = current_size
            
            # ç­–ç•¥é˜Ÿåˆ—
            queue_name = 'strategy'
            current_size = self._strategy_queue.qsize()
            if queue_name in stats['last_queue_sizes']:
                old_size = stats['last_queue_sizes'][queue_name]
                growth = (current_size - old_size) / time_delta
                stats['queue_growth_rates'][queue_name] = growth
            stats['last_queue_sizes'][queue_name] = current_size
            
            stats['last_check_time'] = current_time
        
        # è®¡ç®—å¹³å‡é˜Ÿåˆ—å¢é•¿ç‡
        avg_growth_rate = 0.0
        if stats['queue_growth_rates']:
            avg_growth_rate = sum(stats['queue_growth_rates'].values()) / len(stats['queue_growth_rates'])
        
        # è·å–CPUä½¿ç”¨ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        # ä¿®å¤ï¼šä½¿ç”¨éé˜»å¡æ–¹å¼è·å–CPUä½¿ç”¨ç‡ï¼Œé¿å…é˜»å¡
        cpu_percent = None
        if HAS_PSUTIL:
            try:
                process = psutil.Process()
                # ä½¿ç”¨interval=Noneï¼Œéé˜»å¡è·å–ï¼ˆè¿”å›ä¸Šæ¬¡è°ƒç”¨åçš„å¹³å‡å€¼ï¼‰
                cpu_percent = process.cpu_percent(interval=None)
                # å¦‚æœè·å–å¤±è´¥ï¼Œå°è¯•è·å–ç³»ç»ŸCPUä½¿ç”¨ç‡
                if cpu_percent is None or cpu_percent == 0:
                    cpu_percent = psutil.cpu_percent(interval=None)
            except:
                pass
        
        # åŠ¨æ€è°ƒæ•´max_collect
        adaptive = self._adaptive_max_collect
        base = adaptive['base']
        min_val = adaptive['min']
        # ä¼˜åŒ–ï¼šå½“é˜Ÿåˆ—ä½¿ç”¨ç‡å¾ˆé«˜æ—¶ï¼Œå…è®¸æ›´é«˜çš„max_collect
        # é˜Ÿåˆ—ä½¿ç”¨ç‡100%æ—¶ï¼Œå…è®¸ä¸€æ¬¡æ€§æ¶ˆè´¹é˜Ÿåˆ—å¤§å°çš„10å€ï¼Œå¿«é€Ÿæ¸…ç©ºé˜Ÿåˆ—
        if max_queue_usage >= 0.99:
            # é˜Ÿåˆ—å‡ ä¹æ»¡æ—¶ï¼Œå…è®¸æ›´é«˜çš„max_collectï¼ˆé˜Ÿåˆ—å¤§å°çš„10å€ï¼‰
            max_val = min(adaptive['max'], max(self.queue_maxsize * 10, 50000))
        else:
            # æ­£å¸¸æƒ…å†µï¼Œè‡³å°‘é˜Ÿåˆ—å¤§å°çš„4å€ï¼Œæˆ–20000
            max_val = min(adaptive['max'], max(self.queue_maxsize * 4, 20000))
        
        # 1. æ ¹æ®é˜Ÿåˆ—ä½¿ç”¨ç‡è°ƒæ•´ï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
        # ä¼˜åŒ–ï¼šå½“é˜Ÿåˆ—ä½¿ç”¨ç‡100%æ—¶ï¼Œå¤§å¹…å¢åŠ æ¶ˆè´¹é€Ÿåº¦ï¼ˆæœ€å¤š10å€ï¼‰ï¼Œå¿«é€Ÿæ¸…ç©ºé˜Ÿåˆ—
        if max_queue_usage >= 0.99:
            # é˜Ÿåˆ—ä½¿ç”¨ç‡ >= 99%ï¼ˆå‡ ä¹æ»¡ï¼‰ï¼Œå¤§å¹…å¢åŠ æ¶ˆè´¹é€Ÿåº¦
            factor = 1.0 + (max_queue_usage - 0.80) * 20.0  # æœ€å¤šå¢åŠ åˆ°10å€ï¼ˆ0.99æ—¶çº¦3.8å€ï¼Œ1.0æ—¶10å€ï¼‰
            new_max_collect = int(base * factor)
        elif max_queue_usage > 0.80:
            # é˜Ÿåˆ—ä½¿ç”¨ç‡ > 80%ï¼Œå¢åŠ æ¶ˆè´¹é€Ÿåº¦
            factor = 1.0 + (max_queue_usage - 0.80) * 5.0  # æœ€å¤šå¢åŠ åˆ°2å€ï¼ˆ0.80æ—¶1å€ï¼Œ1.0æ—¶2å€ï¼‰
            new_max_collect = int(base * factor)
        elif max_queue_usage < 0.30:
            # é˜Ÿåˆ—ä½¿ç”¨ç‡ < 30%ï¼Œå‡å°‘æ¶ˆè´¹é€Ÿåº¦ï¼ˆèŠ‚çœCPUï¼‰
            factor = 0.5 + (max_queue_usage / 0.30) * 0.5  # æœ€å°‘å‡å°‘åˆ°0.5å€
            new_max_collect = int(base * factor)
        else:
            # æ­£å¸¸èŒƒå›´ï¼Œä¿æŒåŸºç¡€å€¼
            new_max_collect = base
        
        # 2. æ ¹æ®é˜Ÿåˆ—å¢é•¿ç‡è°ƒæ•´ï¼ˆæ¬¡è¦æŒ‡æ ‡ï¼‰
        # ä¼˜åŒ–ï¼šé™ä½å¢é•¿ç‡é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿåœ°å“åº”é˜Ÿåˆ—å¢é•¿
        if avg_growth_rate > 50:  # æ¯ç§’å¢é•¿è¶…è¿‡50ä¸ªæ ·æœ¬ï¼ˆä»100é™ä½åˆ°50ï¼‰
            growth_factor = min(2.0, 1.0 + avg_growth_rate / 500.0)  # æœ€å¤š2å€ï¼ˆä»1.5å€æé«˜åˆ°2å€ï¼‰
            new_max_collect = int(new_max_collect * growth_factor)
        
        # 3. æ ¹æ®CPUä½¿ç”¨ç‡è°ƒæ•´ï¼ˆé™åˆ¶æŒ‡æ ‡ï¼Œé¿å…CPUè¿‡è½½ï¼‰
        # ä¼˜åŒ–ï¼šå½“é˜Ÿåˆ—ä¸­æœ‰æ ·æœ¬æ—¶ï¼ˆä½¿ç”¨ç‡>5%ï¼‰ï¼Œä¼˜å…ˆæ¶ˆè´¹é˜Ÿåˆ—ï¼Œä¸å› CPUé«˜è€Œå‡é€Ÿ
        # è¿™é¿å…äº†"CPUé«˜â†’max_collectå‡åŠâ†’æ¶ˆè´¹å˜æ…¢â†’é˜Ÿåˆ—ç©ºâ†’å¿™ç­‰å¾…â†’CPUä»é«˜"çš„æ¶æ€§å¾ªç¯
        if cpu_percent is not None:
            if max_queue_usage > 0.05:
                # é˜Ÿåˆ—ä¸­æœ‰æ ·æœ¬ï¼ˆä½¿ç”¨ç‡>5%ï¼‰ï¼Œä¼˜å…ˆæ¶ˆè´¹ï¼Œä¸å‡å°‘max_collect
                # åªæœ‰åœ¨CPUæé«˜ï¼ˆ>98%ï¼‰æ—¶æ‰é€‚å½“é™åˆ¶
                if cpu_percent > 98:
                    cpu_factor = max(0.9, 1.0 - (cpu_percent - 98) / 20.0)  # æœ€å¤šå‡å°‘åˆ°0.9å€
                    new_max_collect = int(new_max_collect * cpu_factor)
                # å¦åˆ™ä¸é™åˆ¶ï¼Œä¼˜å…ˆæ¸…ç©ºé˜Ÿåˆ—
            elif cpu_percent > 95:
                # é˜Ÿåˆ—å‡ ä¹ç©ºï¼ˆ<5%ï¼‰ä¸”CPUæé«˜ï¼Œé€‚å½“å‡å°‘æ¶ˆè´¹é€Ÿåº¦ï¼ˆé¿å…å¿™ç­‰å¾…ï¼‰
                cpu_factor = max(0.7, 1.0 - (cpu_percent - 95) / 15.0)  # æœ€å¤šå‡å°‘åˆ°0.7å€
                new_max_collect = int(new_max_collect * cpu_factor)
            elif cpu_percent < 50 and max_queue_usage > 0.30:
                # CPUä½¿ç”¨ç‡ < 50% ä¸”é˜Ÿåˆ—ä½¿ç”¨ç‡ > 30%ï¼Œå¯ä»¥é€‚å½“å¢åŠ æ¶ˆè´¹é€Ÿåº¦
                cpu_factor = min(1.2, 1.0 + (50 - cpu_percent) / 100.0)  # æœ€å¤šå¢åŠ 1.2å€
                new_max_collect = int(new_max_collect * cpu_factor)
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        new_max_collect = max(min_val, min(max_val, new_max_collect))
        adaptive['current'] = new_max_collect
        
        # è®°å½•è°ƒæ•´åŸå› ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        adaptive['last_adjustment'] = {
            'queue_usage': max_queue_usage,
            'growth_rate': avg_growth_rate,
            'cpu_percent': cpu_percent,
            'final_value': new_max_collect,
        }
        
        return new_max_collect
    
    def _collect_samples(self, timeout=0.1, current_iteration=None):
        """ä»é˜Ÿåˆ—æ”¶é›†æ ·æœ¬
        
        å…³é”®ä¿®å¤ï¼š
        1. åªæ¸…ç†é˜Ÿåˆ—ç§¯å‹ï¼ˆä¸å½±å“æ ·æœ¬æ”¶é›†è¿›åº¦ï¼‰
        2. ç¼“å†²åŒºæ¸…ç†åœ¨æ ·æœ¬æ”¶é›†å®Œæˆåæ‰§è¡Œï¼Œé¿å…åˆ é™¤æ­£åœ¨æ”¶é›†çš„æ ·æœ¬
        3. åŠ¨æ€è°ƒæ•´max_collectï¼Œæ ¹æ®é˜Ÿåˆ—ç§¯å‹æƒ…å†µè‡ªé€‚åº”è°ƒæ•´æ¶ˆè´¹é€Ÿåº¦
        4. åœ¨æ ·æœ¬æ·»åŠ åˆ°ç¼“å†²åŒºæ—¶ï¼Œç”±ä¸»è¿›ç¨‹æ ‡è®°æ ·æœ¬çš„è¿­ä»£æ¬¡æ•°ï¼ˆä¸ä¾èµ–Workerè¿›ç¨‹è¯»å–iteration_counterï¼‰
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
            current_iteration: å½“å‰è¿­ä»£æ¬¡æ•°ï¼ˆç”¨äºæ ‡è®°æ–°æ ·æœ¬ï¼‰
        
        Returns:
            total_collected: æœ¬æ¬¡æ”¶é›†çš„æ€»æ ·æœ¬æ•°ï¼ˆä¼˜åŠ¿æ ·æœ¬ + ç­–ç•¥æ ·æœ¬ï¼‰
        """
        collect_start_time = time.time()
        
        # é˜Ÿåˆ—æ¸…ç†å·²å®Œå…¨ç§»é™¤ï¼Œå› ä¸ºWorkerè¿›ç¨‹å·²ç»å®ç°äº†FIFOæ›¿æ¢
        # FIFOæ›¿æ¢ï¼šå½“é˜Ÿåˆ—æ»¡äº†æ—¶ï¼ŒWorkerè¿›ç¨‹ä¼šå…ˆgetä¸€ä¸ªæ—§æ ·æœ¬ï¼ˆä¸¢å¼ƒï¼‰ï¼Œå†putæ–°æ ·æœ¬
        # é˜Ÿåˆ—å¤§å°ä¿æŒç¨³å®šï¼ˆmaxsizeï¼‰ï¼Œä¸éœ€è¦ä¸»è¿›ç¨‹æ¸…ç†
        cleanup_time = 0.0
        
        # åŠ¨æ€è°ƒæ•´max_collect
        update_start = time.time()
        max_collect = self._update_adaptive_max_collect()
        update_time = time.time() - update_start
        
        # æ£€æŸ¥ Worker çŠ¶æ€ï¼ˆå¢å¼ºç‰ˆï¼šæ£€æŸ¥é€€å‡ºç ï¼‰
        dead_workers = []
        for i, p in enumerate(self._workers):
            if not p.is_alive():
                exit_code = p.exitcode
                dead_workers.append((i, exit_code))
        
        if dead_workers:
            worker_info = ", ".join([f"Worker {wid} (é€€å‡ºç : {ec})" for wid, ec in dead_workers])
            error_msg = f"æ£€æµ‹åˆ° Worker å·²æ­»äº¡: {worker_info}ã€‚è®­ç»ƒæ— æ³•ç»§ç»­ã€‚"
            # å¦‚æœé€€å‡ºç ä¸ä¸º0ï¼Œå¯èƒ½æ˜¯OOMæˆ–å…¶ä»–ä¸¥é‡é”™è¯¯
            for _, exit_code in dead_workers:
                if exit_code != 0:
                    error_msg += f"\n  å¯èƒ½çš„é”™è¯¯åŸå› : é€€å‡ºç  {exit_code} é€šå¸¸è¡¨ç¤ºè¿›ç¨‹è¢«ç³»ç»Ÿæ€æ­»ï¼ˆå¦‚OOMï¼‰"
            raise RuntimeError(error_msg)

        # æ”¶é›†ä¼˜åŠ¿æ ·æœ¬ï¼ˆæ‰¹é‡å¤„ç†ï¼Œæé«˜æ•ˆç‡ï¼‰
        # æ³¨æ„ï¼šé˜Ÿåˆ—æ¸…ç†å·²å®Œå…¨ç§»é™¤ï¼Œå› ä¸ºWorkerè¿›ç¨‹å·²ç»å®ç°äº†FIFOæ›¿æ¢
        advantage_collect_start = time.time()
        total_collected_advantage = 0
        for player in range(self.num_players):
            collected_count = 0
            batch_list = []
            # å…ˆæ‰¹é‡è·å–ï¼Œå‡å°‘é”ç«äº‰
            while collected_count < max_collect:
                try:
                    batch = self._advantage_queues[player].get_nowait()
                    batch_list.append(batch)
                    if isinstance(batch, list):
                        collected_count += len(batch)
                    else:
                        collected_count += 1
                except queue.Empty:
                    break
            
            # æ‰¹é‡æ·»åŠ åˆ°ç¼“å†²åŒºï¼ˆä¼˜åŒ–ï¼šå‡å°‘æ–¹æ³•è°ƒç”¨å¼€é”€ï¼‰
            # å…³é”®ä¿®å¤ï¼šåœ¨æ ·æœ¬æ·»åŠ åˆ°ç¼“å†²åŒºæ—¶ï¼Œç”±ä¸»è¿›ç¨‹æ ‡è®°æ ·æœ¬çš„è¿­ä»£æ¬¡æ•°
            # è¿™æ ·å°±ä¸ä¾èµ–Workerè¿›ç¨‹è¯»å–iteration_counterçš„å€¼äº†
            memory = self._advantage_memories[player]
            for batch in batch_list:
                if isinstance(batch, list):
                    # æ‰¹é‡æ·»åŠ ï¼Œå‡å°‘æ–¹æ³•è°ƒç”¨æ¬¡æ•°
                    for sample in batch:
                        # å¦‚æœæä¾›äº†current_iterationï¼Œæ›´æ–°æ ·æœ¬çš„iterationå­—æ®µ
                        if current_iteration is not None:
                            # åˆ›å»ºæ–°çš„æ ·æœ¬å¯¹è±¡ï¼Œæ›´æ–°iterationå­—æ®µ
                            updated_sample = AdvantageMemory(
                                sample.info_state,
                                current_iteration,  # ä½¿ç”¨ä¸»è¿›ç¨‹çš„current_iteration
                                sample.advantage,
                                sample.action
                            )
                            memory.add(updated_sample)
                        else:
                            memory.add(sample)
                    total_collected_advantage += len(batch)
                else:
                    # å•ä¸ªæ ·æœ¬
                    if current_iteration is not None:
                        updated_sample = AdvantageMemory(
                            batch.info_state,
                            current_iteration,  # ä½¿ç”¨ä¸»è¿›ç¨‹çš„current_iteration
                            batch.advantage,
                            batch.action
                        )
                        memory.add(updated_sample)
                    else:
                        memory.add(batch)
                    total_collected_advantage += 1
        advantage_collect_time = time.time() - advantage_collect_start
        
        # æ”¶é›†ç­–ç•¥æ ·æœ¬ï¼ˆæ‰¹é‡å¤„ç†ï¼Œæé«˜æ•ˆç‡ï¼‰
        # æ³¨æ„ï¼šé˜Ÿåˆ—æ¸…ç†å·²å®Œå…¨ç§»é™¤ï¼Œå› ä¸ºWorkerè¿›ç¨‹å·²ç»å®ç°äº†FIFOæ›¿æ¢
        strategy_collect_start = time.time()
        collected_count = 0
        batch_list = []
        # å…ˆæ‰¹é‡è·å–ï¼Œå‡å°‘é”ç«äº‰
        while collected_count < max_collect:
            try:
                batch = self._strategy_queue.get_nowait()
                batch_list.append(batch)
                if isinstance(batch, list):
                    collected_count += len(batch)
                else:
                    collected_count += 1
            except queue.Empty:
                break
        
        # æ‰¹é‡æ·»åŠ åˆ°ç¼“å†²åŒºï¼ˆä¼˜åŒ–ï¼šå‡å°‘æ–¹æ³•è°ƒç”¨å¼€é”€ï¼‰
        # å…³é”®ä¿®å¤ï¼šåœ¨æ ·æœ¬æ·»åŠ åˆ°ç¼“å†²åŒºæ—¶ï¼Œç”±ä¸»è¿›ç¨‹æ ‡è®°æ ·æœ¬çš„è¿­ä»£æ¬¡æ•°
        # è¿™æ ·å°±ä¸ä¾èµ–Workerè¿›ç¨‹è¯»å–iteration_counterçš„å€¼äº†
        total_collected_strategy = 0
        memory = self._strategy_memories
        for batch in batch_list:
            if isinstance(batch, list):
                # æ‰¹é‡æ·»åŠ ï¼Œå‡å°‘æ–¹æ³•è°ƒç”¨æ¬¡æ•°
                for sample in batch:
                    # å¦‚æœæä¾›äº†current_iterationï¼Œæ›´æ–°æ ·æœ¬çš„iterationå­—æ®µ
                    if current_iteration is not None:
                        # åˆ›å»ºæ–°çš„æ ·æœ¬å¯¹è±¡ï¼Œæ›´æ–°iterationå­—æ®µ
                        updated_sample = StrategyMemory(
                            sample.info_state,
                            current_iteration,  # ä½¿ç”¨ä¸»è¿›ç¨‹çš„current_iteration
                            sample.strategy_action_probs
                        )
                        memory.add(updated_sample)
                    else:
                        memory.add(sample)
                total_collected_strategy += len(batch)
            else:
                # å•ä¸ªæ ·æœ¬
                if current_iteration is not None:
                    updated_sample = StrategyMemory(
                        batch.info_state,
                        current_iteration,  # ä½¿ç”¨ä¸»è¿›ç¨‹çš„current_iteration
                        batch.strategy_action_probs
                    )
                    memory.add(updated_sample)
                else:
                    memory.add(batch)
                total_collected_strategy += 1
        strategy_collect_time = time.time() - strategy_collect_start
        
        # è®°å½•æ”¶é›†ç»Ÿè®¡ï¼ˆç”¨äºè°ƒè¯•å’Œç›‘æ§ï¼‰
        if total_collected_advantage > 0 or total_collected_strategy > 0:
            self._queue_stats['collection_counts']['advantage'] = total_collected_advantage
            self._queue_stats['collection_counts']['strategy'] = total_collected_strategy
        
        # è®°å½•è€—æ—¶
        total_collect_time = time.time() - collect_start_time
        total_collected = total_collected_advantage + total_collected_strategy
        # if total_collected_advantage > 0 or total_collected_strategy > 0:
        #     get_logger().info(f"  æ ·æœ¬æ”¶é›†è€—æ—¶: {total_collect_time*1000:.1f}ms (æ¸…ç†: {cleanup_time*1000:.1f}ms, è°ƒæ•´: {update_time*1000:.1f}ms, ä¼˜åŠ¿: {advantage_collect_time*1000:.1f}ms, ç­–ç•¥: {strategy_collect_time*1000:.1f}ms)")
        
        return total_collected
    
    def _learn_advantage_network(self, player, current_iteration=None):
        """è®­ç»ƒä¼˜åŠ¿ç½‘ç»œï¼ˆNumPy ä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            player: ç©å®¶ID
            current_iteration: å½“å‰è¿­ä»£æ¬¡æ•°ï¼ˆç”¨äºç»Ÿè®¡æ–°æ ·æœ¬æ¯”ä¾‹ï¼‰
        
        ä¼˜åŒ–ï¼šä½¿ç”¨ NumPy æ•°ç»„ç›´æ¥é‡‡æ ·ï¼Œé¿å… Python å¯¹è±¡éå†ç“¶é¢ˆã€‚
        æ•°æ®å‡†å¤‡æ—¶é—´ä» ~700ms é™åˆ° ~5msï¼ˆ140x åŠ é€Ÿï¼‰ã€‚
        """
        train_start_time = time.time()
        
        num_samples = len(self._advantage_memories[player])
        if num_samples < 32:  # æœ€å°‘éœ€è¦ 32 ä¸ªæ ·æœ¬æ‰è®­ç»ƒ
            return None
        
        # ä½¿ç”¨å®é™…æ ·æœ¬æ•°å’Œ batch_size çš„è¾ƒå°å€¼
        sample_start = time.time()
        actual_batch_size = min(num_samples, self.batch_size_advantage)
        
        # ä¼˜åŒ–ï¼šç›´æ¥è·å– NumPy æ•°ç»„ï¼Œæ— éœ€éå† Python å¯¹è±¡
        sample_result = self._advantage_memories[player].sample(
            actual_batch_size, 
            current_iteration=current_iteration,
            new_sample_ratio=self.new_sample_ratio,
            new_sample_window=self.new_sample_window
        )
        sample_time = time.time() - sample_start
        
        data_prep_start = time.time()
        
        # ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨ NumPy æ•°ç»„ï¼Œæ— éœ€éå†
        if isinstance(sample_result, dict):
            # æ–°æ¥å£ï¼šç›´æ¥è¿”å› NumPy æ•°ç»„
            info_states = sample_result['info_states']
            advantages = sample_result['advantages']
            iterations_arr = sample_result['iterations'].reshape(-1, 1).astype(np.float32)
            new_sample_count = sample_result['new_sample_count']
            num_samples_batch = len(info_states)
        else:
            # å…¼å®¹æ—§æ¥å£ï¼šsample_result æ˜¯ namedtuple åˆ—è¡¨
            samples = sample_result
            if len(samples) > 0:
                num_samples_batch = len(samples)
                expected_len = self._num_actions
                
                info_states = np.empty((num_samples_batch, len(samples[0].info_state)), dtype=np.float32)
                advantages = np.empty((num_samples_batch, expected_len), dtype=np.float32)
                iterations_arr = np.empty((num_samples_batch, 1), dtype=np.float32)
                
                new_sample_count = 0
                for i, s in enumerate(samples):
                    info_states[i] = s.info_state
                    adv = s.advantage
                    if len(adv) >= expected_len:
                        advantages[i] = adv[:expected_len]
                    else:
                        advantages[i, :len(adv)] = adv
                        advantages[i, len(adv):] = 0
                    iterations_arr[i, 0] = s.iteration
                if current_iteration is not None and hasattr(s, 'iteration'):
                    try:
                        w = int(self.new_sample_window) if self.new_sample_window is not None else 0
                    except Exception:
                        w = 0
                    w = max(0, w)
                    if w <= 0:
                        if s.iteration == current_iteration:
                            new_sample_count += 1
                    else:
                        if (s.iteration >= (current_iteration - w)) and (s.iteration <= current_iteration):
                            new_sample_count += 1
            else:
                info_states = np.array([], dtype=np.float32)
                advantages = np.array([], dtype=np.float32)
                iterations_arr = np.array([], dtype=np.float32).reshape(0, 1)
                new_sample_count = 0
                num_samples_batch = 0
        
        # sqrt(iteration)
        iterations = np.sqrt(iterations_arr)
        
        # ç»Ÿè®¡æ–°æ ·æœ¬å’Œè€æ ·æœ¬æ¯”ä¾‹
        if current_iteration is not None and num_samples_batch > 0:
            old_sample_count = num_samples_batch - new_sample_count
            new_ratio = new_sample_count / num_samples_batch * 100
            get_logger().info(f"    ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œè®­ç»ƒæ ·æœ¬: æ–°æ ·æœ¬ {new_sample_count}/{num_samples_batch} ({new_ratio:.1f}%), è€æ ·æœ¬ {old_sample_count}/{num_samples_batch} ({100-new_ratio:.1f}%)")
        
        data_prep_time = time.time() - data_prep_start
        
        # ä¿®å¤ï¼šå¦‚æœsamplesä¸ºç©ºï¼Œç›´æ¥è¿”å›Noneï¼Œé¿å…ç©ºtensorè®­ç»ƒ
        if num_samples_batch == 0:
            return None
        
        # ä¼˜åŒ–ï¼šè·å–ç½‘ç»œæ‰€åœ¨çš„è®¾å¤‡ï¼ˆæ”¯æŒå¤šGPUåˆ†é…ï¼‰
        network = self._advantage_networks[player]
        if isinstance(network, nn.DataParallel):
            # DataParallelåŒ…è£…çš„ç½‘ç»œï¼Œä½¿ç”¨ä¸»è®¾å¤‡
            device = next(network.parameters()).device
        else:
            # å•ä¸ªGPUä¸Šçš„ç½‘ç»œï¼Œä½¿ç”¨ç½‘ç»œæ‰€åœ¨çš„è®¾å¤‡
            device = next(network.parameters()).device
        
        # ä¼˜åŒ–ï¼šé¢„è®¡ç®—æ‰‹åŠ¨ç‰¹å¾ï¼Œé¿å…åœ¨ forward() ä¸­é‡å¤è®¡ç®—
        # è¿™æ˜¯å…³é”®ä¼˜åŒ–ï¼åŸæ¥æ¯æ¬¡ forward() éƒ½è¦éå†æ‰€æœ‰æ ·æœ¬è®¡ç®—ç‰¹å¾ï¼Œéå¸¸æ…¢
        feature_calc_start = time.time()
        info_states_tensor = torch.from_numpy(info_states).to(device)
        
        # é¢„è®¡ç®—ç‰¹å¾å¹¶æ‹¼æ¥åˆ°è¾“å…¥
        if hasattr(network, 'extract_manual_features'):
            # SimpleFeatureMLP éœ€è¦è®¡ç®—æ‰‹åŠ¨ç‰¹å¾
            with torch.no_grad():  # ç‰¹å¾è®¡ç®—ä¸éœ€è¦æ¢¯åº¦
                manual_features = network.extract_manual_features(info_states_tensor)
            # æ‹¼æ¥ç‰¹å¾åˆ°è¾“å…¥
            info_states_tensor = torch.cat([info_states_tensor, manual_features], dim=1)
        feature_calc_time = time.time() - feature_calc_start
        
        # åˆ›å»ºå…¶ä»– tensor
        advantages_tensor = torch.from_numpy(advantages).to(device)
        iters = torch.from_numpy(iterations).to(device)

        # è®­ç»ƒæœŸå¯è§‚æµ‹æ€§ï¼šæ‰“å° advantage label åˆ†å¸ƒï¼ˆç¡®è®¤å½’ä¸€åŒ–/è£å‰ªæ˜¯å¦ç”Ÿæ•ˆï¼‰
        if current_iteration is not None and current_iteration % 1000 == 0 and player == 0:
            try:
                flat = np.asarray(advantages, dtype=np.float32).reshape(-1)
                abs_flat = np.abs(flat)
                p50, p90, p99 = np.quantile(abs_flat, [0.5, 0.9, 0.99]).tolist()
                mx = float(abs_flat.max()) if abs_flat.size else 0.0
                get_logger().info(
                    f"  ğŸ§¯ advantage targets stats(abs) @iter={current_iteration}: "
                    f"p50={p50:.4f}, p90={p90:.4f}, p99={p99:.4f}, max={mx:.4f} | "
                    f"scale={self._advantage_target_scale:.1f}, clip={self._advantage_target_clip}, loss={self._advantage_loss_type}"
                )
            except Exception:
                pass
        
        # ä¿®å¤ï¼šDataParallelåœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹å¯èƒ½æ­»é”ï¼Œéœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡
        import os
        os.environ['OMP_NUM_THREADS'] = '1'
        os.environ['MKL_NUM_THREADS'] = '1'
        
        # å¦‚æœä½¿ç”¨DataParallelï¼Œå¯èƒ½éœ€è¦è®¾ç½®torch.set_num_threads
        if isinstance(network, nn.DataParallel):
            torch.set_num_threads(1)
        
        forward_backward_start = time.time()
        # å¤šæ¬¡è®­ç»ƒï¼ˆä¸åŸå§‹OpenSpielä¸€è‡´ï¼‰
        final_loss = None
        for step in range(self._advantage_network_train_steps):
            self._optimizer_advantages[player].zero_grad()
            try:
                outputs = network(info_states_tensor)
            except Exception as e:
                import traceback
                traceback.print_exc()
                raise
            loss = self._loss_advantages(iters * outputs, iters * advantages_tensor)
            
            # è°ƒè¯•è¾“å‡ºå·²ç¦ç”¨ï¼ˆå½±å“æ€§èƒ½ï¼‰
            
            loss.backward()
            self._optimizer_advantages[player].step()
            final_loss = loss  # ä¿å­˜æœ€åä¸€æ¬¡è®­ç»ƒçš„æŸå¤±
        
        # ä¼˜åŒ–ï¼šç¡®ä¿CUDAæ“ä½œå®Œæˆï¼Œé¿å…å¤šçº¿ç¨‹ç«äº‰
        # æ³¨æ„ï¼šsynchronizeä¸æ¥å—deviceå¯¹è±¡ï¼Œéœ€è¦è·å–è®¾å¤‡ç´¢å¼•
        if device.type == 'cuda':
            device_index = device.index if device.index is not None else 0
            torch.cuda.synchronize(device_index)
        forward_backward_time = time.time() - forward_backward_start
        
        total_train_time = time.time() - train_start_time
        # å¼€å¯è¯¦ç»†è€—æ—¶æ—¥å¿—ï¼ˆç”¨äºæ€§èƒ½è°ƒä¼˜ï¼‰
        if total_train_time > 1.0:  # åªè®°å½•è¶…è¿‡1ç§’çš„è®­ç»ƒ
            get_logger().warning(f"  âš ï¸ ç©å®¶ {player} è®­ç»ƒæ…¢: {total_train_time:.2f}ç§’ (é‡‡æ ·: {sample_time*1000:.0f}ms, æ•°æ®å‡†å¤‡: {data_prep_time*1000:.0f}ms, ç‰¹å¾è®¡ç®—: {feature_calc_time*1000:.0f}ms, å‰å‘åå‘: {forward_backward_time*1000:.0f}ms, æ ·æœ¬æ•°: {num_samples_batch})")
        
        # ä¿®å¤ï¼šç¡®ä¿è¿”å›æ ‡é‡ï¼ˆMSELossè¿”å›æ ‡é‡tensorï¼Œ.numpy()å¯èƒ½è¿”å›0ç»´æ•°ç»„ï¼‰
        if final_loss is not None:
            loss_value = final_loss.detach().cpu().numpy()
            return float(loss_value) if np.isscalar(loss_value) else float(loss_value.item())
        return None

    def _build_advantage_loss(self):
        """æ„å»ºä¼˜åŠ¿ç½‘ç»œæŸå¤±å‡½æ•°ï¼ˆé»˜è®¤ Huberï¼‰"""
        if self._advantage_loss_type == "mse":
            return nn.MSELoss(reduction="mean")

        delta = float(self._huber_delta)

        def huber(pred, target):
            diff = pred - target
            abs_diff = torch.abs(diff)
            quad = torch.clamp(abs_diff, max=delta)
            lin = abs_diff - quad
            loss = 0.5 * (quad ** 2) / delta + lin
            return loss.mean()

        return huber
    
    def _learn_strategy_network(self, current_iteration=None):
        """è®­ç»ƒç­–ç•¥ç½‘ç»œï¼ˆNumPy ä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            current_iteration: å½“å‰è¿­ä»£æ¬¡æ•°ï¼ˆç”¨äºç»Ÿè®¡æ–°æ ·æœ¬æ¯”ä¾‹ï¼‰
        
        ä¼˜åŒ–ï¼šä½¿ç”¨ NumPy æ•°ç»„ç›´æ¥é‡‡æ ·ï¼Œé¿å… Python å¯¹è±¡éå†ç“¶é¢ˆã€‚
        """
        train_start_time = time.time()
        
        num_samples = len(self._strategy_memories)
        if num_samples < 32:  # æœ€å°‘éœ€è¦ 32 ä¸ªæ ·æœ¬æ‰è®­ç»ƒ
            return None
        
        # ä½¿ç”¨å®é™…æ ·æœ¬æ•°å’Œ batch_size çš„è¾ƒå°å€¼
        sample_start = time.time()
        actual_batch_size = min(num_samples, self.batch_size_strategy)
        sample_result = self._strategy_memories.sample(
            actual_batch_size,
            current_iteration=current_iteration,
            new_sample_ratio=self.new_sample_ratio,
            new_sample_window=self.new_sample_window
        )
        sample_time = time.time() - sample_start
        
        data_prep_start = time.time()
        
        # ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨ NumPy æ•°ç»„ï¼Œæ— éœ€éå†
        if isinstance(sample_result, dict):
            # æ–°æ¥å£ï¼šç›´æ¥è¿”å› NumPy æ•°ç»„
            info_states = sample_result['info_states']
            action_probs = sample_result['action_probs']
            iterations_arr = sample_result['iterations'].astype(np.float32)
            new_sample_count = sample_result['new_sample_count']
            num_samples_batch = len(info_states)
            
            # sqrt(iteration)ï¼Œä¿æŒ 2D å½¢çŠ¶ [batch_size, 1]
            iterations = np.sqrt(iterations_arr.reshape(-1, 1))
            
            # ç»Ÿè®¡æ–°æ ·æœ¬å’Œè€æ ·æœ¬æ¯”ä¾‹
            if current_iteration is not None and num_samples_batch > 0:
                old_sample_count = num_samples_batch - new_sample_count
                new_ratio = new_sample_count / num_samples_batch * 100
                get_logger().info(f"    ç­–ç•¥ç½‘ç»œè®­ç»ƒæ ·æœ¬: æ–°æ ·æœ¬ {new_sample_count}/{num_samples_batch} ({new_ratio:.1f}%), è€æ ·æœ¬ {old_sample_count}/{num_samples_batch} ({100-new_ratio:.1f}%)")
        else:
            # å…¼å®¹æ—§æ¥å£ï¼šsample_result æ˜¯ namedtuple åˆ—è¡¨
            samples = sample_result
            
            # ç»Ÿè®¡æ–°æ ·æœ¬å’Œè€æ ·æœ¬æ¯”ä¾‹
            if current_iteration is not None and len(samples) > 0:
                try:
                    w = int(self.new_sample_window) if self.new_sample_window is not None else 0
                except Exception:
                    w = 0
                w = max(0, w)
                if w <= 0:
                    new_samples = sum(1 for s in samples if hasattr(s, 'iteration') and s.iteration == current_iteration)
                else:
                    low = current_iteration - w
                    new_samples = sum(1 for s in samples if hasattr(s, 'iteration') and (s.iteration >= low) and (s.iteration <= current_iteration))
                old_samples = len(samples) - new_samples
                new_ratio = new_samples / len(samples) * 100 if len(samples) > 0 else 0
                get_logger().info(f"    ç­–ç•¥ç½‘ç»œè®­ç»ƒæ ·æœ¬: æ–°æ ·æœ¬ {new_samples}/{len(samples)} ({new_ratio:.1f}%), è€æ ·æœ¬ {old_samples}/{len(samples)} ({100-new_ratio:.1f}%)")
            
            if len(samples) > 0:
                info_states = np.array([s.info_state for s in samples], dtype=np.float32)
                action_probs_list = [s.strategy_action_probs for s in samples]
                if len(action_probs_list) > 0:
                    first_probs = action_probs_list[0]
                    if isinstance(first_probs, (list, np.ndarray)):
                        expected_len = self._num_actions
                        action_probs = np.array([
                            np.array(probs, dtype=np.float32)[:expected_len] if len(probs) >= expected_len
                            else np.pad(np.array(probs, dtype=np.float32), (0, expected_len - len(probs)), 
                                       mode='constant', constant_values=0)
                            for probs in action_probs_list
                        ], dtype=np.float32)
                    else:
                        action_probs = np.array(action_probs_list, dtype=np.float32)
                        if action_probs.ndim == 1:
                            action_probs = action_probs[:, np.newaxis]
                    if action_probs.ndim > 2:
                        action_probs = np.squeeze(action_probs)
                else:
                    action_probs = np.array([], dtype=np.float32)
                iterations = np.sqrt(np.array([[s.iteration] for s in samples], dtype=np.float32))
                num_samples_batch = len(samples)
            else:
                info_states = np.array([], dtype=np.float32)
                action_probs = np.array([], dtype=np.float32)
                iterations = np.array([], dtype=np.float32).reshape(0, 1)
                num_samples_batch = 0
        
        data_prep_time = time.time() - data_prep_start
        
        # ä¿®å¤ï¼šå¦‚æœsamplesä¸ºç©ºï¼Œç›´æ¥è¿”å›Noneï¼Œé¿å…ç©ºtensorè®­ç»ƒ
        if num_samples_batch == 0:
            return None
        
        # ä¼˜åŒ–ï¼šè·å–ç­–ç•¥ç½‘ç»œæ‰€åœ¨çš„è®¾å¤‡ï¼ˆæ”¯æŒå¤šGPUåˆ†é…ï¼‰
        network = self._policy_network
        if isinstance(network, nn.DataParallel):
            # DataParallelåŒ…è£…çš„ç½‘ç»œï¼Œä½¿ç”¨ä¸»è®¾å¤‡
            device = next(network.parameters()).device
        else:
            # å•ä¸ªGPUä¸Šçš„ç½‘ç»œï¼Œä½¿ç”¨ç½‘ç»œæ‰€åœ¨çš„è®¾å¤‡
            device = next(network.parameters()).device
        
        # ä¼˜åŒ–ï¼šæ‰¹é‡åˆ›å»ºtensorï¼Œå‡å°‘CPU-GPUæ•°æ®ä¼ è¾“æ¬¡æ•°ï¼Œå¹¶åˆ†é…åˆ°æ­£ç¡®çš„è®¾å¤‡
        iters = torch.from_numpy(iterations).to(device)
        ac_probs = torch.from_numpy(action_probs).to(device)
        info_states_tensor = torch.from_numpy(info_states).to(device)
        
        forward_backward_start = time.time()
        # å¤šæ¬¡è®­ç»ƒï¼ˆä¸åŸå§‹OpenSpielä¸€è‡´ï¼‰
        final_loss = None
        for step in range(self._policy_network_train_steps):
            self._optimizer_policy.zero_grad()
            logits = network(info_states_tensor)
            outputs = self._policy_sm(logits)
            loss = self._loss_policy(iters * outputs, iters * ac_probs)
            loss.backward()
            self._optimizer_policy.step()
            final_loss = loss  # ä¿å­˜æœ€åä¸€æ¬¡è®­ç»ƒçš„æŸå¤±
        
        # ä¼˜åŒ–ï¼šç¡®ä¿CUDAæ“ä½œå®Œæˆï¼Œé¿å…å¤šçº¿ç¨‹ç«äº‰
        # æ³¨æ„ï¼šsynchronizeä¸æ¥å—deviceå¯¹è±¡ï¼Œéœ€è¦è·å–è®¾å¤‡ç´¢å¼•
        if device.type == 'cuda':
            device_index = device.index if device.index is not None else 0
            torch.cuda.synchronize(device_index)
        forward_backward_time = time.time() - forward_backward_start
        
        total_train_time = time.time() - train_start_time
        get_logger().info(f"  ç­–ç•¥ç½‘ç»œè®­ç»ƒè€—æ—¶: {total_train_time*1000:.1f}ms (é‡‡æ ·: {sample_time*1000:.1f}ms, æ•°æ®å‡†å¤‡: {data_prep_time*1000:.1f}ms, å‰å‘åå‘: {forward_backward_time*1000:.1f}ms)")
        
        # ä¿®å¤ï¼šç¡®ä¿è¿”å›æ ‡é‡ï¼ˆMSELossè¿”å›æ ‡é‡tensorï¼Œ.numpy()å¯èƒ½è¿”å›0ç»´æ•°ç»„ï¼‰
        if final_loss is not None:
            loss_value = final_loss.detach().cpu().numpy()
            return float(loss_value) if np.isscalar(loss_value) else float(loss_value.item())
        return None
    
    def should_start_transition(self, iteration, advantage_losses, win_rate=None, avg_return=None):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼€å§‹è¿‡æ¸¡é˜¶æ®µ
        
        Args:
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            advantage_losses: ä¼˜åŠ¿ç½‘ç»œæŸå¤±å€¼å†å²ï¼ˆdictï¼Œkeyä¸ºç©å®¶IDï¼‰
            win_rate: å½“å‰è¿­ä»£çš„èƒœç‡ï¼ˆvs Randomï¼‰
            avg_return: å½“å‰è¿­ä»£çš„å¹³å‡æ”¶ç›Šï¼ˆvs Randomï¼Œå•ä½ï¼šBBï¼‰
        
        Returns:
            bool: æ˜¯å¦åº”è¯¥å¼€å§‹è¿‡æ¸¡é˜¶æ®µ
        """
        if self.switch_start_iteration is not None:
            return False  # å·²ç»å¼€å§‹è¿‡æ¸¡é˜¶æ®µ
        
        if win_rate is None or avg_return is None:
            return False  # ç¼ºå°‘è¯„ä¼°æ•°æ®
        
        # è®°å½•å†å²
        self.win_rate_history.append(win_rate)
        self.avg_return_history.append(avg_return)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†å²
        if len(self.win_rate_history) < self.switch_stable_iterations:
            return False
        
        # æ£€æŸ¥èƒœç‡å’Œå¹³å‡æ”¶ç›Šæ¡ä»¶
        recent_win_rates = self.win_rate_history[-self.switch_stable_iterations:]
        recent_avg_returns = self.avg_return_history[-self.switch_stable_iterations:]
        
        avg_win_rate = np.mean(recent_win_rates)
        min_win_rate = min(recent_win_rates)
        avg_return_value = np.mean(recent_avg_returns)
        min_avg_return = min(recent_avg_returns)
        
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ä¸¥æ ¼æ¡ä»¶æˆ–å®½æ¾æ¡ä»¶
        strict_condition = (
            avg_win_rate >= self.switch_threshold_win_rate_strict and
            min_win_rate >= self.switch_threshold_win_rate_strict and
            avg_return_value >= self.switch_threshold_avg_return_strict and
            min_avg_return >= self.switch_threshold_avg_return_strict
        )
        
        relaxed_condition = (
            avg_win_rate >= self.switch_threshold_win_rate_relaxed and
            min_win_rate >= self.switch_threshold_win_rate_relaxed and
            avg_return_value >= self.switch_threshold_avg_return_relaxed and
            min_avg_return >= self.switch_threshold_avg_return_relaxed
        )
        
        if not (strict_condition or relaxed_condition):
            return False
        
        # æ£€æŸ¥ç¨³å®šæ€§
        std_win_rate = np.std(recent_win_rates)
        std_avg_return = np.std(recent_avg_returns)
        
        if std_win_rate >= self.switch_win_rate_std:
            return False
        
        if std_avg_return >= self.switch_avg_return_std:
            return False
        
        # æ»¡è¶³æ‰€æœ‰æ¡ä»¶ï¼Œå¯ä»¥å¼€å§‹è¿‡æ¸¡é˜¶æ®µ
        self.switch_start_iteration = iteration
        print(f"\nğŸ¯ å¼€å§‹è¿‡æ¸¡é˜¶æ®µï¼ˆè¿­ä»£ {iteration + 1}ï¼‰")
        print(f"   - éšæœºç­–ç•¥æœŸæœ›èƒœç‡: {self.expected_win_rate_random*100:.2f}%")
        print(f"   - å¹³å‡èƒœç‡: {avg_win_rate*100:.1f}% (vs Random)")
        print(f"   - æœ€å°èƒœç‡: {min_win_rate*100:.1f}% (vs Random)")
        print(f"   - èƒœç‡æå‡: {(avg_win_rate - self.expected_win_rate_random)*100:.1f}% ({(avg_win_rate / self.expected_win_rate_random - 1)*100:.1f}% ç›¸å¯¹æå‡)")
        print(f"   - å¹³å‡æ”¶ç›Š: {avg_return_value:.2f} BB (vs Random)")
        print(f"   - æœ€å°æ”¶ç›Š: {min_avg_return:.2f} BB (vs Random)")
        print(f"   - æ”¶ç›Šæ ‡å‡†å·®: {std_avg_return:.2f} BB")
        
        if strict_condition:
            print(f"   - æ»¡è¶³ä¸¥æ ¼æ¡ä»¶ï¼ˆèƒœç‡ > 25% ä¸” æ”¶ç›Š > 0 BBï¼‰")
        else:
            print(f"   - æ»¡è¶³å®½æ¾æ¡ä»¶ï¼ˆèƒœç‡ > 20% ä¸” æ”¶ç›Š > 10 BBï¼‰")
        print()
        
        return True
    
    def solve(self, verbose=True, eval_interval=10, checkpoint_interval=0, 
              model_dir=None, save_prefix=None, game=None, start_iteration=0,
              eval_with_games=False, num_test_games=1000, training_state=None):
        """è¿è¡Œå¹¶è¡Œ DeepCFR è®­ç»ƒ
        
        Args:
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            eval_interval: è¯„ä¼°é—´éš”
            checkpoint_interval: checkpoint ä¿å­˜é—´éš”ï¼ˆ0=ä¸ä¿å­˜ï¼‰
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            save_prefix: ä¿å­˜æ–‡ä»¶å‰ç¼€
            game: æ¸¸æˆå®ä¾‹ï¼ˆç”¨äºä¿å­˜ checkpointï¼‰
            start_iteration: èµ·å§‹è¿­ä»£æ¬¡æ•°ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
            eval_with_games: æ˜¯å¦åœ¨è¯„ä¼°æ—¶è¿è¡Œæµ‹è¯•å¯¹å±€
            training_state: è®­ç»ƒçŠ¶æ€å­—å…¸ï¼ˆç”¨äºæ¢å¤å¤šé˜¶æ®µè®­ç»ƒçŠ¶æ€ï¼‰
        
        Returns:
            policy_network: è®­ç»ƒå¥½çš„ç­–ç•¥ç½‘ç»œ
            advantage_losses: ä¼˜åŠ¿ç½‘ç»œæŸå¤±å†å²
            policy_loss: ç­–ç•¥ç½‘ç»œæœ€ç»ˆæŸå¤±
        """
        print("=" * 70)
        print("å¹¶è¡Œ DeepCFR è®­ç»ƒ")
        print("=" * 70)
        print(f"  Worker æ•°é‡: {self.num_workers}")
        print(f"  è¿­ä»£æ¬¡æ•°: {self.num_iterations}")
        if start_iteration > 0:
            print(f"  ä»è¿­ä»£ {start_iteration + 1} æ¢å¤")
        print(f"  æ¯æ¬¡è¿­ä»£éå†æ¬¡æ•°: {self.num_traversals}")
        print(f"  è®¾å¤‡: {self.device}")
        print()
        
        # æ¢å¤å¤šé˜¶æ®µè®­ç»ƒçŠ¶æ€ï¼ˆå¦‚æœæä¾›ï¼‰
        if training_state is not None:
            self.switch_start_iteration = training_state.get('switch_start_iteration')
            self.win_rate_history = training_state.get('win_rate_history', [])
            self.avg_return_history = training_state.get('avg_return_history', [])
            # æ¢å¤è¿­ä»£è®¡æ•°å™¨
            if start_iteration > 0:
                self._iteration = start_iteration + 1
            if verbose:
                if self.switch_start_iteration is not None:
                    print(f"  âœ“ æ¢å¤å¤šé˜¶æ®µè®­ç»ƒçŠ¶æ€:")
                    print(f"    - switch_start_iteration: {self.switch_start_iteration}")
                    print(f"    - win_rate_historyé•¿åº¦: {len(self.win_rate_history)}")
                    print(f"    - avg_return_historyé•¿åº¦: {len(self.avg_return_history)}")
                    # ç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µé€šè¿‡switch_start_iterationåŒºåˆ†
                    if self.switch_start_iteration is None:
                        print(f"    - å½“å‰é˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µï¼ˆç½‘ç»œæƒé‡éšæœºåˆå§‹åŒ–ï¼‰")
                    else:
                        print(f"    - å½“å‰é˜¶æ®µï¼šç¬¬äºŒé˜¶æ®µï¼ˆè‡ªåšå¼ˆï¼Œswitch_start_iteration={self.switch_start_iteration}ï¼‰")
                else:
                    print(f"  âœ“ æ¢å¤å¤šé˜¶æ®µè®­ç»ƒçŠ¶æ€: ä»åœ¨ç¬¬ä¸€é˜¶æ®µï¼ˆå®Œå…¨éšæœºç­–ç•¥ï¼‰")
                print()
        
        # å¯åŠ¨ Worker
        self._start_workers()
        
        advantage_losses = {p: [] for p in range(self.num_players)}
        policy_losses = []  # ç­–ç•¥ç½‘ç»œæŸå¤±å†å²
        start_time = time.time()
        
        try:
            # ç­‰å¾… Worker å¯åŠ¨å¹¶å¼€å§‹äº§ç”Ÿæ ·æœ¬
            print("  ç­‰å¾… Worker å¯åŠ¨...", end="", flush=True)
            warmup_time = 0
            max_warmup = 30  # æœ€å¤šç­‰å¾… 30 ç§’
            # ä¿®å¤ï¼šåœ¨warmupæœŸé—´ä¹Ÿæ£€æŸ¥WorkerçŠ¶æ€ï¼Œé¿å…å¡ä½
            while warmup_time < max_warmup:
                # æ£€æŸ¥WorkerçŠ¶æ€
                dead_workers = [i for i, p in enumerate(self._workers) if not p.is_alive()]
                if dead_workers:
                    worker_info = ", ".join([f"Worker {wid}" for wid in dead_workers])
                    raise RuntimeError(f"æ£€æµ‹åˆ° Worker å·²æ­»äº¡: {worker_info}ã€‚è®­ç»ƒæ— æ³•ç»§ç»­ã€‚")
                
                time.sleep(1)
                warmup_time += 1
                collected = self._collect_samples(current_iteration=None)  # warmupé˜¶æ®µä¸éœ€è¦æ ‡è®°è¿­ä»£æ¬¡æ•°
                total_samples = sum(len(m) for m in self._advantage_memories)
                if total_samples > 0:
                    print(f" å°±ç»ª (è€—æ—¶ {warmup_time} ç§’ï¼Œå·²æ”¶é›† {total_samples} ä¸ªæ ·æœ¬)")
                    break
                print(".", end="", flush=True)
            else:
                print(f" è­¦å‘Š: Worker å¯åŠ¨è¶…æ—¶ï¼Œç»§ç»­è®­ç»ƒ...")
            
            for iteration in range(start_iteration, self.num_iterations):
                iter_start = time.time()
                
                # æ›´æ–°è¿­ä»£è®¡æ•°å™¨
                self._iteration_counter.value = iteration + 1
                
                # åŠ¨æ€æ”¶é›†æ ·æœ¬ï¼šç›´åˆ°æ”¶é›†åˆ°è¶³å¤Ÿæ•°é‡çš„æ–°æ ·æœ¬
                # è¿™æ ·å¯ä»¥ç¡®ä¿æ¯æ¬¡è¿­ä»£çš„æ•°æ®é‡æ˜¯æ’å®šçš„ï¼Œä¸å— Worker é€Ÿåº¦å½±å“
                # åŒæ—¶é€šè¿‡å¾ªç¯ sleep(1) é¿å…äº†ä¸»è¿›ç¨‹é•¿æ—¶é—´æ— å“åº”
                
                collection_start_time = time.time()
                current_total_samples = sum(len(m) for m in self._advantage_memories)
                # ç›®æ ‡ï¼šæœ¬è½®æ–°å¢ num_traversals ä¸ªæ ·æœ¬
                # æ³¨æ„ï¼šç”±äºå¯èƒ½æœ‰å¤šä¸ª Worker åŒæ—¶æäº¤ï¼Œå¯èƒ½ä¼šç•¥å¤šä¸€ç‚¹ï¼Œæ²¡å…³ç³»
                target_total_samples = current_total_samples + self.num_traversals
                
                # è®¾ç½®ä¸€ä¸ªè¶…æ—¶ä¿æŠ¤ï¼ˆä¾‹å¦‚ 10 åˆ†é’Ÿï¼‰ï¼Œé˜²æ­¢ Worker å…¨éƒ¨æŒ‚æ­»å¯¼è‡´ä¸»è¿›ç¨‹æ­»å¾ªç¯
                last_sample_count = current_total_samples
                
                # å…³é”®ä¿®å¤ï¼šæ ·æœ¬æ”¶é›†å¾ªç¯ä¸­åªæ¸…ç†é˜Ÿåˆ—ç§¯å‹ï¼Œä¸æ¸…ç†ç¼“å†²åŒº
                # ä¼˜åŒ–ï¼šæ ¹æ®é˜Ÿåˆ—çŠ¶æ€åŠ¨æ€è°ƒæ•´sleepæ—¶é—´ï¼Œé˜Ÿåˆ—æ»¡æ—¶ä¸sleepï¼Œé˜Ÿåˆ—ç©ºæ—¶æ‰sleep
                collected_in_this_iteration = 0  # æœ¬æ¬¡è¿­ä»£æ”¶é›†çš„æ ·æœ¬æ•°
                loop_count = 0
                no_progress_count = 0  # è¿ç»­æ— è¿›å±•æ¬¡æ•°
                last_warning_time = 0  # ä¸Šæ¬¡è­¦å‘Šæ—¶é—´ï¼Œé¿å…é‡å¤æ‰“å°
                while True:
                    loop_count += 1
                    # æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€ï¼Œå†³å®šæ˜¯å¦éœ€è¦sleep
                    queue_sizes = [q.qsize() for q in self._advantage_queues]
                    strategy_queue_size = self._strategy_queue.qsize()
                    total_queue_size = sum(queue_sizes) + strategy_queue_size
                    max_queue_size = max(max(queue_sizes) if queue_sizes else 0, strategy_queue_size)
                    queue_usage = max_queue_size / self.queue_maxsize if self.queue_maxsize > 0 else 0
                    
                    # æ”¶é›†æ ·æœ¬ï¼Œè¿”å›æœ¬æ¬¡æ”¶é›†çš„æ ·æœ¬æ•°
                    # å…³é”®ä¿®å¤ï¼šä¼ é€’current_iterationï¼Œè®©ä¸»è¿›ç¨‹æ ‡è®°æ ·æœ¬çš„è¿­ä»£æ¬¡æ•°
                    collected = self._collect_samples(current_iteration=self._iteration_counter.value)  # å†…éƒ¨åªæ¸…ç†é˜Ÿåˆ—ç§¯å‹
                    collected_in_this_iteration += collected
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾æ ‡ï¼šæ£€æŸ¥æœ¬æ¬¡è¿­ä»£æ”¶é›†çš„æ ·æœ¬æ•°ï¼Œè€Œä¸æ˜¯ç¼“å†²åŒºæ€»æ ·æœ¬æ•°
                    # å› ä¸ºç¼“å†²åŒºæ»¡äº†ä¹‹åï¼Œè™½ç„¶ä¼šéšæœºæ›¿æ¢ï¼Œä½†æ€»æ ·æœ¬æ•°ä¸ä¼šå¢åŠ 
                    if collected_in_this_iteration >= self.num_traversals:
                        break
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¿›å±•ï¼šå¦‚æœæœ¬æ¬¡æ”¶é›†äº†æ ·æœ¬ï¼Œè¯´æ˜æœ‰è¿›å±•
                    # ä¼˜åŒ–ï¼šåŒºåˆ†"é˜Ÿåˆ—ç©º"å’Œ"é˜Ÿåˆ—æ»¡ä½†æ”¶é›†å¤±è´¥"ä¸¤ç§æƒ…å†µ
                    if collected > 0:
                        no_progress_count = 0
                    else:
                        # åªæœ‰åœ¨é˜Ÿåˆ—ä¸ä¸ºç©ºæ—¶æ‰è§†ä¸º"æ— è¿›å±•"
                        # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼Œè¯´æ˜ Worker å¯èƒ½æš‚æ—¶æ²¡æœ‰äº§ç”Ÿæ ·æœ¬ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                        if total_queue_size > 0:
                            # ä¿®å¤ï¼šå¦‚æœé˜Ÿåˆ—ä½¿ç”¨ç‡è¾ƒé«˜ï¼ˆ>=80%ï¼‰ï¼Œä¸åº”è¯¥è§¦å‘è­¦å‘Š
                            # å› ä¸ºé˜Ÿåˆ—ä¸­æœ‰æ ·æœ¬ï¼Œåªæ˜¯æ”¶é›†é€Ÿåº¦æ…¢ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µ
                            # é˜Ÿåˆ—ä½¿ç”¨ç‡ >= 99%ï¼šé˜Ÿåˆ—æ»¡äº†ï¼ŒWorkeræ— æ³•ç»§ç»­æ·»åŠ æ ·æœ¬ï¼Œé˜Ÿåˆ—å¤§å°ä¸ä¼šå¢åŠ 
                            # é˜Ÿåˆ—ä½¿ç”¨ç‡ >= 80%ï¼šé˜Ÿåˆ—ä¸­æœ‰æ ·æœ¬ï¼Œåªæ˜¯æ”¶é›†é€Ÿåº¦æ…¢ï¼Œä¸åº”è¯¥è§¦å‘è­¦å‘Š
                            if queue_usage >= 0.80:
                                # é˜Ÿåˆ—ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œé‡ç½®è®¡æ•°å™¨ï¼ˆè¿™æ˜¯æ­£å¸¸æƒ…å†µï¼Œä¸æ˜¯é—®é¢˜ï¼‰
                                # é˜Ÿåˆ—ä¸­æœ‰æ ·æœ¬ï¼Œåªæ˜¯æ”¶é›†é€Ÿåº¦æ…¢ï¼Œä¸åº”è¯¥è§¦å‘è­¦å‘Š
                                no_progress_count = 0
                            else:
                                # é˜Ÿåˆ—ä½¿ç”¨ç‡è¾ƒä½ï¼Œä½†é˜Ÿåˆ—ä¸­æœ‰æ ·æœ¬ï¼Œå¯èƒ½æ˜¯æ”¶é›†é€Ÿåº¦æ…¢
                                # å¢åŠ è®¡æ•°å™¨ï¼Œä½†é˜ˆå€¼å¯ä»¥é€‚å½“æ”¾å®½
                                no_progress_count += 1
                        else:
                            # é˜Ÿåˆ—ä¸ºç©ºï¼Œé‡ç½®è®¡æ•°å™¨ï¼ˆè¿™æ˜¯æ­£å¸¸æƒ…å†µï¼Œä¸æ˜¯é—®é¢˜ï¼‰
                            no_progress_count = 0
                    
                    # æ£€æŸ¥è¶…æ—¶ (10åˆ†é’Ÿ)
                    elapsed_time = time.time() - collection_start_time
                    if elapsed_time > 600:
                        if verbose:
                            print(f"\n  âš ï¸ è­¦å‘Š: æ ·æœ¬æ”¶é›†è¶…æ—¶ (å·²æ”¶é›† {collected_in_this_iteration}/{self.num_traversals})")
                            # è¯Šæ–­ä¿¡æ¯
                            print(f"    è¯Šæ–­ä¿¡æ¯:")
                            print(f"      - è€—æ—¶: {elapsed_time:.1f}ç§’")
                            current_total_samples = sum(len(m) for m in self._advantage_memories)
                            print(f"      - å½“å‰ä¼˜åŠ¿æ ·æœ¬æ€»æ•°: {current_total_samples:,}")
                            print(f"      - ç­–ç•¥æ ·æœ¬æ€»æ•°: {len(self._strategy_memories):,}")
                            
                            # æ£€æŸ¥ Worker çŠ¶æ€
                            alive_workers = sum(1 for p in self._workers if p.is_alive())
                            print(f"      - å­˜æ´»çš„ Worker: {alive_workers}/{self.num_workers}")
                            
                            # æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
                            queue_sizes = [q.qsize() for q in self._advantage_queues]
                            total_queue_size = sum(queue_sizes)
                            print(f"      - é˜Ÿåˆ—ä¸­å¾…å¤„ç†æ ·æœ¬: {total_queue_size:,}")
                            
                            # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
                            if HAS_PSUTIL:
                                try:
                                    process = psutil.Process()
                                    mem_info = process.memory_info()
                                    mem_mb = mem_info.rss / 1024 / 1024
                                    mem_percent = process.memory_percent()
                                    print(f"      - ä¸»è¿›ç¨‹å†…å­˜ä½¿ç”¨: {mem_mb:.1f}MB ({mem_percent:.1f}%)")
                                    
                                    # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
                                    sys_mem = psutil.virtual_memory()
                                    print(f"      - ç³»ç»Ÿå†…å­˜: {sys_mem.percent:.1f}% å·²ä½¿ç”¨ ({sys_mem.used/1024/1024/1024:.1f}GB / {sys_mem.total/1024/1024/1024:.1f}GB)")
                                    
                                    if sys_mem.percent > 90:
                                        print(f"      âš ï¸ ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({sys_mem.percent:.1f}%)ï¼Œå¯èƒ½å¯¼è‡´ OOM")
                                except:
                                    pass
                            
                            # å¦‚æœ Worker å…¨éƒ¨æ­»äº¡ï¼ŒæŠ›å‡ºå¼‚å¸¸
                            if alive_workers == 0:
                                raise RuntimeError("æ‰€æœ‰ Worker è¿›ç¨‹å·²æ­»äº¡ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒã€‚")
                            
                            # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºä¸” Worker å­˜æ´»ï¼Œå¯èƒ½æ˜¯ Worker é™·å…¥æ­»é”æˆ–å†…å­˜ä¸è¶³
                            if total_queue_size == 0 and alive_workers > 0:
                                print(f"      âš ï¸ é˜Ÿåˆ—ä¸ºç©ºä½† Worker å­˜æ´»ï¼Œå¯èƒ½é™·å…¥æ­»é”æˆ–å†…å­˜ä¸è¶³")
                                print(f"      å»ºè®®: æ£€æŸ¥ç³»ç»Ÿæ—¥å¿— (dmesg | grep -i oom) æŸ¥çœ‹æ˜¯å¦æœ‰ OOM æ€æ­»è¿›ç¨‹")
                        break
                    
                    # å¦‚æœè¿ç»­å¤šæ¬¡æ— è¿›å±•ï¼Œæå‰è­¦å‘Šï¼ˆä¼˜åŒ–ï¼šé¿å…é‡å¤æ‰“å°ï¼‰
                    # ä¿®å¤ï¼šä½¿ç”¨æ—¶é—´é—´éš”æ§åˆ¶ï¼Œæ¯30ç§’æœ€å¤šè­¦å‘Šä¸€æ¬¡
                    # ä¿®å¤ï¼šå¢åŠ è¯Šæ–­ä¿¡æ¯ï¼Œè®°å½•max_collectå’Œå®é™…æ”¶é›†çš„æ ·æœ¬æ•°
                    current_time = time.time()
                    if no_progress_count >= 20 and verbose:  # 10ç§’æ— è¿›å±•ï¼ˆ20æ¬¡ Ã— 0.5ç§’ï¼‰
                        if current_time - last_warning_time > 30:  # æ¯30ç§’æœ€å¤šè­¦å‘Šä¸€æ¬¡
                            # è·å–å½“å‰çš„max_collectå€¼
                            current_max_collect = self._adaptive_max_collect.get('current', 'N/A')
                            print(f"\n  âš ï¸ è­¦å‘Š: è¿ç»­ {no_progress_count * 0.5:.1f}ç§’æ— æ ·æœ¬æ”¶é›†è¿›å±•")
                            print(f"    è¯Šæ–­ä¿¡æ¯:")
                            print(f"      - é˜Ÿåˆ—å¤§å°: {total_queue_size:,}")
                            print(f"      - é˜Ÿåˆ—ä½¿ç”¨ç‡: {queue_usage:.1%}")
                            print(f"      - max_collect: {current_max_collect}")
                            print(f"      - æœ¬æ¬¡æ”¶é›†æ ·æœ¬æ•°: {collected}")
                            print(f"      - æœ¬æ¬¡è¿­ä»£å·²æ”¶é›†: {collected_in_this_iteration}/{self.num_traversals}")
                            print(f"      - è€—æ—¶: {time.time() - collection_start_time:.1f}ç§’")
                            last_warning_time = current_time
                    
                    # ä¼˜åŒ–ï¼šæ ¹æ®é˜Ÿåˆ—çŠ¶æ€åŠ¨æ€è°ƒæ•´sleepæ—¶é—´
                    # é˜Ÿåˆ—ä½¿ç”¨ç‡ > 80% æ—¶ä¸sleepï¼Œé˜Ÿåˆ—ç©ºæ—¶æ‰sleepï¼Œé¿å…æ¶ˆè´¹é€Ÿåº¦ä¸å¤Ÿå¿«
                    if queue_usage > 0.80:
                        # é˜Ÿåˆ—æ»¡äº†ï¼Œä¸sleepï¼Œç«‹å³ç»§ç»­æ¶ˆè´¹
                        sleep_time = 0.0
                    elif total_queue_size == 0:
                        # é˜Ÿåˆ—ä¸ºç©ºï¼Œsleep 0.5ç§’ï¼Œé¿å…CPUç©ºè½¬
                        sleep_time = 0.5
                    elif queue_usage > 0.50:
                        # é˜Ÿåˆ—ä½¿ç”¨ç‡ > 50%ï¼Œsleepæ—¶é—´ç¼©çŸ­åˆ°0.01ç§’ï¼ŒåŠ å¿«æ¶ˆè´¹é€Ÿåº¦
                        sleep_time = 0.01
                    else:
                        # é˜Ÿåˆ—æœ‰æ•°æ®ä½†ä½¿ç”¨ç‡è¾ƒä½ï¼Œsleep 0.1ç§’ï¼Œå¹³è¡¡æ¶ˆè´¹é€Ÿåº¦å’ŒCPUä½¿ç”¨ç‡
                        sleep_time = 0.1
                    
                    if sleep_time > 0:
                        time.sleep(sleep_time)
                
                # æ¸…ç†é˜Ÿåˆ—ç§¯å‹ï¼ˆç¼“å†²åŒºæ¸…ç†å·²ç¦ç”¨ï¼Œå®Œå…¨ä¾èµ–éšæœºæ›¿æ¢ç­–ç•¥ï¼‰
                cleanup_final_start = time.time()
                self._check_and_cleanup_memory(cleanup_buffers=False)
                cleanup_final_time = time.time() - cleanup_final_start
                
                collection_total_time = time.time() - collection_start_time
                
                # è®­ç»ƒä¼˜åŠ¿ç½‘ç»œï¼ˆå¹¶è¡Œä¼˜åŒ–ï¼‰
                # å…³é”®ä¿®å¤ï¼šç¬¬ä¸€é˜¶æ®µï¼ˆå®Œå…¨éšæœºç­–ç•¥ï¼‰ä»ç„¶è®­ç»ƒç½‘ç»œï¼ˆç”¨äºcheckpointå’Œå­¦ä¹ æ ·æœ¬ï¼‰
                # ä½†ä¸åŒæ­¥åˆ°Workerï¼Œè®©Workerç»§ç»­ä½¿ç”¨éšæœºç­–ç•¥ï¼Œé¿å…è‡ªåšå¼ˆ
                advantage_train_start = time.time()
                player_train_times = {}  # è®°å½•æ¯ä¸ªç©å®¶çš„è®­ç»ƒæ—¶é—´
                
                # ä¼˜åŒ–ï¼šä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè®­ç»ƒå¤šä¸ªä¼˜åŠ¿ç½‘ç»œ
                # PyTorchçš„CUDAæ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œå¤šçº¿ç¨‹å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼Œå……åˆ†åˆ©ç”¨GPUèµ„æº
                
                def train_advantage(player):
                    """è®­ç»ƒå•ä¸ªç©å®¶çš„ä¼˜åŠ¿ç½‘ç»œ"""
                    player_start_time = time.time()
                    try:
                        # é‡æ–°åˆå§‹åŒ–ä¼˜åŠ¿ç½‘ç»œï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        # æ³¨æ„ï¼šé»˜è®¤å…³é—­é‡æ–°åˆå§‹åŒ–ï¼Œå› ä¸ºæ¯æ¬¡è¿­ä»£é‡ç½®ä¼šå¯¼è‡´ï¼š
                        # 1. ä¼˜åŠ¿ç½‘ç»œæ— æ³•æŒç»­å­¦ä¹ ï¼Œç”Ÿæˆçš„ç­–ç•¥æ ·æœ¬è´¨é‡å·®
                        # 2. ç­–ç•¥ç½‘ç»œå­¦ä¹ å·®çš„ç­–ç•¥æ ·æœ¬ï¼Œå½¢æˆæ¶æ€§å¾ªç¯
                        # 3. Foldæ¦‚ç‡æ¥è¿‘0ï¼ŒAll-Inæ¦‚ç‡è¿‡é«˜ï¼Œç­–ç•¥ä¸ç¨³å®š
                        if self._reinitialize_advantage_networks:
                            self.reinitialize_advantage_network(player)
                        
                        # æ³¨æ„ï¼šæ ·æœ¬çš„iterationå­—æ®µè®°å½•çš„æ˜¯åˆ›å»ºæ—¶çš„iteration_counter.value
                        # Workerè¿›ç¨‹è¯»å–iteration_counter.valueï¼Œæ ·æœ¬è®°å½•çš„iteration = iteration_counter.value
                        # è®­ç»ƒæ—¶ï¼Œåº”è¯¥ä½¿ç”¨self._iteration_counter.valueæ¥åŒ¹é…æ ·æœ¬çš„iterationå­—æ®µ
                        # ä½¿ç”¨self._iteration_counter.valueè€Œä¸æ˜¯iteration+1ï¼Œç¡®ä¿ä¸æ ·æœ¬çš„iterationå­—æ®µåŒ¹é…
                        result = self._learn_advantage_network(player, current_iteration=self._iteration_counter.value)
                        player_train_time = time.time() - player_start_time
                        player_train_times[player] = player_train_time
                        return result, player_train_time
                    except Exception as e:
                        import traceback
                        traceback.print_exc()
                        raise
                
                # å¹¶è¡Œè®­ç»ƒï¼ˆæœ€å¤šä½¿ç”¨min(ç©å®¶æ•°, GPUæ•°, 4)ä¸ªçº¿ç¨‹ï¼‰
                # å¦‚æœGPUæ•°é‡ >= ç©å®¶æ•°é‡ï¼Œæ¯ä¸ªç©å®¶å·²ç»åœ¨ä¸åŒGPUä¸Šï¼Œå¯ä»¥å®Œå…¨å¹¶è¡Œ
                # å¦åˆ™ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè®­ç»ƒï¼ˆCUDAæ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œå¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼‰
                if self.use_multi_gpu:
                    max_workers = min(self.num_players, len(self.gpu_ids))
                else:
                    max_workers = min(self.num_players, 6)
                
                # ç­–ç•¥ç½‘ç»œè®­ç»ƒå·²ç§»åˆ°checkpointéƒ¨åˆ†ï¼Œè¿™é‡Œåªè®­ç»ƒä¼˜åŠ¿ç½‘ç»œ
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # æäº¤ä¼˜åŠ¿ç½‘ç»œè®­ç»ƒä»»åŠ¡
                    advantage_futures = {executor.submit(train_advantage, player): player 
                                       for player in range(self.num_players)}
                    
                    # ç­‰å¾…ä¼˜åŠ¿ç½‘ç»œè®­ç»ƒå®Œæˆ
                    completed_count = 0
                    pending_players = set(range(self.num_players))
                    try:
                        for future in as_completed(advantage_futures, timeout=300):  # æ·»åŠ æ€»è¶…æ—¶ï¼Œé¿å…æ— é™ç­‰å¾…
                            player = advantage_futures[future]
                            completed_count += 1
                            pending_players.discard(player)
                            try:
                                result = future.result()
                                if result is not None:
                                    if isinstance(result, tuple):
                                        loss, player_time = result
                                    else:
                                        loss = result
                                        player_time = player_train_times.get(player, 0)
                                    if loss is not None:
                                        advantage_losses[player].append(loss)
                            except Exception as e:
                                import traceback
                                traceback.print_exc()
                                get_logger().warning(f"ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œè®­ç»ƒå¤±è´¥: {e}")
                    except TimeoutError:
                        # å–æ¶ˆå‰©ä½™ä»»åŠ¡
                        for future in advantage_futures:
                            if not future.done():
                                future.cancel()
                        raise RuntimeError(f"ä¼˜åŠ¿ç½‘ç»œè®­ç»ƒè¶…æ—¶ï¼Œå·²å®Œæˆ {completed_count}/{self.num_players} ä¸ªç©å®¶")
                
                advantage_train_time = time.time() - advantage_train_start
                
                # è¾“å‡ºæ¯ä¸ªç©å®¶çš„è®­ç»ƒæ—¶é—´ï¼ˆç”¨äºéªŒè¯å¹¶è¡Œè®­ç»ƒæ˜¯å¦ç”Ÿæ•ˆï¼‰
                if verbose and player_train_times:
                    player_times_str = ", ".join([f"ç©å®¶{i}: {player_train_times.get(i, 0):.2f}ç§’" 
                                                  for i in range(self.num_players)])
                    get_logger().info(f"    å„ç©å®¶ä¼˜åŠ¿ç½‘ç»œè®­ç»ƒæ—¶é—´: {player_times_str}")
                    # è®¡ç®—å¹¶è¡Œæ•ˆç‡
                    max_player_time = max(player_train_times.values()) if player_train_times else 0
                    total_sequential_time = sum(player_train_times.values()) if player_train_times else 0
                    if max_player_time > 0 and advantage_train_time > 0:
                        # æ­£ç¡®çš„å¹¶è¡Œæ•ˆç‡è®¡ç®—ï¼š
                        # 1. ç†æƒ³å¹¶è¡Œæ—¶é—´ = max_player_timeï¼ˆæ‰€æœ‰ç©å®¶å¹¶è¡Œæ‰§è¡Œï¼Œæ€»æ—¶é—´ç­‰äºæœ€æ…¢çš„é‚£ä¸ªï¼‰
                        # 2. å®é™…å¹¶è¡Œæ—¶é—´ = advantage_train_timeï¼ˆå®é™…æµ‹é‡çš„æ€»æ—¶é—´ï¼‰
                        # 3. å¹¶è¡Œæ•ˆç‡ = (ç†æƒ³å¹¶è¡Œæ—¶é—´ / å®é™…å¹¶è¡Œæ—¶é—´) * 100
                        ideal_parallel_time = max_player_time
                        parallel_efficiency = (ideal_parallel_time / advantage_train_time) * 100
                        
                        # è®¡ç®—åŠ é€Ÿæ¯”
                        speedup = total_sequential_time / advantage_train_time if advantage_train_time > 0 else 0
                        theoretical_max_speedup = self.num_players
                        speedup_efficiency = (speedup / theoretical_max_speedup) * 100 if theoretical_max_speedup > 0 else 0
                        
                        get_logger().info(f"    å¹¶è¡Œæ•ˆç‡: {parallel_efficiency:.1f}% (ç†æƒ³æ—¶é—´: {ideal_parallel_time:.2f}ç§’, å®é™…æ—¶é—´: {advantage_train_time:.2f}ç§’)")
                        get_logger().info(f"    åŠ é€Ÿæ¯”: {speedup:.2f}x (ä¸²è¡Œ: {total_sequential_time:.2f}ç§’, å¹¶è¡Œ: {advantage_train_time:.2f}ç§’, ç†è®ºæœ€å¤§: {theoretical_max_speedup}x, æ•ˆç‡: {speedup_efficiency:.1f}%)")
                        
                        if parallel_efficiency < 70:
                            get_logger().warning(f"    âš ï¸ å¹¶è¡Œæ•ˆç‡è¾ƒä½ ({parallel_efficiency:.1f}%)ï¼Œå¯èƒ½å¹¶è¡Œè®­ç»ƒæœªå®Œå…¨ç”Ÿæ•ˆ")
                        elif speedup_efficiency < 70:
                            get_logger().warning(f"    âš ï¸ åŠ é€Ÿæ¯”æ•ˆç‡è¾ƒä½ ({speedup_efficiency:.1f}%)ï¼Œå¯èƒ½å¹¶è¡Œè®­ç»ƒæœªå®Œå…¨ç”Ÿæ•ˆ")
                
                # ç­–ç•¥ç½‘ç»œè®­ç»ƒï¼šæ¯æ¬¡è¿­ä»£éƒ½è®­ç»ƒï¼Œç¡®ä¿ä¸ä¼˜åŠ¿ç½‘ç»œåŒæ­¥æ›´æ–°
                # ä¿®æ”¹ï¼šä»åªåœ¨checkpointæ—¶è®­ç»ƒæ”¹ä¸ºæ¯æ¬¡è¿­ä»£éƒ½è®­ç»ƒ
                # è¿™æ ·å¯ä»¥ç¡®ä¿ç­–ç•¥ç½‘ç»œå’Œä¼˜åŠ¿ç½‘ç»œåŒæ­¥æ›´æ–°ï¼Œé¿å…ç­–ç•¥æ ·æœ¬ä¸ä¸€è‡´çš„é—®é¢˜
                strategy_train_start = time.time()
                policy_loss = self._learn_strategy_network(current_iteration=self._iteration_counter.value)
                strategy_train_time = time.time() - strategy_train_start
                if policy_loss is not None:
                    # ä¿å­˜ç­–ç•¥æŸå¤±ï¼ˆç”¨äºcheckpointæ—¶æ˜¾ç¤ºï¼‰
                    if not hasattr(self, '_last_policy_loss'):
                        self._last_policy_loss = []
                    self._last_policy_loss.append(policy_loss)
                
                # åŒæ­¥ç½‘ç»œå‚æ•°åˆ° Workerï¼ˆå§‹ç»ˆåŒæ­¥ï¼Œç§»é™¤ä¸¤é˜¶æ®µæœºåˆ¶ï¼‰
                # ä¿®æ”¹ï¼šåƒå•è¿›ç¨‹è®­ç»ƒä¸€æ ·ï¼Œå§‹ç»ˆä½¿ç”¨è®­ç»ƒåçš„ç½‘ç»œï¼Œé¿å…ä¸¤é˜¶æ®µé—®é¢˜
                sync_start = time.time()
                if (iteration + 1) % self.sync_interval == 0:
                    self._sync_network_params()
                sync_time = time.time() - sync_start
                
                self._iteration += 1
                
                iter_time = time.time() - iter_start
                
                # è®°å½•å„ç¯èŠ‚è€—æ—¶
                if verbose:
                    get_logger().info(f"  è¿­ä»£ {iteration + 1} å„ç¯èŠ‚è€—æ—¶:")
                    get_logger().info(f"    - æ ·æœ¬æ”¶é›†: {collection_total_time:.2f}ç§’")
                    get_logger().info(f"    - ä¼˜åŠ¿ç½‘ç»œè®­ç»ƒ: {advantage_train_time:.2f}ç§’")
                    get_logger().info(f"    - ç­–ç•¥ç½‘ç»œè®­ç»ƒ: {strategy_train_time:.2f}ç§’")
                    if (iteration + 1) % self.sync_interval == 0:
                        get_logger().info(f"    - ç½‘ç»œåŒæ­¥: {sync_time:.2f}ç§’")
                    get_logger().info(f"    - å…¶ä»–(æ¸…ç†ç­‰): {cleanup_final_time:.2f}ç§’")
                    get_logger().info(f"    - è¿­ä»£æ€»è€—æ—¶: {iter_time:.2f}ç§’")
                
                if verbose:
                    # æ˜¾ç¤ºé˜Ÿåˆ—çŠ¶æ€å’Œæ¶ˆè´¹é€Ÿåº¦ä¿¡æ¯
                    queue_info = []
                    max_queue_usage = 0.0
                    for player in range(self.num_players):
                        q_size = self._advantage_queues[player].qsize()
                        usage = q_size / self.queue_maxsize if self.queue_maxsize > 0 else 0
                        max_queue_usage = max(max_queue_usage, usage)
                        if q_size > 0:
                            queue_info.append(f"ç©å®¶{player}:{q_size}")
                    
                    strategy_q_size = self._strategy_queue.qsize()
                    strategy_usage = strategy_q_size / self.queue_maxsize if self.queue_maxsize > 0 else 0
                    max_queue_usage = max(max_queue_usage, strategy_usage)
                    
                    queue_status = ""
                    if queue_info or strategy_q_size > 0:
                        queue_status = f" | é˜Ÿåˆ—: {', '.join(queue_info)}"
                        if strategy_q_size > 0:
                            queue_status += f",ç­–ç•¥:{strategy_q_size}"
                        
                        # æ˜¾ç¤ºè°ƒæ•´ä¿¡æ¯
                        adj_info = self._adaptive_max_collect.get('last_adjustment', {})
                        cpu_info = ""
                        if adj_info.get('cpu_percent') is not None:
                            cpu_info = f", CPU:{adj_info['cpu_percent']:.0f}%"
                        growth_info = ""
                        if adj_info.get('growth_rate', 0) > 0:
                            growth_info = f", å¢é•¿ç‡:{adj_info['growth_rate']:.0f}/s"
                        
                        queue_status += f" (ä½¿ç”¨ç‡:{max_queue_usage*100:.0f}%{growth_info}{cpu_info}, max_collect:{self._adaptive_max_collect['current']:,})"
                    
                    get_logger().info(f"  è¿­ä»£ {iteration + 1}/{self.num_iterations} "
                          f"(è€—æ—¶: {iter_time:.2f}ç§’) | "
                          f"ä¼˜åŠ¿æ ·æœ¬: {sum(len(m) for m in self._advantage_memories):,} | "
                          f"ç­–ç•¥æ ·æœ¬: {len(self._strategy_memories):,}{queue_status}")
                
                # è¯„ä¼°ï¼šåªåœ¨checkpointæ—¶è¯„ä¼°ï¼ˆä¼˜åŒ–ï¼šå‡å°‘è¯„ä¼°æ—¶é—´ï¼‰
                # è¯„ä¼°é€»è¾‘å·²ç§»åˆ°checkpointéƒ¨åˆ†
                
                # ä¿å­˜ checkpointï¼ˆåŒæ—¶è®­ç»ƒç­–ç•¥ç½‘ç»œå’Œè¯„ä¼°ï¼‰
                if checkpoint_interval > 0 and (iteration + 1) % checkpoint_interval == 0:
                        if model_dir and save_prefix and game:
                            get_logger().info(f"\n  ğŸ’¾ ä¿å­˜ checkpoint (è¿­ä»£ {iteration + 1})...")
                            try:
                                # 1. è®­ç»ƒç­–ç•¥ç½‘ç»œï¼ˆcheckpointæ—¶é¢å¤–è®­ç»ƒä¸€æ¬¡ï¼Œç¡®ä¿ç­–ç•¥ç½‘ç»œå……åˆ†å­¦ä¹ ï¼‰
                                get_logger().info("    æ­£åœ¨è®­ç»ƒç­–ç•¥ç½‘ç»œ (ç”¨äº Checkpointï¼Œé¢å¤–è®­ç»ƒ)...")
                                # æ³¨æ„ï¼šæ ·æœ¬çš„iterationå­—æ®µè®°å½•çš„æ˜¯åˆ›å»ºæ—¶çš„iteration_counter.value
                                # Workerè¿›ç¨‹è¯»å–iteration_counter.valueï¼Œæ ·æœ¬è®°å½•çš„iteration = iteration_counter.value
                                # è®­ç»ƒæ—¶ï¼Œåº”è¯¥ä½¿ç”¨self._iteration_counter.valueæ¥åŒ¹é…æ ·æœ¬çš„iterationå­—æ®µ
                                # ä½¿ç”¨self._iteration_counter.valueè€Œä¸æ˜¯iteration+1ï¼Œç¡®ä¿ä¸æ ·æœ¬çš„iterationå­—æ®µåŒ¹é…
                                # ä½¿ç”¨æœ€åä¸€æ¬¡è¿­ä»£çš„ç­–ç•¥æŸå¤±ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™é‡æ–°è®­ç»ƒ
                                if hasattr(self, '_last_policy_loss') and self._last_policy_loss:
                                    policy_loss = self._last_policy_loss[-1]
                                    get_logger().info(f"    ä½¿ç”¨æœ€åä¸€æ¬¡è¿­ä»£çš„ç­–ç•¥æŸå¤±: MSE={policy_loss/self._iteration_counter.value:.6f} (åŸå§‹: {policy_loss:.2f})")
                                else:
                                    policy_loss = self._learn_strategy_network(current_iteration=self._iteration_counter.value)
                                    if policy_loss is not None:
                                        # ä¿å­˜ç­–ç•¥æŸå¤±
                                        policy_losses.append(policy_loss)
                                        # æ‰“å°å½’ä¸€åŒ–çš„æŸå¤±å€¼ï¼ˆMSEï¼‰
                                        current_iter = iteration + 1
                                        mse = policy_loss / current_iter if current_iter > 0 else policy_loss
                                        get_logger().info(f"    å®Œæˆ (MSE: {mse:.6f}, åŸå§‹: {policy_loss:.2f})")
                                    else:
                                        get_logger().info("    å®Œæˆ (æ— è¶³å¤Ÿæ ·æœ¬è®­ç»ƒ)")
                                
                                # 2. è¿è¡Œè¯„ä¼°ï¼ˆcheckpointæ—¶è¯„ä¼°ï¼‰
                                print()
                                # æ‰“å°ä¼˜åŠ¿ç½‘ç»œæŸå¤±
                                current_iter = iteration + 1
                                for player, losses in advantage_losses.items():
                                    if not losses:
                                        continue
                                    raw_loss = losses[-1]
                                    if self._advantage_loss_type == "mse":
                                        # å½’ä¸€åŒ–ï¼šé™¤ä»¥iterationï¼ˆå› ä¸ºæŸå¤±å€¼ = iteration * MSEï¼‰
                                        mse = raw_loss / current_iter if current_iter > 0 else raw_loss
                                        print(f"    ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œæŸå¤±: MSE={mse:.2f} (åŸå§‹: {raw_loss:.2f})")
                                    else:
                                        # Huber ç­‰ç¨³å¥æŸå¤±ä¸å†åšâ€œé™¤ä»¥iterationâ€çš„å½’ä¸€åŒ–å±•ç¤º
                                        print(f"    ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œæŸå¤±: Loss({self._advantage_loss_type})={raw_loss:.6f}")
                                
                                # æ‰“å°å½’ä¸€åŒ–çš„ç­–ç•¥ç½‘ç»œæŸå¤±ï¼ˆé™¤ä»¥iterationå¾—åˆ°MSEï¼‰
                                if policy_losses:
                                    raw_policy_loss = policy_losses[-1]
                                    # å½’ä¸€åŒ–ï¼šé™¤ä»¥iterationï¼ˆå› ä¸ºæŸå¤±å€¼ = iteration * MSEï¼‰
                                    # å½’ä¸€åŒ–åçš„å€¼å°±æ˜¯MSEæœ¬èº«
                                    mse = raw_policy_loss / current_iter if current_iter > 0 else raw_policy_loss
                                    print(f"    ç­–ç•¥ç½‘ç»œæŸå¤±: MSE={mse:.6f} (åŸå§‹: {raw_policy_loss:.2f})")
                                
                                # è¿è¡Œè¯„ä¼°
                                if game is not None:
                                    try:
                                        from training_evaluator import quick_evaluate, evaluate_with_test_games, _get_action_name
                                        print(f"  è¯„ä¼°è®­ç»ƒæ•ˆæœ...", end="", flush=True)
                                        eval_result = quick_evaluate(
                                            game,
                                            self,
                                            include_test_games=eval_with_games,
                                            num_test_games=num_test_games,
                                            max_depth=None,
                                            verbose=True  # å¯ç”¨è¯¦ç»†è¾“å‡ºä»¥æŸ¥çœ‹é”™è¯¯
                                        )
                                        get_logger().info(" å®Œæˆ")
                                        
                                        # æ‰“å°ç®€è¦è¯„ä¼°ä¿¡æ¯
                                        metrics = eval_result['metrics']
                                        print(f"    ç­–ç•¥ç†µ: {metrics.get('avg_entropy', 0):.4f} | "
                                              f"ç­–ç•¥ç¼“å†²åŒº: {len(self._strategy_memories):,} | "
                                              f"ä¼˜åŠ¿æ ·æœ¬: {sum(len(m) for m in self._advantage_memories):,}")
                                        
                                        if eval_with_games and eval_result.get('test_results'):
                                            test_results = eval_result['test_results']
                                            num_games = test_results.get('games_played', 0)
                                            mode = test_results.get('mode', 'unknown')
                                            num_players = test_results.get('num_players', 0)
                                            
                                            # è·å–å¤§ç›²æ³¨å€¼
                                            bb = None
                                            try:
                                                game_string = str(game)
                                                blind_match = re.search(r'blind=([\d\s]+)', game_string)
                                                if blind_match:
                                                    blinds = [int(x) for x in blind_match.group(1).strip().split()]
                                                    bb = max([b for b in blinds if b > 0], default=None)
                                            except:
                                                pass
                                            
                                            if num_games > 0:
                                                if mode == "self_play":
                                                    # è‡ªå¯¹å¼ˆæ¨¡å¼ï¼šæ˜¾ç¤ºæ‰€æœ‰ä½ç½®
                                                    print(f"    æµ‹è¯•å¯¹å±€: {num_games} å±€ (è‡ªå¯¹å¼ˆ)")
                                                    for i in range(num_players):
                                                        avg_return = test_results.get(f'player{i}_avg_return', 0)
                                                        win_rate = test_results.get(f'player{i}_win_rate', 0) * 100
                                                        if bb is not None and bb > 0:
                                                            bb_value = avg_return / bb
                                                            print(f"      ç©å®¶{i}: å¹³å‡å›æŠ¥ {avg_return:.2f} ({bb_value:+.2f} BB), èƒœç‡ {win_rate:.1f}%")
                                                        else:
                                                            print(f"      ç©å®¶{i}: å¹³å‡å›æŠ¥ {avg_return:.2f}, èƒœç‡ {win_rate:.1f}%")
                                                    
                                                    # æ‰“å°æµ‹è¯•å¯¹å±€ä¸­çš„åŠ¨ä½œå¹³å‡å æ¯”ï¼ˆè‡ªå¯¹å¼ˆæ¨¡å¼ï¼‰
                                                    if test_results.get('action_statistics'):
                                                        action_stats = test_results['action_statistics']
                                                        total_count = sum(s['count'] for s in action_stats.values())
                                                        if total_count > 0:
                                                            print(f"    åŠ¨ä½œç»Ÿè®¡ (æµ‹è¯•å¯¹å±€):")
                                                            # æŒ‰å æ¯”æ’åº
                                                            sorted_actions = sorted(action_stats.items(), 
                                                                                  key=lambda x: x[1]['percentage'], 
                                                                                  reverse=True)
                                                            action_info = []
                                                            for action, stats in sorted_actions:
                                                                count = stats['count']
                                                                percentage = stats['percentage']
                                                                avg_prob = stats['avg_probability']
                                                                action_name = _get_action_name(action, game)
                                                                action_info.append(f"{action_name}: {percentage:.1f}% (å¹³å‡æ¦‚ç‡: {avg_prob:.3f})")
                                                            print(f"      {' | '.join(action_info)}")
                                                else:
                                                    # vs_randomæ¨¡å¼ï¼šæ˜¾ç¤ºæ‰€æœ‰ä½ç½®ä½¿ç”¨è®­ç»ƒç­–ç•¥æ—¶çš„è¡¨ç°
                                                    print(f"    æµ‹è¯•å¯¹å±€: {num_games} å±€ (vs Random, éšæœºä½ç½®)")
                                                    # æ˜¾ç¤ºå„ä½ç½®çš„è¡¨ç°
                                                    position_stats = []
                                                    for i in range(num_players):
                                                        trained_count = test_results.get(f'player{i}_trained_count', 0)
                                                        if trained_count > 0:
                                                            avg_return = test_results.get(f'player{i}_trained_avg_return', 0)
                                                            win_rate = test_results.get(f'player{i}_trained_win_rate', 0) * 100
                                                            if bb is not None and bb > 0:
                                                                bb_value = avg_return / bb
                                                                position_stats.append(f"ç©å®¶{i}: {trained_count}å±€, å›æŠ¥{avg_return:.0f} ({bb_value:+.2f}BB), èƒœç‡{win_rate:.0f}%")
                                                            else:
                                                                position_stats.append(f"ç©å®¶{i}: {trained_count}å±€, å›æŠ¥{avg_return:.0f}, èƒœç‡{win_rate:.0f}%")
                                                    
                                                    if position_stats:
                                                        print(f"      {' | '.join(position_stats)}")
                                                    
                                                    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
                                                    overall_avg_return = test_results.get('player0_avg_return', 0)
                                                    overall_win_rate = test_results.get('player0_win_rate', 0) * 100
                                                    if bb is not None and bb > 0:
                                                        bb_value = overall_avg_return / bb
                                                        print(f"      æ€»ä½“: å¹³å‡å›æŠ¥ {overall_avg_return:.2f} ({bb_value:+.2f} BB), èƒœç‡ {overall_win_rate:.1f}%")
                                                    else:
                                                        print(f"      æ€»ä½“: å¹³å‡å›æŠ¥ {overall_avg_return:.2f}, èƒœç‡ {overall_win_rate:.1f}%")
                                                    
                                                    # æ‰“å°æµ‹è¯•å¯¹å±€ä¸­çš„åŠ¨ä½œå¹³å‡å æ¯”
                                                    if test_results.get('action_statistics'):
                                                        action_stats = test_results['action_statistics']
                                                        total_count = sum(s['count'] for s in action_stats.values())
                                                        if total_count > 0:
                                                            print(f"    åŠ¨ä½œç»Ÿè®¡ (æµ‹è¯•å¯¹å±€):")
                                                            # æŒ‰å æ¯”æ’åº
                                                            sorted_actions = sorted(action_stats.items(), 
                                                                                  key=lambda x: x[1]['percentage'], 
                                                                                  reverse=True)
                                                            action_info = []
                                                            for action, stats in sorted_actions:
                                                                count = stats['count']
                                                                percentage = stats['percentage']
                                                                avg_prob = stats['avg_probability']
                                                                action_name = _get_action_name(action, game)
                                                                action_info.append(f"{action_name}: {percentage:.1f}% (å¹³å‡æ¦‚ç‡: {avg_prob:.3f})")
                                                            print(f"      {' | '.join(action_info)}")

                                                    # ===== é¢å¤–è¯„ä¼°ï¼šä¸æ­¢ vs Randomï¼ˆåªç”¨äºè¯Šæ–­ï¼Œä¸å›çŒè®­ç»ƒï¼‰=====
                                                    try:
                                                        extra_opponents = getattr(self, "_eval_extra_opponents", "snapshot") or ""
                                                        extra_opponents = [s.strip().lower() for s in str(extra_opponents).split(",") if s.strip()]
                                                    except Exception:
                                                        extra_opponents = ["snapshot"]

                                                    try:
                                                        extra_games = int(getattr(self, "_eval_extra_games", 200))
                                                    except Exception:
                                                        extra_games = 200
                                                    extra_games = max(10, extra_games)

                                                    def _print_overall_line(label, tr):
                                                        ng = tr.get('games_played', 0)
                                                        if ng <= 0:
                                                            print(f"    {label}: æ— æœ‰æ•ˆå¯¹å±€")
                                                            return
                                                        overall_avg_return = tr.get('player0_avg_return', 0)
                                                        overall_win_rate = tr.get('player0_win_rate', 0) * 100
                                                        if bb is not None and bb > 0:
                                                            bb_value2 = overall_avg_return / bb
                                                            print(f"    {label}: {ng}å±€ | æ€»ä½“: å¹³å‡å›æŠ¥ {overall_avg_return:.2f} ({bb_value2:+.2f} BB), èƒœç‡ {overall_win_rate:.1f}%")
                                                        else:
                                                            print(f"    {label}: {ng}å±€ | æ€»ä½“: å¹³å‡å›æŠ¥ {overall_avg_return:.2f}, èƒœç‡ {overall_win_rate:.1f}%")
                                                        if tr.get('action_statistics'):
                                                            action_stats2 = tr['action_statistics']
                                                            sorted_actions2 = sorted(action_stats2.items(),
                                                                                    key=lambda x: x[1]['percentage'],
                                                                                    reverse=True)
                                                            parts = []
                                                            for action, st in sorted_actions2[:8]:
                                                                parts.append(f"{_get_action_name(action, game)}: {st['percentage']:.1f}% (p={st['avg_probability']:.3f})")
                                                            print(f"      åŠ¨ä½œ: {' | '.join(parts)}")

                                                    if extra_opponents:
                                                        # 1) vs Snapshotï¼šç”¨æ—§checkpointç­–ç•¥å½“å¯¹æ‰‹
                                                        if "snapshot" in extra_opponents and model_dir and save_prefix:
                                                            try:
                                                                gap = int(getattr(self, "_eval_snapshot_gap", 1000))
                                                            except Exception:
                                                                gap = 1000
                                                            gap = max(1, gap)
                                                            snap_iter = max(1, (iteration + 1) - gap)
                                                            snap_path = os.path.join(
                                                                model_dir, "checkpoints", f"iter_{snap_iter}",
                                                                f"{save_prefix}_policy_network_iter{snap_iter}.pt"
                                                            )
                                                            if os.path.exists(snap_path):
                                                                try:
                                                                    import torch
                                                                    snap_solver = ParallelDeepCFRSolver(
                                                                        game,
                                                                        num_workers=0,  # ä»…ç”¨äºå‡ºåŠ¨ä½œï¼Œä¸å¯åŠ¨worker
                                                                        policy_network_layers=tuple(self._policy_network_layers),
                                                                        advantage_network_layers=tuple(self._advantage_network_layers),
                                                                        num_iterations=1,
                                                                        num_traversals=1,
                                                                        learning_rate=self.learning_rate,
                                                                        batch_size_advantage=32,
                                                                        batch_size_strategy=32,
                                                                        memory_capacity=1,
                                                                        strategy_memory_capacity=1,
                                                                        device="cpu",
                                                                        gpu_ids=None,
                                                                        new_sample_ratio=self.new_sample_ratio,
                                                                        new_sample_window=self.new_sample_window,
                                                                        advantage_target_scale=self._advantage_target_scale,
                                                                        advantage_target_clip=self._advantage_target_clip,
                                                                        advantage_loss=self._advantage_loss_type,
                                                                        huber_delta=self._huber_delta
                                                                    )
                                                                    sd = torch.load(snap_path, map_location="cpu")
                                                                    snap_solver._policy_network.load_state_dict(sd)
                                                                    snap_solver._policy_network.eval()
                                                                    tr_snap = evaluate_with_test_games(
                                                                        game,
                                                                        self,
                                                                        num_games=extra_games,
                                                                        verbose=False,
                                                                        mode="vs_random",
                                                                        opponent_solver=snap_solver,
                                                                        opponent_strategy="solver"
                                                                    )
                                                                    _print_overall_line(f"æµ‹è¯•å¯¹å±€: {extra_games} å±€ (vs Snapshot@{snap_iter})", tr_snap)
                                                                except Exception as e:
                                                                    print(f"    æµ‹è¯•å¯¹å±€: vs Snapshot å¤±è´¥: {e}")
                                                            else:
                                                                print(f"    æµ‹è¯•å¯¹å±€: snapshot ä¸å­˜åœ¨ï¼ˆæœŸæœ› {snap_path}ï¼‰")

                                                        # 2) vs Tightï¼šèƒ½foldå°±foldï¼Œå¦åˆ™call/check
                                                        if "tight" in extra_opponents:
                                                            tr_tight = evaluate_with_test_games(
                                                                game,
                                                                self,
                                                                num_games=extra_games,
                                                                verbose=False,
                                                                mode="vs_random",
                                                                opponent_solver=None,
                                                                opponent_strategy="tight"
                                                            )
                                                            _print_overall_line(f"æµ‹è¯•å¯¹å±€: {extra_games} å±€ (vs Tight)", tr_tight)

                                                        # 3) vs Callï¼šä¼˜å…ˆcall/check
                                                        if "call" in extra_opponents:
                                                            tr_call = evaluate_with_test_games(
                                                                game,
                                                                self,
                                                                num_games=extra_games,
                                                                verbose=False,
                                                                mode="vs_random",
                                                                opponent_solver=None,
                                                                opponent_strategy="call"
                                                            )
                                                            _print_overall_line(f"æµ‹è¯•å¯¹å±€: {extra_games} å±€ (vs Call)", tr_call)
                                                        
                                                        # 4) vs æŒ‡å®šCheckpointï¼šä½¿ç”¨æŒ‡å®šçš„checkpointä½œä¸ºå¯¹æ‰‹
                                                        eval_checkpoint_path = getattr(self, "_eval_checkpoint_path", None)
                                                        if eval_checkpoint_path and os.path.exists(eval_checkpoint_path):
                                                            try:
                                                                import torch
                                                                import json
                                                                import glob
                                                                import re
                                                                
                                                                # æŸ¥æ‰¾ç­–ç•¥ç½‘ç»œæ–‡ä»¶
                                                                policy_files = glob.glob(os.path.join(eval_checkpoint_path, "*_policy_network*.pt"))
                                                                if not policy_files:
                                                                    # å°è¯•ä»çˆ¶ç›®å½•æŸ¥æ‰¾
                                                                    parent_dir = os.path.dirname(eval_checkpoint_path)
                                                                    policy_files = glob.glob(os.path.join(parent_dir, "*_policy_network*.pt"))
                                                                
                                                                if policy_files:
                                                                    # é€‰æ‹©æœ€æ–°çš„checkpointæ–‡ä»¶
                                                                    latest_file = None
                                                                    max_iter = 0
                                                                    for f in policy_files:
                                                                        match = re.search(r'_iter(\d+)\.pt$', f)
                                                                        if match:
                                                                            iter_num = int(match.group(1))
                                                                            if iter_num > max_iter:
                                                                                max_iter = iter_num
                                                                                latest_file = f
                                                                    if not latest_file:
                                                                        latest_file = policy_files[0]
                                                                    
                                                                    # åŠ è½½é…ç½®
                                                                    config_path = os.path.join(eval_checkpoint_path, "config.json")
                                                                    if not os.path.exists(config_path):
                                                                        parent_dir = os.path.dirname(eval_checkpoint_path)
                                                                        config_path = os.path.join(parent_dir, "config.json")
                                                                    
                                                                    checkpoint_solver = ParallelDeepCFRSolver(
                                                                        game,
                                                                        num_workers=0,
                                                                        policy_network_layers=tuple(self._policy_network_layers),
                                                                        advantage_network_layers=tuple(self._advantage_network_layers),
                                                                        num_iterations=1,
                                                                        num_traversals=1,
                                                                        learning_rate=self.learning_rate,
                                                                        batch_size_advantage=32,
                                                                        batch_size_strategy=32,
                                                                        memory_capacity=1,
                                                                        strategy_memory_capacity=1,
                                                                        device="cpu",
                                                                        gpu_ids=None,
                                                                        new_sample_ratio=self.new_sample_ratio,
                                                                        new_sample_window=self.new_sample_window,
                                                                        advantage_target_scale=self._advantage_target_scale,
                                                                        advantage_target_clip=self._advantage_target_clip,
                                                                        advantage_loss=self._advantage_loss_type,
                                                                        huber_delta=self._huber_delta
                                                                    )
                                                                    sd = torch.load(latest_file, map_location="cpu")
                                                                    checkpoint_solver._policy_network.load_state_dict(sd)
                                                                    checkpoint_solver._policy_network.eval()
                                                                    
                                                                    checkpoint_name = os.path.basename(eval_checkpoint_path)
                                                                    tr_checkpoint = evaluate_with_test_games(
                                                                        game,
                                                                        self,
                                                                        num_games=extra_games,
                                                                        verbose=False,
                                                                        mode="vs_random",
                                                                        opponent_solver=checkpoint_solver,
                                                                        opponent_strategy="solver"
                                                                    )
                                                                    _print_overall_line(f"æµ‹è¯•å¯¹å±€: {extra_games} å±€ (vs Checkpoint: {checkpoint_name})", tr_checkpoint)
                                                                else:
                                                                    print(f"    æµ‹è¯•å¯¹å±€: æ‰¾ä¸åˆ°checkpointæ–‡ä»¶ ({eval_checkpoint_path})")
                                                            except Exception as e:
                                                                print(f"    æµ‹è¯•å¯¹å±€: vs Checkpoint å¤±è´¥: {e}")
                                                                import traceback
                                                                traceback.print_exc()
                                                    
                                                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¼€å§‹è¿‡æ¸¡é˜¶æ®µ
                                                    win_rate = test_results.get('player0_win_rate', None)
                                                    avg_return = test_results.get('player0_avg_return', None)
                                                    
                                                    # è½¬æ¢ä¸ºBBå•ä½ï¼ˆå¦‚æœéœ€è¦ï¼‰
                                                    if avg_return is not None and bb is not None and bb > 0:
                                                        avg_return_bb = avg_return / bb
                                                    else:
                                                        avg_return_bb = avg_return
                                                    
                                                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¼€å§‹è¿‡æ¸¡é˜¶æ®µ
                                                    self.should_start_transition(iteration, advantage_losses, win_rate, avg_return_bb)
                                    except ImportError:
                                        pass  # training_evaluator ä¸å¯ç”¨
                                    except Exception as e:
                                        print(f" è¯„ä¼°å¤±è´¥: {e}")
                                
                                # 3. ä¿å­˜checkpoint
                                save_checkpoint(self, game, model_dir, save_prefix, iteration + 1)
                                get_logger().info("  âœ“ Checkpoint å·²ä¿å­˜")
                            except Exception as e:
                                print(f" å¤±è´¥: {e}")
            
            print()
            
            # è®­ç»ƒç­–ç•¥ç½‘ç»œï¼ˆæœ€ç»ˆè®­ç»ƒï¼Œä½¿ç”¨æœ€åä¸€æ¬¡è¿­ä»£å·ï¼‰
            print("  è®­ç»ƒç­–ç•¥ç½‘ç»œ...")
            final_iteration = self.num_iterations - 1 if start_iteration < self.num_iterations else start_iteration
            policy_loss = self._learn_strategy_network(current_iteration=final_iteration)
            
            total_time = time.time() - start_time
            print(f"\n  âœ“ è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f} ç§’")
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            if model_dir and save_prefix and game:
                get_logger().info(f"  ğŸ’¾ ä¿å­˜ä¸­æ–­æ—¶çš„ checkpoint (è¿­ä»£ {self._iteration})...")
                try:
                    save_checkpoint(self, game, model_dir, save_prefix, self._iteration)
                    get_logger().info(f"  âœ“ Checkpoint å·²ä¿å­˜")
                except Exception as e:
                    get_logger().error(f"  âœ— ä¿å­˜å¤±è´¥: {e}")
        finally:
            # åœæ­¢ Worker
            self._stop_workers()
        
        # è¿”å›ç­–ç•¥ç½‘ç»œã€ä¼˜åŠ¿ç½‘ç»œæŸå¤±å†å²å’Œç­–ç•¥ç½‘ç»œæŸå¤±å†å²
        final_policy_loss = policy_losses[-1] if policy_losses else None
        return self._policy_network, advantage_losses, final_policy_loss
    
    def action_probabilities(self, state, player_id=None):
        """è®¡ç®—åŠ¨ä½œæ¦‚ç‡ï¼ˆç”¨äºæ¨ç†ï¼‰"""
        del player_id
        cur_player = state.current_player()
        legal_actions = state.legal_actions(cur_player)
        info_state_vector = np.array(state.information_state_tensor())
        if len(info_state_vector.shape) == 1:
            info_state_vector = np.expand_dims(info_state_vector, axis=0)
        with torch.no_grad():
            logits = self._policy_network(
                torch.FloatTensor(info_state_vector).to(self.device)
            )
            probs = self._policy_sm(logits).cpu().numpy()
        
        # ç¡®ä¿åªè¿”å›åˆæ³•åŠ¨ä½œçš„æ¦‚ç‡ï¼Œå¹¶é‡æ–°å½’ä¸€åŒ–
        action_probs = {action: float(probs[0][action]) for action in legal_actions}
        total_prob = sum(action_probs.values())
        
        if total_prob > 1e-10:
            # é‡æ–°å½’ä¸€åŒ–
            action_probs = {a: p / total_prob for a, p in action_probs.items()}
        else:
            # å¦‚æœæ‰€æœ‰æ¦‚ç‡éƒ½æ¥è¿‘0ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
            action_probs = {a: 1.0 / len(legal_actions) for a in legal_actions}
            
        return action_probs


def load_checkpoint(solver, model_dir, save_prefix, game):
    """ä» checkpoint åŠ è½½ç½‘ç»œæƒé‡å’Œå¤šé˜¶æ®µè®­ç»ƒçŠ¶æ€
    
    Args:
        solver: ParallelDeepCFRSolver å®ä¾‹
        model_dir: æ¨¡å‹ç›®å½•
        save_prefix: ä¿å­˜å‰ç¼€
        game: æ¸¸æˆå®ä¾‹
        
    Returns:
        start_iteration: æ¢å¤çš„è¿­ä»£æ¬¡æ•°
        training_state: è®­ç»ƒçŠ¶æ€å­—å…¸ï¼ˆåŒ…å«switch_start_iterationã€win_rate_historyã€avg_return_historyï¼‰
    """
    import glob
    import re
    
    # æŸ¥æ‰¾æœ€æ–°çš„ checkpoint
    checkpoint_root = os.path.join(model_dir, "checkpoints")
    
    latest_file = None
    max_iter = 0
    
    # ä¼˜å…ˆä» checkpoints ç›®å½•åŠ è½½
    if os.path.exists(checkpoint_root):
        # å°è¯•æ–°çš„ç›®å½•ç»“æ„: checkpoints/iter_X/prefix_policy_iterX.pt
        iter_dirs = glob.glob(os.path.join(checkpoint_root, "iter_*"))
        for d in iter_dirs:
            match = re.search(r'iter_(\d+)$', d)
            if match:
                iter_num = int(match.group(1))
                policy_file = os.path.join(d, f"{save_prefix}_policy_network_iter{iter_num}.pt")
                if os.path.exists(policy_file) and iter_num > max_iter:
                    max_iter = iter_num
                    latest_file = policy_file
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼ˆæˆ–è€…æ˜¯æ—§ç»“æ„ï¼‰ï¼Œå°è¯•æ—§çš„æ‰å¹³ç»“æ„: checkpoints/prefix_policy_iterX.pt
        if latest_file is None:
            policy_files = glob.glob(os.path.join(checkpoint_root, f"{save_prefix}_policy_network_iter*.pt"))
            for f in policy_files:
                match = re.search(r'_iter(\d+)\.pt$', f)
                if match:
                    iter_num = int(match.group(1))
                    if iter_num > max_iter:
                        max_iter = iter_num
                        latest_file = f
            
        if latest_file:
            print(f"  æ‰¾åˆ° checkpoint: è¿­ä»£ {max_iter}")
            policy_path = latest_file
            start_iteration = max_iter
        else:
            policy_path = None
            start_iteration = 0
    else:
        # ... (åç»­é€»è¾‘ä¸å˜)
        policy_path = None
        start_iteration = 0

    if policy_path is None: # å¦‚æœ checkpoint æ²¡æ‰¾åˆ°ï¼Œå°è¯•åŠ è½½æœ€ç»ˆæ¨¡å‹
        policy_path = os.path.join(model_dir, f"{save_prefix}_policy_network.pt")
        if os.path.exists(policy_path):
            print(f"  æ‰¾åˆ°æœ€ç»ˆæ¨¡å‹")
            start_iteration = 0  # æœ€ç»ˆæ¨¡å‹æ²¡æœ‰è¿­ä»£ä¿¡æ¯
        else:
            policy_path = None
            start_iteration = 0
    
    if policy_path is None or not os.path.exists(policy_path):
        print(f"  âœ— æœªæ‰¾åˆ°å¯åŠ è½½çš„æ¨¡å‹")
        return 0, None
    
    # åŠ è½½ç­–ç•¥ç½‘ç»œ
    print(f"  åŠ è½½ç­–ç•¥ç½‘ç»œ: {policy_path}")
    policy_state = torch.load(policy_path, map_location=solver.device)
    policy_net = solver._policy_network
    if isinstance(policy_net, nn.DataParallel):
        policy_net.module.load_state_dict(policy_state)
    else:
        policy_net.load_state_dict(policy_state)
    print(f"  âœ“ ç­–ç•¥ç½‘ç»œå·²åŠ è½½")
    
    # åŠ è½½ä¼˜åŠ¿ç½‘ç»œ
    # ç¡®å®šä¼˜åŠ¿ç½‘ç»œæ‰€åœ¨çš„ç›®å½•
    if start_iteration > 0:
        # æ£€æŸ¥æ˜¯æ–°ç»“æ„è¿˜æ˜¯æ—§ç»“æ„
        if "iter_" in os.path.dirname(policy_path):
            # æ–°ç»“æ„: checkpoints/iter_X/
            adv_dir = os.path.dirname(policy_path)
            training_state_dir = adv_dir
        else:
            # æ—§ç»“æ„: checkpoints/
            adv_dir = checkpoint_root
            training_state_dir = checkpoint_root
    else:
        adv_dir = model_dir
        training_state_dir = model_dir

    for player in range(game.num_players()):
        if start_iteration > 0:
            adv_path = os.path.join(adv_dir, f"{save_prefix}_advantage_player_{player}_iter{start_iteration}.pt")
        else:
            adv_path = os.path.join(adv_dir, f"{save_prefix}_advantage_player_{player}.pt")
        
        if os.path.exists(adv_path):
            # ... (åŠ è½½é€»è¾‘ä¸å˜)
            adv_state = torch.load(adv_path, map_location=solver.device)
            adv_net = solver._advantage_networks[player]
            if isinstance(adv_net, nn.DataParallel):
                adv_net.module.load_state_dict(adv_state)
            else:
                adv_net.load_state_dict(adv_state)
            print(f"  âœ“ ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œå·²åŠ è½½")
        else:
            print(f"  âš ï¸ ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œæœªæ‰¾åˆ°: {adv_path}")
    
    # åŠ è½½å¤šé˜¶æ®µè®­ç»ƒçŠ¶æ€
    training_state = None
    if start_iteration > 0:
        training_state_path = os.path.join(training_state_dir, "training_state.json")
        if os.path.exists(training_state_path):
            import json
            with open(training_state_path, 'r') as f:
                training_state = json.load(f)
            print(f"  âœ“ å¤šé˜¶æ®µè®­ç»ƒçŠ¶æ€å·²åŠ è½½:")
            print(f"    - switch_start_iteration: {training_state.get('switch_start_iteration')}")
            print(f"    - win_rate_historyé•¿åº¦: {len(training_state.get('win_rate_history', []))}")
            print(f"    - avg_return_historyé•¿åº¦: {len(training_state.get('avg_return_history', []))}")
        else:
            print(f"  âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒçŠ¶æ€æ–‡ä»¶: {training_state_path}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
    
    return start_iteration, training_state


def create_save_directory(save_prefix, save_dir="models"):
    """åˆ›å»ºä¿å­˜ç›®å½•"""
    import time as time_module
    base_dir = save_dir
    if not os.path.exists(base_dir):
        os.makedirs(base_dir)
    
    model_dir = os.path.join(base_dir, save_prefix)
    if os.path.exists(model_dir):
        timestamp = time_module.strftime("%Y%m%d_%H%M%S")
        model_dir = f"{model_dir}_{timestamp}"
    
    os.makedirs(model_dir, exist_ok=True)
    return model_dir


def save_checkpoint(solver, game, model_dir, save_prefix, iteration, is_final=False):
    """ä¿å­˜ checkpoint"""
    if is_final:
        suffix = ""
        checkpoint_dir = model_dir
    else:
        suffix = f"_iter{iteration}"
        # å°†æ¯ä¸ª iteration çš„ checkpoint æ”¾å…¥ç‹¬ç«‹å­ç›®å½•
        checkpoint_dir = os.path.join(model_dir, "checkpoints", f"iter_{iteration}")
        os.makedirs(checkpoint_dir, exist_ok=True)
    
    # ä¿å­˜ç­–ç•¥ç½‘ç»œï¼ˆå¤„ç† DataParallelï¼‰
    policy_path = os.path.join(checkpoint_dir, f"{save_prefix}_policy_network{suffix}.pt")
    policy_net = solver._policy_network
    if isinstance(policy_net, nn.DataParallel):
        torch.save(policy_net.module.state_dict(), policy_path)
    else:
        torch.save(policy_net.state_dict(), policy_path)
    
    # ä¿å­˜ä¼˜åŠ¿ç½‘ç»œï¼ˆå¤„ç† DataParallelï¼‰
    for player in range(game.num_players()):
        advantage_path = os.path.join(checkpoint_dir, f"{save_prefix}_advantage_player_{player}{suffix}.pt")
        adv_net = solver._advantage_networks[player]
        if isinstance(adv_net, nn.DataParallel):
            torch.save(adv_net.module.state_dict(), advantage_path)
        else:
            torch.save(adv_net.state_dict(), advantage_path)
    
    # ä¿å­˜å¤šé˜¶æ®µè®­ç»ƒçŠ¶æ€
    training_state_path = os.path.join(checkpoint_dir, "training_state.json")
    import json
    training_state = {
        'switch_start_iteration': solver.switch_start_iteration,
        'win_rate_history': solver.win_rate_history,
        'avg_return_history': solver.avg_return_history,
        'iteration': iteration
    }
    with open(training_state_path, 'w') as f:
        json.dump(training_state, f, indent=2)
    
    return checkpoint_dir


def main():
    global logger
    # åˆå§‹åŒ–loggingï¼ˆè¾“å‡ºåˆ°stdoutï¼Œnohupä¼šæ•è·ï¼‰
    logger = setup_logging()
    
    # æ³¨å†Œä¿¡å·å¤„ç†ï¼Œç¡®ä¿è¢« kill æ—¶ä¹Ÿèƒ½æ¸…ç†å­è¿›ç¨‹
    def signal_handler(signum, frame):
        get_logger().info(f"\næ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨æ¸…ç†å¹¶é€€å‡º...")
        # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ç›´æ¥è°ƒç”¨ solver._stop_workers() å› ä¸º solver ä¸åœ¨ä½œç”¨åŸŸå†…
        # ä½†ç”±äº worker è¿›ç¨‹å·²è®¾ç½®ä¸º daemon=Trueï¼Œä¸»è¿›ç¨‹é€€å‡ºæ—¶å®ƒä»¬ä¼šè‡ªåŠ¨è¢«ç³»ç»Ÿæ¸…ç†
        sys.exit(0)
        
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)

    parser = argparse.ArgumentParser(description="å¤šè¿›ç¨‹å¹¶è¡Œ DeepCFR è®­ç»ƒ")
    parser.add_argument("--num_players", type=int, default=2, help="ç©å®¶æ•°é‡")
    parser.add_argument("--num_workers", type=int, default=4, help="Worker è¿›ç¨‹æ•°é‡")
    parser.add_argument("--num_iterations", type=int, default=100, help="è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--num_traversals", type=int, default=40, help="æ¯æ¬¡è¿­ä»£éå†æ¬¡æ•°")
    parser.add_argument("--policy_layers", type=int, nargs="+", default=[128, 128])
    parser.add_argument("--advantage_layers", type=int, nargs="+", default=[128, 128])
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--batch_size", type=int, default=2048)
    parser.add_argument("--memory_capacity", type=int, default=1000000,
                        help="ä¼˜åŠ¿ç½‘ç»œç»éªŒå›æ”¾ç¼“å†²åŒºå®¹é‡ï¼ˆæ¯ä¸ªç©å®¶ï¼Œé»˜è®¤: 1000000ï¼‰")
    parser.add_argument("--strategy_memory_capacity", type=int, default=None,
                        help="ç­–ç•¥ç½‘ç»œç»éªŒå›æ”¾ç¼“å†²åŒºå®¹é‡ï¼ˆæ‰€æœ‰ç©å®¶å…±äº«ï¼Œé»˜è®¤: ä½¿ç”¨memory_capacityï¼‰")
    parser.add_argument("--max_memory_gb", type=float, default=None,
                        help="æœ€å¤§å†…å­˜é™åˆ¶ï¼ˆGBï¼‰ï¼Œè¶…è¿‡æ­¤é™åˆ¶ä¼šè‡ªåŠ¨æ¸…ç†æ—§æ ·æœ¬ï¼ˆé»˜è®¤: ä¸é™åˆ¶ï¼‰")
    parser.add_argument("--queue_maxsize", type=int, default=50000,
                        help="é˜Ÿåˆ—æœ€å¤§å¤§å°ï¼Œé™ä½å¯å‡å°‘å†…å­˜å ç”¨ï¼ˆé»˜è®¤: 50000ï¼‰")
    parser.add_argument("--new_sample_ratio", type=float, default=0.5,
                        help="æ–°æ ·æœ¬å æ¯”ï¼ˆåˆ†å±‚åŠ æƒé‡‡æ ·ï¼Œé»˜è®¤0.5å³50%ï¼‰")
    parser.add_argument("--new_sample_window", type=int, default=0,
                        help="æ–°æ ·æœ¬çª—å£ï¼ˆæœ€è¿‘Wè½®ç®—â€œæ–°â€ï¼‰ï¼›0è¡¨ç¤ºä»…å½“å‰è½®ã€‚å»ºè®® 10~50")
    parser.add_argument("--betting_abstraction", type=str, default="fcpa")
    parser.add_argument("--save_prefix", type=str, default="deepcfr_parallel")
    parser.add_argument("--save_dir", type=str, default="models")
    parser.add_argument("--eval_interval", type=int, default=10)
    parser.add_argument("--checkpoint_interval", type=int, default=0, 
                        help="Checkpoint ä¿å­˜é—´éš”ï¼ˆ0=ä¸ä¿å­˜ä¸­é—´checkpointï¼‰")
    parser.add_argument("--skip_nashconv", action="store_true", 
                        help="è·³è¿‡ NashConv è®¡ç®—ï¼ˆ6äººå±€å¼ºçƒˆå»ºè®®ï¼‰")
    parser.add_argument("--use_gpu", action="store_true", default=True,
                        help="ä½¿ç”¨ GPU è®­ç»ƒ")
    parser.add_argument("--gpu_ids", type=int, nargs="+", default=None,
                        help="ä½¿ç”¨çš„ GPU ID åˆ—è¡¨ï¼ˆä¾‹å¦‚ --gpu_ids 0 1 2 3ï¼‰")
    parser.add_argument("--resume", type=str, default=None,
                        help="ä»æŒ‡å®šç›®å½•æ¢å¤è®­ç»ƒï¼ˆä¾‹å¦‚ --resume models/deepcfr_parallel_6pï¼‰")
    parser.add_argument("--eval_with_games", action="store_true",
                        help="è¯„ä¼°æ—¶è¿è¡Œæµ‹è¯•å¯¹å±€")
    parser.add_argument("--num_test_games", type=int, default=1000,
                        help="è¯„ä¼°æ—¶çš„æµ‹è¯•å¯¹å±€æ•°é‡ï¼ˆé»˜è®¤: 1000ï¼‰")
    parser.add_argument("--eval_extra_opponents", type=str, default="snapshot",
                        help="é¢å¤–è¯„ä¼°å¯¹æ‰‹ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼šsnapshot,tight,callã€‚ä¾‹å¦‚ 'snapshot,tight'ã€‚ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¸é¢å¤–è¯„ä¼°")
    parser.add_argument("--eval_extra_games", type=int, default=200,
                        help="é¢å¤–è¯„ä¼°æ¯ç§å¯¹æ‰‹çš„å¯¹å±€æ•°é‡ï¼ˆé»˜è®¤: 200ï¼Œé¿å…è¯„ä¼°è¿‡æ…¢ï¼‰")
    parser.add_argument("--eval_snapshot_gap", type=int, default=1000,
                        help="snapshotè¯„ä¼°ä½¿ç”¨çš„æ—§checkpointé—´éš”ï¼ˆé»˜è®¤: 1000ï¼Œä¾‹å¦‚ç”¨ iter-(gap) ä½œä¸ºå¯¹æ‰‹ï¼‰")
    parser.add_argument("--eval_checkpoint_path", type=str, default=None,
                        help="æŒ‡å®šcheckpointè·¯å¾„ä½œä¸ºè¯„æµ‹å¯¹æ‰‹ï¼ˆä¾‹å¦‚ï¼šmodels/deepcfr_6p_multi_20260116_171819/checkpoints/iter_114200ï¼‰")
    parser.add_argument("--blinds", type=str, default=None,
                        help="ç›²æ³¨é…ç½®ï¼Œæ ¼å¼ï¼š'å°ç›² å¤§ç›²' æˆ– '50 100 0 0 0 0'ï¼ˆå¤šäººåœºå®Œæ•´é…ç½®ï¼‰ã€‚å¦‚æœä¸æŒ‡å®šï¼Œå°†æ ¹æ®ç©å®¶æ•°é‡è‡ªåŠ¨ç”Ÿæˆ")
    parser.add_argument("--stack_size", type=int, default=None,
                        help="æ¯ä¸ªç©å®¶çš„åˆå§‹ç­¹ç ï¼ˆé»˜è®¤: 2000ï¼‰ã€‚å¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼2000")
    parser.add_argument("--advantage_train_steps", type=int, default=1,
                        help="ä¼˜åŠ¿ç½‘ç»œè®­ç»ƒæ­¥æ•°ï¼ˆæ¯æ¬¡è¿­ä»£ï¼Œé»˜è®¤: 1ï¼‰")
    parser.add_argument("--policy_train_steps", type=int, default=1,
                        help="ç­–ç•¥ç½‘ç»œè®­ç»ƒæ­¥æ•°ï¼ˆæ¯æ¬¡è¿­ä»£ï¼Œé»˜è®¤: 1ï¼‰")
    
    args = parser.parse_args()
    
    # å¤„ç†æ¢å¤è®­ç»ƒé…ç½®ï¼ˆå¿…é¡»åœ¨åˆ›å»ºæ¸¸æˆä¹‹å‰ï¼‰
    if args.resume:
        model_dir = args.resume
        if not os.path.exists(model_dir):
            print(f"âœ— æ¢å¤ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            import sys
            sys.exit(1)
        
        # å°è¯•ä» config.json è¯»å–é…ç½®
        config_path = os.path.join(model_dir, "config.json")
        if os.path.exists(config_path):
            import json
            with open(config_path, 'r') as f:
                resume_config = json.load(f)
            
            print(f"ä» {model_dir} æ¢å¤è®­ç»ƒ")
            
            # è‡ªåŠ¨è¦†ç›–å…³é”®å‚æ•°ï¼Œç¡®ä¿ç½‘ç»œç»“æ„ä¸€è‡´
            # æ³¨æ„ï¼šå‘½ä»¤è¡Œæ˜¾å¼æŒ‡å®šçš„å‚æ•°ä¼˜å…ˆçº§åº”è¯¥æ›´é«˜ï¼Œä½†ä¸ºäº†ç®€åŒ–ç»­è®­ï¼Œè¿™é‡Œé»˜è®¤ä½¿ç”¨ config ä¸­çš„å€¼
            # é™¤éç”¨æˆ·æƒ³è¦æ”¹å˜æŸäº›è®­ç»ƒè¶…å‚æ•°ï¼ˆå¦‚ batch_size, learning_rateï¼‰
            
            if 'num_players' in resume_config:
                args.num_players = resume_config['num_players']
            if 'policy_layers' in resume_config:
                args.policy_layers = resume_config['policy_layers']
            if 'advantage_layers' in resume_config:
                args.advantage_layers = resume_config['advantage_layers']
            if 'betting_abstraction' in resume_config:
                args.betting_abstraction = resume_config['betting_abstraction']
            if 'save_prefix' in resume_config:
                args.save_prefix = resume_config['save_prefix']
            if 'num_traversals' in resume_config:
                args.num_traversals = resume_config['num_traversals']
            if 'blinds' in resume_config and args.blinds is None:
                args.blinds = resume_config['blinds']
            if 'stack_size' in resume_config and args.stack_size is None:
                args.stack_size = resume_config['stack_size']
            if 'strategy_memory_capacity' in resume_config and args.strategy_memory_capacity is None:
                args.strategy_memory_capacity = resume_config['strategy_memory_capacity']
            if 'advantage_train_steps' in resume_config:
                args.advantage_train_steps = resume_config['advantage_train_steps']
            if 'policy_train_steps' in resume_config:
                args.policy_train_steps = resume_config['policy_train_steps']
            if 'new_sample_ratio' in resume_config:
                args.new_sample_ratio = resume_config['new_sample_ratio']
                
            print(f"  è‡ªåŠ¨åŠ è½½é…ç½®: {args.num_players}äººå±€, ç­–ç•¥å±‚{args.policy_layers}, ä¼˜åŠ¿å±‚{args.advantage_layers}")
            print(f"  save_prefix: {args.save_prefix}")
            print(f"  ä¼˜åŠ¿ç½‘ç»œè®­ç»ƒæ­¥æ•°: {args.advantage_train_steps}, ç­–ç•¥ç½‘ç»œè®­ç»ƒæ­¥æ•°: {args.policy_train_steps}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° config.jsonï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°")

    # åˆ›å»ºæ¸¸æˆ
    num_players = args.num_players
    
    # å¤„ç†ç›²æ³¨é…ç½®
    if args.blinds is not None:
        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†ç›²æ³¨ï¼Œç›´æ¥ä½¿ç”¨
        blinds_str = args.blinds
        print(f"  ä½¿ç”¨æŒ‡å®šçš„ç›²æ³¨é…ç½®: {blinds_str}")
    else:
        # å¦åˆ™æ ¹æ®ç©å®¶æ•°é‡è‡ªåŠ¨ç”Ÿæˆ
        if num_players == 2:
            blinds_str = "100 50"
        else:
            blinds_list = ["50", "100"] + ["0"] * (num_players - 2)
            blinds_str = " ".join(blinds_list)
        print(f"  ä½¿ç”¨é»˜è®¤ç›²æ³¨é…ç½®: {blinds_str}")
    
    # å¤„ç†ç­¹ç é…ç½®
    stack_size = args.stack_size if args.stack_size is not None else 2000
    stacks_str = " ".join([str(stack_size)] * num_players)
    print(f"  æ¯ä¸ªç©å®¶åˆå§‹ç­¹ç : {stack_size}")
    
    # å¤„ç†è¡ŒåŠ¨é¡ºåºé…ç½®
    if num_players == 2:
        first_player_str = "2 1 1 1"
    else:
        first_player_str = " ".join(["3"] + ["1"] * 3)
    
    game_string = (
        f"universal_poker("
        f"betting=nolimit,"
        f"numPlayers={num_players},"
        f"numRounds=4,"
        f"blind={blinds_str},"
        f"stack={stacks_str},"
        f"numHoleCards=2,"
        f"numBoardCards=0 3 1 1,"
        f"firstPlayer={first_player_str},"
        f"numSuits=4,"
        f"numRanks=13,"
        f"bettingAbstraction={args.betting_abstraction}"
        f")"
    )
    
    # è®¾ç½®è®¾å¤‡
    gpu_ids = None
    if args.use_gpu and torch.cuda.is_available():
        if args.gpu_ids is not None and len(args.gpu_ids) > 0:
            gpu_ids = args.gpu_ids
            device = f"cuda:{gpu_ids[0]}"
            if len(gpu_ids) > 1:
                print(f"ä½¿ç”¨å¤š GPU: {gpu_ids}")
                for gid in gpu_ids:
                    print(f"  GPU {gid}: {torch.cuda.get_device_name(gid)}")
            else:
                print(f"ä½¿ç”¨ GPU: {torch.cuda.get_device_name(gpu_ids[0])}")
        else:
            device = "cuda:0"
            print(f"ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        print("ä½¿ç”¨ CPU")
    
    print(f"åˆ›å»ºæ¸¸æˆ: {game_string}")
    game = pyspiel.load_game(game_string)
    
    # å¤„ç†æ¢å¤è®­ç»ƒ
    start_iteration = 0
    if args.resume:
        # ç›®å½•æ£€æŸ¥å·²åœ¨å‰é¢å®Œæˆ
        model_dir = args.resume
    else:
        # åˆ›å»ºæ–°çš„ä¿å­˜ç›®å½•
        model_dir = create_save_directory(args.save_prefix, args.save_dir)
    
    print(f"æ¨¡å‹ä¿å­˜ç›®å½•: {model_dir}")
    if args.checkpoint_interval > 0:
        print(f"Checkpoint ä¿å­˜é—´éš”: æ¯ {args.checkpoint_interval} æ¬¡è¿­ä»£")
    
    # åˆ›å»ºæ±‚è§£å™¨
    solver = ParallelDeepCFRSolver(
        game,
        num_workers=args.num_workers,
        policy_network_layers=tuple(args.policy_layers),
        advantage_network_layers=tuple(args.advantage_layers),
        num_iterations=args.num_iterations,
        num_traversals=args.num_traversals,
        learning_rate=args.learning_rate,
        batch_size_advantage=args.batch_size,
        batch_size_strategy=args.batch_size,
        memory_capacity=args.memory_capacity,
        strategy_memory_capacity=args.strategy_memory_capacity,
        device=device,
        gpu_ids=gpu_ids,
        max_memory_gb=args.max_memory_gb,
        queue_maxsize=args.queue_maxsize,
        new_sample_ratio=args.new_sample_ratio,
        new_sample_window=args.new_sample_window,
        advantage_network_train_steps=args.advantage_train_steps,
        policy_network_train_steps=args.policy_train_steps,
    )

    # è¯„ä¼°æ‰©å±•å‚æ•°ï¼ˆä»…ç”¨äº checkpoint è¯„ä¼°æ‰“å°ï¼Œä¸å½±å“è®­ç»ƒæ ·æœ¬ï¼‰
    solver._eval_extra_opponents = args.eval_extra_opponents
    solver._eval_extra_games = args.eval_extra_games
    solver._eval_snapshot_gap = args.eval_snapshot_gap
    solver._eval_checkpoint_path = args.eval_checkpoint_path
    
    # æ˜¾ç¤ºå†…å­˜é…ç½®
    if args.max_memory_gb:
        print(f"  å†…å­˜é™åˆ¶: {args.max_memory_gb}GB")
    print(f"  é˜Ÿåˆ—å¤§å°: {args.queue_maxsize}")
    
    # ç«‹å³ä¿å­˜é…ç½®ï¼ˆæ–¹ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æŸ¥çœ‹æˆ–æ¢å¤ï¼‰
    if not args.resume:
        import json
        config_path = os.path.join(model_dir, "config.json")
        config = {
            'num_players': num_players,
            'num_workers': args.num_workers,
            'num_iterations': args.num_iterations,
            'num_traversals': args.num_traversals,
            'policy_layers': args.policy_layers,
            'advantage_layers': args.advantage_layers,
            'learning_rate': args.learning_rate,
            'batch_size': args.batch_size,
            'memory_capacity': args.memory_capacity,
            'strategy_memory_capacity': args.strategy_memory_capacity,
            'max_memory_gb': args.max_memory_gb,
            'queue_maxsize': args.queue_maxsize,
            'new_sample_ratio': args.new_sample_ratio,
            'new_sample_window': args.new_sample_window,
            'advantage_train_steps': args.advantage_train_steps,
            'policy_train_steps': args.policy_train_steps,
            'eval_extra_opponents': args.eval_extra_opponents,
            'eval_extra_games': args.eval_extra_games,
            'eval_snapshot_gap': args.eval_snapshot_gap,
            'eval_checkpoint_path': args.eval_checkpoint_path,
            'betting_abstraction': args.betting_abstraction,
            'blinds': blinds_str,
            'stack_size': stack_size,
            'device': device,
            'gpu_ids': gpu_ids,
            'game_string': game_string,
            'multi_gpu': gpu_ids is not None and len(gpu_ids) > 1,
            'parallel': True,
            'use_feature_transform': True,
            'use_simple_feature': True,
            'save_prefix': args.save_prefix,
        }
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        print(f"  âœ“ é…ç½®å·²ä¿å­˜: {config_path}")
    
    # å¦‚æœæ˜¯æ¢å¤è®­ç»ƒï¼ŒåŠ è½½ checkpoint
    training_state = None
    if args.resume:
        print(f"\nåŠ è½½ checkpoint...")
        start_iteration, training_state = load_checkpoint(solver, model_dir, args.save_prefix, game)
        if start_iteration > 0:
            print(f"  âœ“ å°†ä»è¿­ä»£ {start_iteration + 1} ç»§ç»­è®­ç»ƒ")
        else:
            print(f"  âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆ checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
    else:
        start_iteration = 0
    
    # è®­ç»ƒï¼ˆå¸¦ checkpoint æ”¯æŒï¼‰
    policy_network, advantage_losses, policy_loss = solver.solve(
        verbose=True,
        eval_interval=args.eval_interval,
        checkpoint_interval=args.checkpoint_interval,
        model_dir=model_dir,
        save_prefix=args.save_prefix,
        game=game,
        start_iteration=start_iteration,
        eval_with_games=args.eval_with_games,
        num_test_games=args.num_test_games,
        training_state=training_state,
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    save_checkpoint(solver, game, model_dir, args.save_prefix, args.num_iterations, is_final=True)
    print(f"  âœ“ ç­–ç•¥ç½‘ç»œå·²ä¿å­˜: {os.path.join(model_dir, f'{args.save_prefix}_policy_network.pt')}")
    for player in range(num_players):
        print(f"  âœ“ ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œå·²ä¿å­˜")
    
    # ä¿å­˜é…ç½®
    import json
    config_path = os.path.join(model_dir, "config.json")
    config = {
        'num_players': num_players,
        'num_workers': args.num_workers,
        'num_iterations': args.num_iterations,
        'num_traversals': args.num_traversals,
        'policy_layers': args.policy_layers,
        'advantage_layers': args.advantage_layers,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size,
        'memory_capacity': args.memory_capacity,
        'max_memory_gb': args.max_memory_gb,
        'queue_maxsize': args.queue_maxsize,
        'new_sample_ratio': args.new_sample_ratio,
        'advantage_train_steps': args.advantage_train_steps,
        'policy_train_steps': args.policy_train_steps,
        'strategy_memory_capacity': args.strategy_memory_capacity,
        'betting_abstraction': args.betting_abstraction,
        'device': device,
        'gpu_ids': gpu_ids,
        'game_string': game_string,
        'multi_gpu': gpu_ids is not None and len(gpu_ids) > 1,
        'parallel': True,
        'use_feature_transform': True,
        'use_simple_feature': True,
        'save_prefix': args.save_prefix,
    }
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"  âœ“ é…ç½®å·²ä¿å­˜: {config_path}")
    
    # NashConv è®¡ç®—ï¼ˆå¯é€‰ï¼‰
    if not args.skip_nashconv:
        print(f"\nè®¡ç®— NashConv...")
        if num_players > 2:
            print(f"  âš ï¸ è­¦å‘Š: {num_players} äººæ¸¸æˆçš„ NashConv è®¡ç®—å¯èƒ½éå¸¸æ…¢æˆ–ä¸å¯è¡Œ")
            print(f"  å»ºè®®: ä½¿ç”¨ --skip_nashconv è·³è¿‡")
        try:
            from open_spiel.python import policy
            
            average_policy = policy.tabular_policy_from_callable(
                game, solver.action_probabilities
            )
            pyspiel_policy = policy.python_policy_to_pyspiel_policy(average_policy)
            conv = pyspiel.nash_conv(game, pyspiel_policy, use_cpp_br=True)
            print(f"  âœ“ NashConv: {conv:.6f}")
        except Exception as e:
            print(f"  âœ— NashConv è®¡ç®—å¤±è´¥: {e}")
            print(f"  å»ºè®®: ä½¿ç”¨ --skip_nashconv è·³è¿‡")
    else:
        print(f"\n  â­ï¸ è·³è¿‡ NashConv è®¡ç®—")
    
    print("\n" + "=" * 70)
    print("âœ“ å¹¶è¡Œ DeepCFR è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)


if __name__ == "__main__":
    main()

