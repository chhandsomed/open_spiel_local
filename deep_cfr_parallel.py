#!/usr/bin/env python3
"""
å¤šè¿›ç¨‹å¹¶è¡Œ DeepCFR è®­ç»ƒå™¨

æ¶æ„ï¼š
- å¤šä¸ª Worker è¿›ç¨‹å¹¶è¡Œéå†æ¸¸æˆæ ‘ï¼Œæ”¶é›†æ ·æœ¬
- ä¸»è¿›ç¨‹ä»å…±äº«ç¼“å†²åŒºé‡‡æ ·ï¼Œè®­ç»ƒç¥ç»ç½‘ç»œ
- ä½¿ç”¨å…±äº«å†…å­˜å®ç°è¿›ç¨‹é—´é«˜æ•ˆé€šä¿¡

ä¼˜åŠ¿ï¼š
- çœŸæ­£çš„å¹¶è¡ŒåŒ–ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸ CPU
- æ¸¸æˆæ ‘éå†ï¼ˆCPU å¯†é›†ï¼‰å’Œç½‘ç»œè®­ç»ƒï¼ˆGPU å¯†é›†ï¼‰å¯ä»¥åŒæ—¶è¿›è¡Œ
- çº¿æ€§æ‰©å±•ï¼šN ä¸ª Worker å¯ä»¥è·å¾—æ¥è¿‘ N å€çš„éå†é€Ÿåº¦
"""

import os
os.environ.setdefault('TORCH_COMPILE_DISABLE', '1')

import time
import signal
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.multiprocessing as mp
from multiprocessing import Process, Queue, Event, Value, Manager
from collections import namedtuple
import queue
import resource

try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

import pyspiel
from open_spiel.python.pytorch import deep_cfr
from deep_cfr_simple_feature import SimpleFeatureMLP


# æ ·æœ¬æ•°æ®ç»“æ„
AdvantageMemory = namedtuple("AdvantageMemory", "info_state iteration advantage action")
StrategyMemory = namedtuple("StrategyMemory", "info_state iteration strategy_action_probs")


class SharedBuffer:
    """å…±äº«å†…å­˜ç¼“å†²åŒºï¼Œç”¨äºè¿›ç¨‹é—´é€šä¿¡
    
    ä½¿ç”¨ Manager å®ç°è·¨è¿›ç¨‹å…±äº«çš„åˆ—è¡¨
    """
    
    def __init__(self, manager, capacity=1000000):
        self.capacity = capacity
        self._data = manager.list()
        self._add_calls = Value('i', 0)
        self._lock = manager.Lock()
    
    def add(self, element):
        """æ·»åŠ æ ·æœ¬ï¼ˆReservoir Samplingï¼‰"""
        with self._lock:
            if len(self._data) < self.capacity:
                self._data.append(element)
            else:
                idx = np.random.randint(0, self._add_calls.value + 1)
                if idx < self.capacity:
                    self._data[idx] = element
            self._add_calls.value += 1
    
    def sample(self, num_samples):
        """é‡‡æ ·"""
        with self._lock:
            if len(self._data) < num_samples:
                return list(self._data)
            indices = np.random.choice(len(self._data), num_samples, replace=False)
            return [self._data[i] for i in indices]
    
    def __len__(self):
        return len(self._data)
    
    def clear(self):
        with self._lock:
            while len(self._data) > 0:
                self._data.pop()
            self._add_calls.value = 0


class NetworkWrapper:
    """ç½‘ç»œåŒ…è£…å™¨ï¼Œæ”¯æŒè·¨è¿›ç¨‹å…±äº«
    
    ä½¿ç”¨å…±äº«å†…å­˜å­˜å‚¨ç½‘ç»œå‚æ•°ï¼ŒWorker å¯ä»¥è¯»å–æœ€æ–°å‚æ•°
    """
    
    def __init__(self, network, device='cpu'):
        self.network = network
        self.device = device
        self._state_dict = None
    
    def get_state_dict(self):
        """è·å–ç½‘ç»œå‚æ•°ï¼ˆç”¨äº Worker åŒæ­¥ï¼‰"""
        return {k: v.cpu().numpy() for k, v in self.network.state_dict().items()}
    
    def load_state_dict_from_numpy(self, numpy_dict):
        """ä» numpy å­—å…¸åŠ è½½å‚æ•°"""
        state_dict = {k: torch.from_numpy(v) for k, v in numpy_dict.items()}
        self.network.load_state_dict(state_dict)
        self.network = self.network.to(self.device)


def worker_process(
    worker_id,
    game_string,
    num_players,
    embedding_size,
    num_actions,
    advantage_network_layers,
    advantage_queues,  # æ¯ä¸ªç©å®¶ä¸€ä¸ªé˜Ÿåˆ—
    strategy_queue,
    network_params_queue,  # æ¥æ”¶æœ€æ–°ç½‘ç»œå‚æ•°
    stop_event,
    iteration_counter,
    num_traversals_per_batch,
    device='cpu',
    max_memory_gb=None  # Worker å†…å­˜é™åˆ¶
):
    """Worker è¿›ç¨‹ï¼šå¹¶è¡Œéå†æ¸¸æˆæ ‘
    
    Args:
        worker_id: Worker ID
        game_string: æ¸¸æˆé…ç½®å­—ç¬¦ä¸²
        num_players: ç©å®¶æ•°é‡
        embedding_size: ä¿¡æ¯çŠ¶æ€ç»´åº¦
        num_actions: åŠ¨ä½œæ•°é‡
        advantage_network_layers: ä¼˜åŠ¿ç½‘ç»œå±‚é…ç½®
        advantage_queues: ä¼˜åŠ¿æ ·æœ¬é˜Ÿåˆ—ï¼ˆæ¯ä¸ªç©å®¶ä¸€ä¸ªï¼‰
        strategy_queue: ç­–ç•¥æ ·æœ¬é˜Ÿåˆ—
        network_params_queue: ç½‘ç»œå‚æ•°é˜Ÿåˆ—
        stop_event: åœæ­¢ä¿¡å·
        iteration_counter: å½“å‰è¿­ä»£è®¡æ•°å™¨
        num_traversals_per_batch: æ¯æ‰¹éå†æ¬¡æ•°
        device: è®¡ç®—è®¾å¤‡
    """
    # è®¾ç½®è¿›ç¨‹åç§°
    try:
        import setproctitle
        setproctitle.setproctitle(f"deepcfr_worker_{worker_id}")
    except ImportError:
        pass
    except:
        pass
    
    # è®¾ç½®å†…å­˜é™åˆ¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if max_memory_gb:
        try:
            max_bytes = int(max_memory_gb * 1024 * 1024 * 1024)
            # è®¾ç½®è™šæ‹Ÿå†…å­˜é™åˆ¶ï¼ˆRLIMIT_ASï¼‰
            resource.setrlimit(resource.RLIMIT_AS, (max_bytes, max_bytes))
            print(f"[Worker {worker_id}] å·²è®¾ç½®å†…å­˜é™åˆ¶: {max_memory_gb}GB")
        except (ValueError, OSError) as e:
            print(f"[Worker {worker_id}] âš ï¸ æ— æ³•è®¾ç½®å†…å­˜é™åˆ¶: {e}")
    
    # è®¾ç½®å¼‚å¸¸å¤„ç†
    try:
        # è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
        if HAS_PSUTIL:
            try:
                process = psutil.Process()
                mem_info = process.memory_info()
                mem_mb = mem_info.rss / 1024 / 1024
                print(f"[Worker {worker_id}] å¯åŠ¨ï¼Œè®¾å¤‡: {device}ï¼Œåˆå§‹å†…å­˜: {mem_mb:.1f}MB")
            except:
                print(f"[Worker {worker_id}] å¯åŠ¨ï¼Œè®¾å¤‡: {device}")
        else:
            print(f"[Worker {worker_id}] å¯åŠ¨ï¼Œè®¾å¤‡: {device}")
        
        # åˆ›å»ºæ¸¸æˆ
        game = pyspiel.load_game(game_string)
        root_node = game.new_initial_state()
        
        # åˆ›å»ºæœ¬åœ°ä¼˜åŠ¿ç½‘ç»œï¼ˆç”¨äºé‡‡æ ·åŠ¨ä½œï¼‰
        advantage_networks = []
        for _ in range(num_players):
            # ä»æ¸¸æˆå­—ç¬¦ä¸²ä¸­è§£æmax_stack
            import re
            game_string = str(game)
            match = re.search(r'stack=([\d\s]+)', game_string)
            max_stack = 2000  # é»˜è®¤å€¼
            if match:
                stack_str = match.group(1).strip()
                stack_values = stack_str.split()
                if stack_values:
                    try:
                        max_stack = int(stack_values[0])
                    except ValueError:
                        pass
            
            net = SimpleFeatureMLP(
                embedding_size,
                list(advantage_network_layers),
                num_actions,
                num_players=num_players,
                max_game_length=game.max_game_length(),
                max_stack=max_stack
            )
            net = net.to(device)
            net.eval()
            advantage_networks.append(net)
        
        def sample_action_from_advantage(state, player):
            """ä½¿ç”¨ä¼˜åŠ¿ç½‘ç»œé‡‡æ ·åŠ¨ä½œ"""
            info_state = state.information_state_tensor(player)
            legal_actions = state.legal_actions(player)
            
            with torch.no_grad():
                state_tensor = torch.FloatTensor(np.expand_dims(info_state, axis=0)).to(device)
                raw_advantages = advantage_networks[player](state_tensor)[0].cpu().numpy()
            
            advantages = [max(0., advantage) for advantage in raw_advantages]
            cumulative_regret = sum(advantages[action] for action in legal_actions)
            
            matched_regrets = np.array([0.] * num_actions)
            if cumulative_regret > 0.:
                for action in legal_actions:
                    matched_regrets[action] = advantages[action] / cumulative_regret
            else:
                matched_regrets[max(legal_actions, key=lambda a: raw_advantages[a])] = 1
            
            return advantages, matched_regrets
        
        def traverse_game_tree(state, player, iteration):
            """éå†æ¸¸æˆæ ‘ï¼Œæ”¶é›†æ ·æœ¬"""
            nonlocal local_strategy_batch
            if state.is_terminal():
                return state.returns()[player]
            
            if state.is_chance_node():
                chance_outcome, chance_proba = zip(*state.chance_outcomes())
                action = np.random.choice(chance_outcome, p=chance_proba)
                return traverse_game_tree(state.child(action), player, iteration)
            
            if state.current_player() == player:
                expected_payoff = {}
                sampled_regret = {}
                
                _, strategy = sample_action_from_advantage(state, player)
                
                for action in state.legal_actions():
                    expected_payoff[action] = traverse_game_tree(
                        state.child(action), player, iteration
                    )
                
                cfv = sum(strategy[a] * expected_payoff[a] for a in state.legal_actions())
                
                for action in state.legal_actions():
                    sampled_regret[action] = expected_payoff[action] - cfv
                
                sampled_regret_arr = [0] * num_actions
                for action in sampled_regret:
                    sampled_regret_arr[action] = sampled_regret[action]
                
                # å‘é€ä¼˜åŠ¿æ ·æœ¬
                sample = AdvantageMemory(
                    state.information_state_tensor(),
                    iteration,
                    sampled_regret_arr,
                    action
                )
                
                # --- ä¿®æ”¹å¼€å§‹ ---
                # ç´¯ç§¯åˆ°æ‰¹æ¬¡
                if player not in local_advantage_batches:
                    local_advantage_batches[player] = []
                local_advantage_batches[player].append(sample)
                
                # å¦‚æœæ‰¹æ¬¡æ»¡äº†ï¼Œå‘é€
                if len(local_advantage_batches[player]) >= batch_size_limit:
                    try:
                        advantage_queues[player].put(local_advantage_batches[player], timeout=0.01)
                        local_advantage_batches[player] = []  # æ¸…ç©ºæ‰¹æ¬¡
                    except queue.Full:
                        pass  # é˜Ÿåˆ—æ»¡äº†å°±ä¸¢å¼ƒæ•´ä¸ªæ‰¹æ¬¡


                return cfv
            else:
                other_player = state.current_player()
                _, strategy = sample_action_from_advantage(state, other_player)
                
                probs = np.array(strategy)
                probs /= probs.sum()
                sampled_action = np.random.choice(range(num_actions), p=probs)
                
                sample = StrategyMemory(
                    state.information_state_tensor(other_player),
                    iteration,
                    strategy
                )
                
                # --- ä¿®æ”¹å¼€å§‹ ---
                # ç´¯ç§¯åˆ°æ‰¹æ¬¡
                local_strategy_batch.append(sample)
                
                # å¦‚æœæ‰¹æ¬¡æ»¡äº†ï¼Œå‘é€
                if len(local_strategy_batch) >= batch_size_limit:
                    try:
                        strategy_queue.put(local_strategy_batch, timeout=0.01)
                        local_strategy_batch = []  # æ¸…ç©ºæ‰¹æ¬¡
                    except queue.Full:
                        pass
                
                return traverse_game_tree(state.child(sampled_action), player, iteration)
        
        # ä¸»å¾ªç¯
        last_sync_iteration = 0
        # æœ¬åœ°æ‰¹æ¬¡ç¼“å†²åŒºï¼ˆå‡å°‘ Queue.put è°ƒç”¨é¢‘ç‡ï¼‰
        local_advantage_batches = {}  # {player_id: [samples]}
        local_strategy_batch = []
        batch_size_limit = 100  # æ¯ç§¯ç´¯ 100 ä¸ªæ ·æœ¬å‘é€ä¸€æ¬¡
        
        while not stop_event.is_set():
            current_iteration = iteration_counter.value
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åŒæ­¥ç½‘ç»œå‚æ•°
            try:
                while True:
                    params = network_params_queue.get_nowait()
                    for player in range(num_players):
                        if player in params:
                            numpy_dict = params[player]
                            state_dict = {k: torch.from_numpy(v) for k, v in numpy_dict.items()}
                            advantage_networks[player].load_state_dict(state_dict)
                            advantage_networks[player] = advantage_networks[player].to(device)
                    last_sync_iteration = current_iteration
            except queue.Empty:
                pass
            
            # éå†æ¸¸æˆæ ‘
            for player in range(num_players):
                for _ in range(num_traversals_per_batch):
                    if stop_event.is_set():
                        break
                    traverse_game_tree(root_node.clone(), player, current_iteration)
            
            # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒºï¼šæ— è®ºæ˜¯å¦è¾¾åˆ° batch_limitï¼Œéƒ½å°†æ‰‹ä¸­çš„æ ·æœ¬å‘é€å‡ºå»
            # è¿™é˜²æ­¢äº†åœ¨å¤šç©å®¶æ¸¸æˆä¸­ï¼ŒæŸäº›ç©å®¶çš„æ ·æœ¬ç§¯ç´¯å¤ªæ…¢å¯¼è‡´çš„ä¸»è¿›ç¨‹é¥¥é¥¿
            for p in list(local_advantage_batches.keys()):
                batch = local_advantage_batches[p]
                if batch:
                    try:
                        advantage_queues[p].put(batch, timeout=0.01)
                    except queue.Full:
                        pass # é˜Ÿåˆ—æ»¡å°±ç®—äº†
                # æ¸…ç©ºè¯¥ç©å®¶çš„ç¼“å†²åŒº
                local_advantage_batches[p] = []
            
            if local_strategy_batch:
                try:
                    strategy_queue.put(local_strategy_batch, timeout=0.01)
                    local_strategy_batch = []
                except queue.Full:
                    pass
        
    except Exception as e:
        print(f"\n[Worker {worker_id}] å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        raise e
    finally:
        print(f"[Worker {worker_id}] åœæ­¢")


class ParallelDeepCFRSolver:
    """å¤šè¿›ç¨‹å¹¶è¡Œ DeepCFR æ±‚è§£å™¨
    
    ä½¿ç”¨å¤šä¸ª Worker è¿›ç¨‹å¹¶è¡Œéå†æ¸¸æˆæ ‘ï¼Œä¸»è¿›ç¨‹è®­ç»ƒç½‘ç»œã€‚
    """
    
    def __init__(
        self,
        game,
        num_workers=4,
        policy_network_layers=(128, 128),
        advantage_network_layers=(128, 128),
        num_iterations=100,
        num_traversals=20,
        learning_rate=1e-4,
        batch_size_advantage=2048,
        batch_size_strategy=2048,
        memory_capacity=1000000,
        device='cuda',
        gpu_ids=None,  # å¤š GPU æ”¯æŒ
        sync_interval=1,  # æ¯å¤šå°‘æ¬¡è¿­ä»£åŒæ­¥ä¸€æ¬¡ç½‘ç»œå‚æ•°
        max_memory_gb=None,  # æœ€å¤§å†…å­˜é™åˆ¶ï¼ˆGBï¼‰ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
        queue_maxsize=50000,  # é˜Ÿåˆ—æœ€å¤§å¤§å°ï¼ˆé™ä½ä»¥å‡å°‘å†…å­˜å ç”¨ï¼‰
    ):
        self.game = game
        self.num_workers = num_workers
        self.num_players = game.num_players()
        self.num_iterations = num_iterations
        self.num_traversals = num_traversals
        self.learning_rate = learning_rate
        self.batch_size_advantage = batch_size_advantage
        self.batch_size_strategy = batch_size_strategy
        self.memory_capacity = memory_capacity
        self.sync_interval = sync_interval
        self.max_memory_gb = max_memory_gb
        self.queue_maxsize = queue_maxsize
        
        # å†…å­˜ç›‘æ§
        self._last_memory_check = 0
        self._memory_check_interval = 60  # æ¯60ç§’æ£€æŸ¥ä¸€æ¬¡å†…å­˜
        
        # å¤š GPU è®¾ç½®
        self.gpu_ids = gpu_ids
        self.use_multi_gpu = gpu_ids is not None and len(gpu_ids) > 1 and torch.cuda.is_available()
        
        if self.use_multi_gpu:
            self.device = torch.device(f"cuda:{gpu_ids[0]}")
            print(f"  å¤š GPU æ¨¡å¼: {gpu_ids}")
        else:
            self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # æ¸¸æˆä¿¡æ¯
        self._root_node = game.new_initial_state()
        self._embedding_size = len(self._root_node.information_state_tensor(0))
        self._num_actions = game.num_distinct_actions()
        
        # æ¸¸æˆå­—ç¬¦ä¸²ï¼ˆç”¨äº Worker åˆ›å»ºæ¸¸æˆï¼‰
        self._game_string = str(game)
        
        # ä»æ¸¸æˆé…ç½®ä¸­è§£æmax_stackï¼ˆç”¨äºå½’ä¸€åŒ–ä¸‹æ³¨ç»Ÿè®¡ç‰¹å¾ï¼‰
        self._max_stack = self._parse_max_stack_from_game_string(self._game_string)
        
        # ç½‘ç»œå±‚é…ç½®
        self._policy_network_layers = policy_network_layers
        self._advantage_network_layers = advantage_network_layers
        
        # åˆ›å»ºç½‘ç»œ
        self._create_networks()
        
        # å¤šè¿›ç¨‹ç»„ä»¶
        self._manager = None
        self._workers = []
        self._advantage_queues = []
        self._strategy_queue = None
        self._network_params_queues = []
        self._stop_event = None
        self._iteration_counter = None
        
        # æœ¬åœ°ç¼“å†²åŒº
        self._advantage_memories = [
            deep_cfr.ReservoirBuffer(memory_capacity) for _ in range(self.num_players)
        ]
        self._strategy_memories = deep_cfr.ReservoirBuffer(memory_capacity)
        
        self._iteration = 1
    
    def _parse_max_stack_from_game_string(self, game_string):
        """ä»æ¸¸æˆå­—ç¬¦ä¸²ä¸­è§£æmax_stackå€¼
        
        Args:
            game_string: æ¸¸æˆé…ç½®å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "universal_poker(...,stack=2000 2000 2000,...)"
        
        Returns:
            max_stack: å•ä¸ªç©å®¶çš„æœ€å¤§ç­¹ç é‡ï¼ˆé»˜è®¤2000ï¼‰
        """
        import re
        # åŒ¹é… stack=åé¢çš„å€¼
        match = re.search(r'stack=([\d\s]+)', game_string)
        if match:
            stack_str = match.group(1).strip()
            # è§£æç¬¬ä¸€ä¸ªç©å®¶çš„ç­¹ç é‡ï¼ˆæ‰€æœ‰ç©å®¶åº”è¯¥ç›¸åŒï¼‰
            stack_values = stack_str.split()
            if stack_values:
                try:
                    max_stack = int(stack_values[0])
                    return max_stack
                except ValueError:
                    pass
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼2000
        return 2000
    
    def _create_networks(self):
        """åˆ›å»ºç¥ç»ç½‘ç»œ"""
        # ç­–ç•¥ç½‘ç»œ
        policy_net = SimpleFeatureMLP(
            self._embedding_size,
            list(self._policy_network_layers),
            self._num_actions,
            num_players=self.num_players,
            max_game_length=self.game.max_game_length(),
            max_stack=self._max_stack
        )
        
        # éªŒè¯ç­–ç•¥ç½‘ç»œè¾“å…¥ç»´åº¦
        actual_input_size = policy_net.mlp.model[0]._weight.shape[1]
        expected_input_size = self._embedding_size + 7  # 7ç»´æ‰‹åŠ¨ç‰¹å¾ï¼ˆä½ç½®4 + æ‰‹ç‰Œå¼ºåº¦1 + ä¸‹æ³¨ç»Ÿè®¡2ï¼‰
        assert actual_input_size == expected_input_size, \
            f"ç­–ç•¥ç½‘ç»œè¾“å…¥ç»´åº¦é”™è¯¯: æœŸæœ› {expected_input_size}ï¼Œå®é™… {actual_input_size}"
        
        # å¤š GPU åŒ…è£…
        if self.use_multi_gpu:
            self._policy_network = nn.DataParallel(policy_net, device_ids=self.gpu_ids)
            self._policy_network = self._policy_network.to(self.device)
        else:
            self._policy_network = policy_net.to(self.device)
        
        self._policy_sm = nn.Softmax(dim=-1)
        self._loss_policy = nn.MSELoss()
        self._optimizer_policy = torch.optim.Adam(
            self._policy_network.parameters(), lr=self.learning_rate
        )
        
        # ä¼˜åŠ¿ç½‘ç»œï¼ˆæ¯ä¸ªç©å®¶ä¸€ä¸ªï¼‰
        self._advantage_networks = []
        self._optimizer_advantages = []
        for player in range(self.num_players):
            net = SimpleFeatureMLP(
                self._embedding_size,
                list(self._advantage_network_layers),
                self._num_actions,
                num_players=self.num_players,
                max_game_length=self.game.max_game_length(),
                max_stack=self._max_stack
            )
            
            # éªŒè¯ä¼˜åŠ¿ç½‘ç»œè¾“å…¥ç»´åº¦
            actual_input_size = net.mlp.model[0]._weight.shape[1]
            expected_input_size = self._embedding_size + 7  # 7ç»´æ‰‹åŠ¨ç‰¹å¾ï¼ˆä½ç½®4 + æ‰‹ç‰Œå¼ºåº¦1 + ä¸‹æ³¨ç»Ÿè®¡2ï¼‰
            assert actual_input_size == expected_input_size, \
                f"ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œè¾“å…¥ç»´åº¦é”™è¯¯: æœŸæœ› {expected_input_size}ï¼Œå®é™… {actual_input_size}"
            
            # å¤š GPU åŒ…è£…
            if self.use_multi_gpu:
                net = nn.DataParallel(net, device_ids=self.gpu_ids)
                net = net.to(self.device)
            else:
                net = net.to(self.device)
            
            self._advantage_networks.append(net)
            self._optimizer_advantages.append(
                torch.optim.Adam(net.parameters(), lr=self.learning_rate)
            )
        
        self._loss_advantages = nn.MSELoss(reduction="mean")
    
    def _start_workers(self):
        """å¯åŠ¨ Worker è¿›ç¨‹"""
        mp.set_start_method('spawn', force=True)
        
        self._manager = Manager()
        self._stop_event = Event()
        self._iteration_counter = Value('i', 1)
        
        # åˆ›å»ºé˜Ÿåˆ—ï¼ˆé™ä½ maxsize ä»¥å‡å°‘å†…å­˜å ç”¨ï¼‰
        self._advantage_queues = [Queue(maxsize=self.queue_maxsize) for _ in range(self.num_players)]
        self._strategy_queue = Queue(maxsize=self.queue_maxsize)
        self._network_params_queues = [Queue(maxsize=10) for _ in range(self.num_workers)]
        
        # è®¡ç®—æ¯ä¸ª Worker çš„éå†æ¬¡æ•°
        # å…³é”®ä¿®æ­£ï¼šä¸å†ä¸€æ¬¡æ€§åˆ†é… huge numberï¼Œè€Œæ˜¯åˆ†é…ä¸€ä¸ªå°æ‰¹æ¬¡ï¼Œè®© Worker å¿«é€Ÿå“åº”
        # ä¸»è¿›ç¨‹ä¼šé€šè¿‡æ§åˆ¶æ”¶é›†æ ·æœ¬çš„æ•°é‡æ¥ä¿è¯æ€»éå†æ¬¡æ•°
        traversals_per_worker = 10  # æ¯ä¸ª Worker æ¯æ¬¡åªè·‘ 10 æ¬¡éå†ï¼Œç„¶åæ£€æŸ¥åŒæ­¥
        
        # å¯åŠ¨ Worker
        print(f"  æ­£åœ¨å¹³æ»‘å¯åŠ¨ {self.num_workers} ä¸ª Worker (æ¯ç»„5ä¸ª)...")
        for i in range(self.num_workers):
            p = Process(
                target=worker_process,
                args=(
                    i,
                    self._game_string,
                    self.num_players,
                    self._embedding_size,
                    self._num_actions,
                    self._advantage_network_layers,
                    self._advantage_queues,
                    self._strategy_queue,
                    self._network_params_queues[i],
                    self._stop_event,
                    self._iteration_counter,
                    traversals_per_worker,
                    'cpu',  # Worker åœ¨ CPU ä¸Šè¿è¡Œ
                    self.max_memory_gb,  # Worker å†…å­˜é™åˆ¶
                ),
                daemon=True  # è®¾ç½®ä¸ºå®ˆæŠ¤è¿›ç¨‹ï¼Œä¸»è¿›ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨æ€æ­»
            )
            p.start()
            self._workers.append(p)
            
            # å¹³æ»‘å¯åŠ¨ï¼šæ¯å¯åŠ¨ 5 ä¸ª Worker ç¨å¾®æš‚åœä¸€ä¸‹ï¼Œé¿å…ç¬é—´ IO/CPU æ‹¥å µ
            if (i + 1) % 5 == 0:
                print(f"    å·²å¯åŠ¨ {i + 1}/{self.num_workers}...", end="\r", flush=True)
                time.sleep(1)
        
        print(f"\n  âœ“ å·²å¯åŠ¨ {self.num_workers} ä¸ª Worker è¿›ç¨‹")
        
        # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
        if HAS_PSUTIL:
            try:
                process = psutil.Process()
                mem_info = process.memory_info()
                mem_mb = mem_info.rss / 1024 / 1024
                print(f"  ä¸»è¿›ç¨‹å†…å­˜ä½¿ç”¨: {mem_mb:.1f}MB")
                
                # ä¼°ç®—æ€»å†…å­˜éœ€æ±‚
                # å®é™…å†…å­˜å ç”¨åŒ…æ‹¬ï¼š
                # 1. æ ·æœ¬æ•°æ®æœ¬èº«ï¼šinfo_state (numpyæ•°ç»„ï¼Œå¯èƒ½å‡ ç™¾åˆ°å‡ åƒä¸ªfloat32) + iteration + advantage/strategy
                # 2. Python å¯¹è±¡å¼€é”€ï¼šnamedtupleã€listã€numpyæ•°ç»„å¯¹è±¡ç­‰
                # 3. é˜Ÿåˆ—ç§¯å‹ï¼šå¦‚æœ Worker äº§ç”Ÿé€Ÿåº¦ > æ¶ˆè´¹é€Ÿåº¦
                # 4. Worker è¿›ç¨‹ï¼šæ¯ä¸ª Worker çš„ç½‘ç»œå‰¯æœ¬ã€æ¸¸æˆçŠ¶æ€ç­‰
                # 
                # ä¿å®ˆä¼°ç®—ï¼šæ¯ä¸ªæ ·æœ¬çº¦ 5-10KBï¼ˆåŒ…æ‹¬ Python å¯¹è±¡å¼€é”€ï¼‰
                # å¯¹äº6äººå±€ï¼Œmemory_capacity=1,000,000ï¼š
                #   - ä¼˜åŠ¿æ ·æœ¬ï¼š6 Ã— 1,000,000 Ã— 5KB = 30GB
                #   - ç­–ç•¥æ ·æœ¬ï¼š1 Ã— 1,000,000 Ã— 5KB = 5GB
                #   - æ€»è®¡ï¼šçº¦ 35GBï¼ˆä¸åŒ…æ‹¬é˜Ÿåˆ—å’Œ Workerï¼‰
                sample_size_kb = 5  # ä¿å®ˆä¼°ç®—ï¼Œå®é™…å¯èƒ½æ›´å¤§
                estimated_memory_gb = (
                    self.memory_capacity * self.num_players * sample_size_kb +  # ä¼˜åŠ¿æ ·æœ¬
                    self.memory_capacity * sample_size_kb +  # ç­–ç•¥æ ·æœ¬
                    self.queue_maxsize * (self.num_players + 1) * sample_size_kb +  # é˜Ÿåˆ—ç§¯å‹ï¼ˆæœ€åæƒ…å†µï¼‰
                    self.num_workers * 500  # Worker è¿›ç¨‹å¼€é”€ï¼ˆæ¯ä¸ª Worker çº¦ 500MBï¼‰
                ) / 1024 / 1024
                print(f"  ä¼°ç®—æ€»å†…å­˜éœ€æ±‚: {estimated_memory_gb:.2f}GB")
                print(f"    - ä¼˜åŠ¿æ ·æœ¬ç¼“å†²åŒº: {self.memory_capacity * self.num_players * sample_size_kb / 1024 / 1024:.2f}GB")
                print(f"    - ç­–ç•¥æ ·æœ¬ç¼“å†²åŒº: {self.memory_capacity * sample_size_kb / 1024 / 1024:.2f}GB")
                print(f"    - é˜Ÿåˆ—ç§¯å‹ï¼ˆæœ€åæƒ…å†µï¼‰: {self.queue_maxsize * (self.num_players + 1) * sample_size_kb / 1024 / 1024:.2f}GB")
                print(f"    - Worker è¿›ç¨‹: {self.num_workers * 500 / 1024 / 1024:.2f}GB")
                
                # è·å–ç³»ç»Ÿæ€»å†…å­˜
                try:
                    total_mem = psutil.virtual_memory().total / 1024 / 1024 / 1024
                    available_mem = psutil.virtual_memory().available / 1024 / 1024 / 1024
                    print(f"  ç³»ç»Ÿå†…å­˜: {total_mem:.1f}GB æ€»è®¡, {available_mem:.1f}GB å¯ç”¨")
                    
                    if estimated_memory_mb / 1024 > available_mem * 0.8:
                        print(f"  âš ï¸ è­¦å‘Š: ä¼°ç®—å†…å­˜éœ€æ±‚ ({estimated_memory_mb/1024:.1f}GB) æ¥è¿‘å¯ç”¨å†…å­˜ ({available_mem:.1f}GB)")
                        print(f"  å»ºè®®: å‡å°‘ --memory_capacity æˆ– --num_workers")
                except:
                    pass
            except Exception as e:
                print(f"  âš ï¸ è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
        else:
            print(f"  âš ï¸ æ— æ³•è·å–å†…å­˜ä¿¡æ¯ï¼ˆpsutil æœªå®‰è£…ï¼Œå»ºè®®å®‰è£…: pip install psutilï¼‰")
    
    def _stop_workers(self):
        """åœæ­¢ Worker è¿›ç¨‹"""
        self._stop_event.set()
        
        # æ¸…ç©ºæ‰€æœ‰é˜Ÿåˆ—ï¼Œé˜²æ­¢ Worker é˜»å¡åœ¨ put æ“ä½œ
        def drain_queue(q):
            try:
                while not q.empty():
                    q.get_nowait()
            except:
                pass
        
        for q in self._advantage_queues:
            drain_queue(q)
        drain_queue(self._strategy_queue)
        for q in self._network_params_queues:
            drain_queue(q)
        
        # ç­‰å¾… Worker é€€å‡º
        for p in self._workers:
            p.join(timeout=5)
            if p.is_alive():
                p.terminate()
                p.join(timeout=1)
        
        self._workers = []
        print("æ‰€æœ‰ Worker å·²åœæ­¢")
    
    def _sync_network_params(self):
        """åŒæ­¥ç½‘ç»œå‚æ•°åˆ°æ‰€æœ‰ Worker"""
        params = {}
        for player in range(self.num_players):
            # å¤„ç† DataParallel åŒ…è£…
            net = self._advantage_networks[player]
            if isinstance(net, nn.DataParallel):
                state_dict = net.module.state_dict()
            else:
                state_dict = net.state_dict()
            
            params[player] = {
                k: v.cpu().numpy() 
                for k, v in state_dict.items()
            }
        
        for q in self._network_params_queues:
            try:
                q.put_nowait(params)
            except queue.Full:
                pass
    
    def _check_and_cleanup_memory(self, force=False):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¿…è¦æ—¶æ¸…ç†æ—§æ ·æœ¬
        
        æ³¨æ„ï¼šæ¸…ç†æ ·æœ¬åªæ˜¯ç¼“è§£æªæ–½ï¼Œæ ¹æœ¬è§£å†³æ–¹æ¡ˆæ˜¯ï¼š
        1. å‡å°‘ memory_capacityï¼ˆæ¨èï¼‰
        2. å‡å°‘ num_workers
        3. å‡å°‘ queue_maxsize
        
        Args:
            force: å¼ºåˆ¶æ¸…ç†ï¼Œå³ä½¿å†…å­˜ä½¿ç”¨ä¸é«˜
        """
        if not HAS_PSUTIL or not self.max_memory_gb:
            return
        
        current_time = time.time()
        # æ¯60ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œé¿å…é¢‘ç¹æ£€æŸ¥
        if not force and current_time - self._last_memory_check < self._memory_check_interval:
            return
        
        self._last_memory_check = current_time
        
        try:
            process = psutil.Process()
            mem_info = process.memory_info()
            mem_gb = mem_info.rss / 1024 / 1024 / 1024
            
            # ä¸»åŠ¨æ¸…ç†ç­–ç•¥ï¼š
            # 1. å¦‚æœå†…å­˜ä½¿ç”¨è¶…è¿‡é™åˆ¶çš„90%ï¼Œæ¸…ç†æ—§æ ·æœ¬
            # 2. å¦‚æœç¼“å†²åŒºæ¥è¿‘æ»¡ï¼ˆ95%ï¼‰ï¼Œä¹Ÿä¸»åŠ¨æ¸…ç†ï¼ˆå³ä½¿å†…å­˜ä¸é«˜ï¼‰
            should_cleanup = False
            cleanup_reason = ""
            
            if mem_gb > self.max_memory_gb * 0.9 or force:
                should_cleanup = True
                cleanup_reason = f"å†…å­˜ä½¿ç”¨è¿‡é«˜ ({mem_gb:.2f}GB / {self.max_memory_gb}GB)"
            else:
                # æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦æ¥è¿‘æ»¡
                for player in range(self.num_players):
                    if len(self._advantage_memories[player]) >= self.memory_capacity * 0.95:
                        should_cleanup = True
                        cleanup_reason = f"ç¼“å†²åŒºæ¥è¿‘æ»¡ï¼ˆç©å®¶ {player}: {len(self._advantage_memories[player]):,}/{self.memory_capacity:,}ï¼‰"
                        break
                
                if len(self._strategy_memories) >= self.memory_capacity * 0.95:
                    should_cleanup = True
                    cleanup_reason = f"ç­–ç•¥ç¼“å†²åŒºæ¥è¿‘æ»¡ ({len(self._strategy_memories):,}/{self.memory_capacity:,})"
            
            if should_cleanup:
                print(f"\n  âš ï¸ {cleanup_reason}ï¼Œæ¸…ç†æ—§æ ·æœ¬...")
                if mem_gb > self.max_memory_gb * 0.9:
                    print(f"  âš ï¸ æ³¨æ„ï¼šæ¸…ç†æ ·æœ¬åªæ˜¯ç¼“è§£æªæ–½ï¼Œå»ºè®®å‡å°‘ --memory_capacity æˆ– --num_workers")
                
                # æ¸…ç†ç­–ç•¥ï¼šä¿ç•™æœ€æ–°çš„ 90% æ ·æœ¬ï¼ˆåŸºäº iterationï¼‰ï¼Œåˆ é™¤æœ€æ—§çš„ 10%
                # åªæ¸…ç†å°‘é‡æ ·æœ¬ï¼Œé¿å…å½±å“è®­ç»ƒè´¨é‡
                for player in range(self.num_players):
                    buffer = self._advantage_memories[player]
                    if len(buffer) > self.memory_capacity * 0.9:
                        # è·å–æ‰€æœ‰æ ·æœ¬å¹¶æŒ‰ iteration æ’åº
                        all_samples = list(buffer._data)
                        # æŒ‰ iteration é™åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
                        all_samples.sort(key=lambda x: x.iteration, reverse=True)
                        # ä¿ç•™æœ€æ–°çš„ 90%ï¼ˆåªåˆ é™¤æœ€æ—§çš„ 10%ï¼‰
                        keep_count = int(self.memory_capacity * 0.9)
                        samples_to_keep = all_samples[:keep_count]
                        # æ¸…ç©ºå¹¶é‡æ–°æ·»åŠ ä¿ç•™çš„æ ·æœ¬
                        buffer.clear()
                        for sample in samples_to_keep:
                            buffer.add(sample)
                        print(f"      ç©å®¶ {player} ä¼˜åŠ¿æ ·æœ¬: {len(all_samples):,} -> {len(samples_to_keep):,} (åˆ é™¤æœ€æ—§ {len(all_samples) - len(samples_to_keep):,} ä¸ª)")
                
                # æ¸…ç†ç­–ç•¥æ ·æœ¬
                if len(self._strategy_memories) > self.memory_capacity * 0.9:
                    all_samples = list(self._strategy_memories._data)
                    # æŒ‰ iteration é™åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
                    all_samples.sort(key=lambda x: x.iteration, reverse=True)
                    # ä¿ç•™æœ€æ–°çš„ 90%ï¼ˆåªåˆ é™¤æœ€æ—§çš„ 10%ï¼‰
                    keep_count = int(self.memory_capacity * 0.9)
                    samples_to_keep = all_samples[:keep_count]
                    # æ¸…ç©ºå¹¶é‡æ–°æ·»åŠ ä¿ç•™çš„æ ·æœ¬
                    self._strategy_memories.clear()
                    for sample in samples_to_keep:
                        self._strategy_memories.add(sample)
                    print(f"      ç­–ç•¥æ ·æœ¬: {len(all_samples):,} -> {len(samples_to_keep):,} (åˆ é™¤æœ€æ—§ {len(all_samples) - len(samples_to_keep):,} ä¸ª)")
                
                # æ¸…ç†é˜Ÿåˆ—ä¸­çš„ç§¯å‹æ ·æœ¬ï¼ˆå¦‚æœé˜Ÿåˆ—æ¥è¿‘æ»¡ï¼‰
                # è¿™æ˜¯æ›´é‡è¦çš„æ¸…ç†ï¼Œå› ä¸ºé˜Ÿåˆ—ç§¯å‹ä¼šå ç”¨å¤§é‡å†…å­˜
                # ä¸»åŠ¨æ¸…ç†ï¼šå³ä½¿å†…å­˜ä¸é«˜ï¼Œå¦‚æœé˜Ÿåˆ—ç§¯å‹ä¸¥é‡ä¹Ÿè¦æ¸…ç†
                for player, q in enumerate(self._advantage_queues):
                    queue_size = q.qsize()
                    if queue_size > self.queue_maxsize * 0.7:  # é™ä½é˜ˆå€¼ï¼Œæ›´ä¸»åŠ¨æ¸…ç†
                        # ä¸¢å¼ƒé˜Ÿåˆ—ä¸­æœ€æ—§çš„ä¸€åŠæ ·æœ¬
                        to_discard = queue_size // 2
                        for _ in range(to_discard):
                            try:
                                q.get_nowait()
                            except queue.Empty:
                                break
                        print(f"      ç©å®¶ {player} é˜Ÿåˆ—æ¸…ç†: {queue_size} -> {q.qsize()} (ä¸¢å¼ƒäº† {to_discard} ä¸ªç§¯å‹æ ·æœ¬)")
                
                strategy_queue_size = self._strategy_queue.qsize()
                if strategy_queue_size > self.queue_maxsize * 0.7:  # é™ä½é˜ˆå€¼ï¼Œæ›´ä¸»åŠ¨æ¸…ç†
                    to_discard = strategy_queue_size // 2
                    for _ in range(to_discard):
                        try:
                            self._strategy_queue.get_nowait()
                        except queue.Empty:
                            break
                    print(f"      ç­–ç•¥é˜Ÿåˆ—æ¸…ç†: {strategy_queue_size} -> {self._strategy_queue.qsize()} (ä¸¢å¼ƒäº† {to_discard} ä¸ªç§¯å‹æ ·æœ¬)")
                
                # å¼ºåˆ¶ Python åƒåœ¾å›æ”¶
                import gc
                gc.collect()
                
                # æ£€æŸ¥æ¸…ç†åçš„å†…å­˜
                new_mem_info = process.memory_info()
                new_mem_gb = new_mem_info.rss / 1024 / 1024 / 1024
                print(f"  âœ“ æ¸…ç†å®Œæˆï¼Œå†…å­˜ä½¿ç”¨: {new_mem_gb:.2f}GB (é‡Šæ”¾äº† {mem_gb - new_mem_gb:.2f}GB)")
                print(f"  ğŸ’¡ å»ºè®®ï¼šå¦‚æœé¢‘ç¹å‡ºç°å†…å­˜æ¸…ç†ï¼Œè¯·å‡å°‘ --memory_capacity æˆ– --num_workers")
        except Exception as e:
            print(f"  âš ï¸ å†…å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def _collect_samples(self, timeout=0.1):
        """ä»é˜Ÿåˆ—æ”¶é›†æ ·æœ¬"""
        # æ£€æŸ¥å¹¶æ¸…ç†å†…å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self._check_and_cleanup_memory()
        
        # æ£€æŸ¥ Worker çŠ¶æ€ï¼ˆå¢å¼ºç‰ˆï¼šæ£€æŸ¥é€€å‡ºç ï¼‰
        dead_workers = []
        for i, p in enumerate(self._workers):
            if not p.is_alive():
                exit_code = p.exitcode
                dead_workers.append((i, exit_code))
        
        if dead_workers:
            worker_info = ", ".join([f"Worker {wid} (é€€å‡ºç : {ec})" for wid, ec in dead_workers])
            error_msg = f"æ£€æµ‹åˆ° Worker å·²æ­»äº¡: {worker_info}ã€‚è®­ç»ƒæ— æ³•ç»§ç»­ã€‚"
            # å¦‚æœé€€å‡ºç ä¸ä¸º0ï¼Œå¯èƒ½æ˜¯OOMæˆ–å…¶ä»–ä¸¥é‡é”™è¯¯
            for _, exit_code in dead_workers:
                if exit_code != 0:
                    error_msg += f"\n  å¯èƒ½çš„é”™è¯¯åŸå› : é€€å‡ºç  {exit_code} é€šå¸¸è¡¨ç¤ºè¿›ç¨‹è¢«ç³»ç»Ÿæ€æ­»ï¼ˆå¦‚OOMï¼‰"
            raise RuntimeError(error_msg)

        # æ”¶é›†ä¼˜åŠ¿æ ·æœ¬ï¼ˆå¸¦ä¸»åŠ¨å†…å­˜ä¿æŠ¤ï¼‰
        for player in range(self.num_players):
            collected_count = 0
            max_collect = 10000 
            while collected_count < max_collect:
                try:
                    batch = self._advantage_queues[player].get_nowait()
                    # æ£€æŸ¥æ˜¯å•ä¸ªæ ·æœ¬è¿˜æ˜¯æ‰¹æ¬¡
                    if isinstance(batch, list):
                        # å¦‚æœç¼“å†²åŒºæ¥è¿‘æ»¡ï¼Œåªæ·»åŠ éƒ¨åˆ†æ ·æœ¬ï¼ˆé˜²æ­¢æº¢å‡ºï¼‰
                        buffer = self._advantage_memories[player]
                        if len(buffer) >= self.memory_capacity * 0.95:
                            # ç¼“å†²åŒºæ¥è¿‘æ»¡ï¼Œåªæ·»åŠ  50% çš„æ ·æœ¬
                            samples_to_add = batch[:len(batch)//2]
                            for sample in samples_to_add:
                                buffer.add(sample)
                            collected_count += len(samples_to_add)
                        else:
                            for sample in batch:
                                buffer.add(sample)
                            collected_count += len(batch)
                    else:
                        # å•ä¸ªæ ·æœ¬ï¼šå¦‚æœç¼“å†²åŒºæ¥è¿‘æ»¡ï¼Œè·³è¿‡
                        buffer = self._advantage_memories[player]
                        if len(buffer) < self.memory_capacity * 0.95:
                            buffer.add(batch)
                            collected_count += 1
                except queue.Empty:
                    break
        
        # æ”¶é›†ç­–ç•¥æ ·æœ¬ï¼ˆå¸¦ä¸»åŠ¨å†…å­˜ä¿æŠ¤ï¼‰
        collected_count = 0
        max_collect = 10000
        while collected_count < max_collect:
            try:
                batch = self._strategy_queue.get_nowait()
                # æ£€æŸ¥æ˜¯å•ä¸ªæ ·æœ¬è¿˜æ˜¯æ‰¹æ¬¡
                if isinstance(batch, list):
                    # å¦‚æœç¼“å†²åŒºæ¥è¿‘æ»¡ï¼Œåªæ·»åŠ éƒ¨åˆ†æ ·æœ¬ï¼ˆé˜²æ­¢æº¢å‡ºï¼‰
                    if len(self._strategy_memories) >= self.memory_capacity * 0.95:
                        # ç¼“å†²åŒºæ¥è¿‘æ»¡ï¼Œåªæ·»åŠ  50% çš„æ ·æœ¬
                        samples_to_add = batch[:len(batch)//2]
                        for sample in samples_to_add:
                            self._strategy_memories.add(sample)
                        collected_count += len(samples_to_add)
                    else:
                        for sample in batch:
                            self._strategy_memories.add(sample)
                        collected_count += len(batch)
                else:
                    # å•ä¸ªæ ·æœ¬ï¼šå¦‚æœç¼“å†²åŒºæ¥è¿‘æ»¡ï¼Œè·³è¿‡
                    if len(self._strategy_memories) < self.memory_capacity * 0.95:
                        self._strategy_memories.add(batch)
                        collected_count += 1
            except queue.Empty:
                break
    
    def _learn_advantage_network(self, player):
        """è®­ç»ƒä¼˜åŠ¿ç½‘ç»œ"""
        num_samples = len(self._advantage_memories[player])
        if num_samples < 32:  # æœ€å°‘éœ€è¦ 32 ä¸ªæ ·æœ¬æ‰è®­ç»ƒ
            return None
        
        # ä½¿ç”¨å®é™…æ ·æœ¬æ•°å’Œ batch_size çš„è¾ƒå°å€¼
        actual_batch_size = min(num_samples, self.batch_size_advantage)
        samples = self._advantage_memories[player].sample(actual_batch_size)
        
        info_states = []
        advantages = []
        iterations = []
        for s in samples:
            info_states.append(s.info_state)
            advantages.append(s.advantage)
            iterations.append([s.iteration])
        
        self._optimizer_advantages[player].zero_grad()
        advantages_tensor = torch.FloatTensor(np.array(advantages)).to(self.device)
        iters = torch.FloatTensor(np.sqrt(np.array(iterations))).to(self.device)
        outputs = self._advantage_networks[player](
            torch.FloatTensor(np.array(info_states)).to(self.device)
        )
        loss = self._loss_advantages(iters * outputs, iters * advantages_tensor)
        loss.backward()
        self._optimizer_advantages[player].step()
        
        return loss.detach().cpu().numpy()
    
    def _learn_strategy_network(self):
        """è®­ç»ƒç­–ç•¥ç½‘ç»œ"""
        num_samples = len(self._strategy_memories)
        if num_samples < 32:  # æœ€å°‘éœ€è¦ 32 ä¸ªæ ·æœ¬æ‰è®­ç»ƒ
            return None
        
        # ä½¿ç”¨å®é™…æ ·æœ¬æ•°å’Œ batch_size çš„è¾ƒå°å€¼
        actual_batch_size = min(num_samples, self.batch_size_strategy)
        samples = self._strategy_memories.sample(actual_batch_size)
        
        info_states = []
        action_probs = []
        iterations = []
        for s in samples:
            info_states.append(s.info_state)
            action_probs.append(s.strategy_action_probs)
            iterations.append([s.iteration])
        
        self._optimizer_policy.zero_grad()
        iters = torch.FloatTensor(np.sqrt(np.array(iterations))).to(self.device)
        ac_probs = torch.FloatTensor(np.array(np.squeeze(action_probs))).to(self.device)
        logits = self._policy_network(
            torch.FloatTensor(np.array(info_states)).to(self.device)
        )
        outputs = self._policy_sm(logits)
        loss = self._loss_policy(iters * outputs, iters * ac_probs)
        loss.backward()
        self._optimizer_policy.step()
        
        return loss.detach().cpu().numpy()
    
    def solve(self, verbose=True, eval_interval=10, checkpoint_interval=0, 
              model_dir=None, save_prefix=None, game=None, start_iteration=0,
              eval_with_games=False, num_test_games=50):
        """è¿è¡Œå¹¶è¡Œ DeepCFR è®­ç»ƒ
        
        Args:
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            eval_interval: è¯„ä¼°é—´éš”
            checkpoint_interval: checkpoint ä¿å­˜é—´éš”ï¼ˆ0=ä¸ä¿å­˜ï¼‰
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            save_prefix: ä¿å­˜æ–‡ä»¶å‰ç¼€
            game: æ¸¸æˆå®ä¾‹ï¼ˆç”¨äºä¿å­˜ checkpointï¼‰
            start_iteration: èµ·å§‹è¿­ä»£æ¬¡æ•°ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
            eval_with_games: æ˜¯å¦åœ¨è¯„ä¼°æ—¶è¿è¡Œæµ‹è¯•å¯¹å±€
        
        Returns:
            policy_network: è®­ç»ƒå¥½çš„ç­–ç•¥ç½‘ç»œ
            advantage_losses: ä¼˜åŠ¿ç½‘ç»œæŸå¤±å†å²
            policy_loss: ç­–ç•¥ç½‘ç»œæœ€ç»ˆæŸå¤±
        """
        print("=" * 70)
        print("å¹¶è¡Œ DeepCFR è®­ç»ƒ")
        print("=" * 70)
        print(f"  Worker æ•°é‡: {self.num_workers}")
        print(f"  è¿­ä»£æ¬¡æ•°: {self.num_iterations}")
        if start_iteration > 0:
            print(f"  ä»è¿­ä»£ {start_iteration + 1} æ¢å¤")
        print(f"  æ¯æ¬¡è¿­ä»£éå†æ¬¡æ•°: {self.num_traversals}")
        print(f"  è®¾å¤‡: {self.device}")
        print()
        
        # å¯åŠ¨ Worker
        self._start_workers()
        
        advantage_losses = {p: [] for p in range(self.num_players)}
        start_time = time.time()
        
        try:
            # ç­‰å¾… Worker å¯åŠ¨å¹¶å¼€å§‹äº§ç”Ÿæ ·æœ¬
            print("  ç­‰å¾… Worker å¯åŠ¨...", end="", flush=True)
            warmup_time = 0
            max_warmup = 30  # æœ€å¤šç­‰å¾… 30 ç§’
            while warmup_time < max_warmup:
                time.sleep(1)
                warmup_time += 1
                self._collect_samples()
                total_samples = sum(len(m) for m in self._advantage_memories)
                if total_samples > 0:
                    print(f" å°±ç»ª (è€—æ—¶ {warmup_time} ç§’ï¼Œå·²æ”¶é›† {total_samples} ä¸ªæ ·æœ¬)")
                    break
                print(".", end="", flush=True)
            else:
                print(f" è­¦å‘Š: Worker å¯åŠ¨è¶…æ—¶ï¼Œç»§ç»­è®­ç»ƒ...")
            
            for iteration in range(start_iteration, self.num_iterations):
                iter_start = time.time()
                
                # æ›´æ–°è¿­ä»£è®¡æ•°å™¨
                self._iteration_counter.value = iteration + 1
                
                # åŠ¨æ€æ”¶é›†æ ·æœ¬ï¼šç›´åˆ°æ”¶é›†åˆ°è¶³å¤Ÿæ•°é‡çš„æ–°æ ·æœ¬
                # è¿™æ ·å¯ä»¥ç¡®ä¿æ¯æ¬¡è¿­ä»£çš„æ•°æ®é‡æ˜¯æ’å®šçš„ï¼Œä¸å— Worker é€Ÿåº¦å½±å“
                # åŒæ—¶é€šè¿‡å¾ªç¯ sleep(1) é¿å…äº†ä¸»è¿›ç¨‹é•¿æ—¶é—´æ— å“åº”
                
                current_total_samples = sum(len(m) for m in self._advantage_memories)
                # ç›®æ ‡ï¼šæœ¬è½®æ–°å¢ num_traversals ä¸ªæ ·æœ¬
                # æ³¨æ„ï¼šç”±äºå¯èƒ½æœ‰å¤šä¸ª Worker åŒæ—¶æäº¤ï¼Œå¯èƒ½ä¼šç•¥å¤šä¸€ç‚¹ï¼Œæ²¡å…³ç³»
                target_total_samples = current_total_samples + self.num_traversals
                
                # è®¾ç½®ä¸€ä¸ªè¶…æ—¶ä¿æŠ¤ï¼ˆä¾‹å¦‚ 10 åˆ†é’Ÿï¼‰ï¼Œé˜²æ­¢ Worker å…¨éƒ¨æŒ‚æ­»å¯¼è‡´ä¸»è¿›ç¨‹æ­»å¾ªç¯
                collection_start_time = time.time()
                last_sample_count = current_total_samples
                no_progress_count = 0  # è¿ç»­æ— è¿›å±•æ¬¡æ•°
                
                while True:
                    self._collect_samples()
                    new_current_samples = sum(len(m) for m in self._advantage_memories)
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾æ ‡
                    if new_current_samples >= target_total_samples:
                        break
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¿›å±•
                    if new_current_samples > last_sample_count:
                        last_sample_count = new_current_samples
                        no_progress_count = 0
                    else:
                        no_progress_count += 1
                    
                    # æ£€æŸ¥è¶…æ—¶ (10åˆ†é’Ÿ)
                    elapsed_time = time.time() - collection_start_time
                    if elapsed_time > 600:
                        collected = new_current_samples - current_total_samples
                        if verbose:
                            print(f"\n  âš ï¸ è­¦å‘Š: æ ·æœ¬æ”¶é›†è¶…æ—¶ (å·²æ”¶é›† {collected}/{self.num_traversals})")
                            # è¯Šæ–­ä¿¡æ¯
                            print(f"    è¯Šæ–­ä¿¡æ¯:")
                            print(f"      - è€—æ—¶: {elapsed_time:.1f}ç§’")
                            print(f"      - å½“å‰ä¼˜åŠ¿æ ·æœ¬æ€»æ•°: {new_current_samples:,}")
                            print(f"      - ç­–ç•¥æ ·æœ¬æ€»æ•°: {len(self._strategy_memories):,}")
                            
                            # æ£€æŸ¥ Worker çŠ¶æ€
                            alive_workers = sum(1 for p in self._workers if p.is_alive())
                            print(f"      - å­˜æ´»çš„ Worker: {alive_workers}/{self.num_workers}")
                            
                            # æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
                            queue_sizes = [q.qsize() for q in self._advantage_queues]
                            total_queue_size = sum(queue_sizes)
                            print(f"      - é˜Ÿåˆ—ä¸­å¾…å¤„ç†æ ·æœ¬: {total_queue_size:,}")
                            
                            # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
                            if HAS_PSUTIL:
                                try:
                                    process = psutil.Process()
                                    mem_info = process.memory_info()
                                    mem_mb = mem_info.rss / 1024 / 1024
                                    mem_percent = process.memory_percent()
                                    print(f"      - ä¸»è¿›ç¨‹å†…å­˜ä½¿ç”¨: {mem_mb:.1f}MB ({mem_percent:.1f}%)")
                                    
                                    # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
                                    sys_mem = psutil.virtual_memory()
                                    print(f"      - ç³»ç»Ÿå†…å­˜: {sys_mem.percent:.1f}% å·²ä½¿ç”¨ ({sys_mem.used/1024/1024/1024:.1f}GB / {sys_mem.total/1024/1024/1024:.1f}GB)")
                                    
                                    if sys_mem.percent > 90:
                                        print(f"      âš ï¸ ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({sys_mem.percent:.1f}%)ï¼Œå¯èƒ½å¯¼è‡´ OOM")
                                except:
                                    pass
                            
                            # å¦‚æœ Worker å…¨éƒ¨æ­»äº¡ï¼ŒæŠ›å‡ºå¼‚å¸¸
                            if alive_workers == 0:
                                raise RuntimeError("æ‰€æœ‰ Worker è¿›ç¨‹å·²æ­»äº¡ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒã€‚")
                            
                            # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºä¸” Worker å­˜æ´»ï¼Œå¯èƒ½æ˜¯ Worker é™·å…¥æ­»é”æˆ–å†…å­˜ä¸è¶³
                            if total_queue_size == 0 and alive_workers > 0:
                                print(f"      âš ï¸ é˜Ÿåˆ—ä¸ºç©ºä½† Worker å­˜æ´»ï¼Œå¯èƒ½é™·å…¥æ­»é”æˆ–å†…å­˜ä¸è¶³")
                                print(f"      å»ºè®®: æ£€æŸ¥ç³»ç»Ÿæ—¥å¿— (dmesg | grep -i oom) æŸ¥çœ‹æ˜¯å¦æœ‰ OOM æ€æ­»è¿›ç¨‹")
                        break
                    
                    # å¦‚æœè¿ç»­å¤šæ¬¡æ— è¿›å±•ï¼Œæå‰è­¦å‘Š
                    if no_progress_count >= 20 and verbose:  # 10ç§’æ— è¿›å±•ï¼ˆ20æ¬¡ Ã— 0.5ç§’ï¼‰
                        print(f"\n  âš ï¸ è­¦å‘Š: è¿ç»­ {no_progress_count * 0.5:.1f}ç§’æ— æ ·æœ¬æ”¶é›†è¿›å±•")
                        no_progress_count = 0  # é‡ç½®è®¡æ•°å™¨ï¼Œé¿å…é‡å¤æ‰“å°
                    
                    # ç¨å¾®ç¡ä¸€ä¸‹ï¼Œé¿å… CPU ç©ºè½¬ï¼ŒåŒæ—¶ä¹Ÿç»™ Worker æäº¤æ•°æ®çš„æœºä¼š
                    time.sleep(0.5)
                
                # è®­ç»ƒä¼˜åŠ¿ç½‘ç»œ
                for player in range(self.num_players):
                    loss = self._learn_advantage_network(player)
                    if loss is not None:
                        advantage_losses[player].append(loss)
                
                # è®­ç»ƒç­–ç•¥ç½‘ç»œ
                # ä¸ºäº†åŠ é€Ÿ checkpoint ä¿å­˜æ—¶çš„ç­–ç•¥ç½‘ç»œæ›´æ–°ï¼Œæˆ‘ä»¬åœ¨æ¯æ¬¡è¿­ä»£ä¸­å¢é‡è®­ç»ƒç­–ç•¥ç½‘ç»œ
                # è¿™æ ·å¯ä»¥åˆ†æ‘Šè®¡ç®—æˆæœ¬ï¼Œä½¿å¾— checkpoint æ—¶ç­–ç•¥ç½‘ç»œå·²ç»æ¥è¿‘å°±ç»ª
                policy_loss = self._learn_strategy_network()
                
                # åŒæ­¥ç½‘ç»œå‚æ•°åˆ° Worker
                if (iteration + 1) % self.sync_interval == 0:
                    self._sync_network_params()
                
                self._iteration += 1
                
                iter_time = time.time() - iter_start
                
                if verbose:
                    print(f"\r  è¿­ä»£ {iteration + 1}/{self.num_iterations} "
                          f"(è€—æ—¶: {iter_time:.2f}ç§’) | "
                          f"ä¼˜åŠ¿æ ·æœ¬: {sum(len(m) for m in self._advantage_memories):,} | "
                          f"ç­–ç•¥æ ·æœ¬: {len(self._strategy_memories):,}", end="")
                
                if (iteration + 1) % eval_interval == 0:
                    print()
                    for player, losses in advantage_losses.items():
                        if losses:
                            print(f"    ç©å®¶ {player} æŸå¤±: {losses[-1]:.6f}")
                    
                    # è¿è¡Œè¯„ä¼°
                    if game is not None:
                        try:
                            from training_evaluator import quick_evaluate
                            print(f"  è¯„ä¼°è®­ç»ƒæ•ˆæœ...", end="", flush=True)
                            eval_result = quick_evaluate(
                                game,
                                self,
                                include_test_games=eval_with_games,
                                num_test_games=num_test_games,
                                max_depth=None,
                                verbose=True  # å¯ç”¨è¯¦ç»†è¾“å‡ºä»¥æŸ¥çœ‹é”™è¯¯
                            )
                            print(" å®Œæˆ")
                            
                            # æ‰“å°ç®€è¦è¯„ä¼°ä¿¡æ¯
                            metrics = eval_result['metrics']
                            print(f"    ç­–ç•¥ç†µ: {metrics.get('avg_entropy', 0):.4f} | "
                                  f"ç­–ç•¥ç¼“å†²åŒº: {len(self._strategy_memories):,} | "
                                  f"ä¼˜åŠ¿æ ·æœ¬: {sum(len(m) for m in self._advantage_memories):,}")
                            
                            if eval_with_games and eval_result.get('test_results'):
                                test_results = eval_result['test_results']
                                num_games = test_results.get('games_played', 0)
                                avg_return = test_results.get('player0_avg_return', 0)
                                win_rate = test_results.get('player0_win_rate', 0) * 100
                                if num_games > 0:
                                    print(f"    æµ‹è¯•å¯¹å±€: {num_games} å±€ | "
                                          f"ç©å®¶0å¹³å‡å›æŠ¥: {avg_return:.2f} | "
                                          f"èƒœç‡: {win_rate:.1f}%")
                        except ImportError:
                            pass  # training_evaluator ä¸å¯ç”¨
                        except Exception as e:
                            print(f" è¯„ä¼°å¤±è´¥: {e}")
                
                    # ä¿å­˜ checkpoint
                    if checkpoint_interval > 0 and (iteration + 1) % checkpoint_interval == 0:
                        if model_dir and save_prefix and game:
                            print(f"\n  ğŸ’¾ ä¿å­˜ checkpoint (è¿­ä»£ {iteration + 1})...", end="", flush=True)
                            try:
                                # è™½ç„¶å·²ç»å¢é‡è®­ç»ƒäº†ï¼Œä½†åœ¨ä¿å­˜å‰å†å¤šè®­ç»ƒå‡ æ¬¡ä»¥ç¡®ä¿æœ€æ–°
                                print("\n    æ­£åœ¨æœ€ç»ˆä¼˜åŒ–ç­–ç•¥ç½‘ç»œ (ç”¨äº Checkpoint)...", end="")
                                for _ in range(5):  # é¢å¤–è®­ç»ƒ 5 æ¬¡
                                    policy_loss = self._learn_strategy_network()
                                if policy_loss is not None:
                                    print(f" å®Œæˆ (Loss: {policy_loss:.6f})")
                                else:
                                    print(" å®Œæˆ (æ— è¶³å¤Ÿæ ·æœ¬è®­ç»ƒ)")
                                
                                save_checkpoint(self, game, model_dir, save_prefix, iteration + 1)
                                print("  âœ“ Checkpoint å·²ä¿å­˜")
                            except Exception as e:
                                print(f" å¤±è´¥: {e}")
            
            print()
            
            # è®­ç»ƒç­–ç•¥ç½‘ç»œ
            print("  è®­ç»ƒç­–ç•¥ç½‘ç»œ...")
            policy_loss = self._learn_strategy_network()
            
            total_time = time.time() - start_time
            print(f"\n  âœ“ è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f} ç§’")
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            if model_dir and save_prefix and game:
                print(f"  ğŸ’¾ ä¿å­˜ä¸­æ–­æ—¶çš„ checkpoint (è¿­ä»£ {self._iteration})...")
                try:
                    save_checkpoint(self, game, model_dir, save_prefix, self._iteration)
                    print(f"  âœ“ Checkpoint å·²ä¿å­˜")
                except Exception as e:
                    print(f"  âœ— ä¿å­˜å¤±è´¥: {e}")
        finally:
            # åœæ­¢ Worker
            self._stop_workers()
        
        return self._policy_network, advantage_losses, policy_loss
    
    def action_probabilities(self, state, player_id=None):
        """è®¡ç®—åŠ¨ä½œæ¦‚ç‡ï¼ˆç”¨äºæ¨ç†ï¼‰"""
        del player_id
        cur_player = state.current_player()
        legal_actions = state.legal_actions(cur_player)
        info_state_vector = np.array(state.information_state_tensor())
        if len(info_state_vector.shape) == 1:
            info_state_vector = np.expand_dims(info_state_vector, axis=0)
        with torch.no_grad():
            logits = self._policy_network(
                torch.FloatTensor(info_state_vector).to(self.device)
            )
            probs = self._policy_sm(logits).cpu().numpy()
        
        # ç¡®ä¿åªè¿”å›åˆæ³•åŠ¨ä½œçš„æ¦‚ç‡ï¼Œå¹¶é‡æ–°å½’ä¸€åŒ–
        action_probs = {action: float(probs[0][action]) for action in legal_actions}
        total_prob = sum(action_probs.values())
        
        if total_prob > 1e-10:
            # é‡æ–°å½’ä¸€åŒ–
            action_probs = {a: p / total_prob for a, p in action_probs.items()}
        else:
            # å¦‚æœæ‰€æœ‰æ¦‚ç‡éƒ½æ¥è¿‘0ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
            action_probs = {a: 1.0 / len(legal_actions) for a in legal_actions}
            
        return action_probs


def load_checkpoint(solver, model_dir, save_prefix, game):
    """ä» checkpoint åŠ è½½ç½‘ç»œæƒé‡
    
    Args:
        solver: ParallelDeepCFRSolver å®ä¾‹
        model_dir: æ¨¡å‹ç›®å½•
        save_prefix: ä¿å­˜å‰ç¼€
        game: æ¸¸æˆå®ä¾‹
        
    Returns:
        start_iteration: æ¢å¤çš„è¿­ä»£æ¬¡æ•°
    """
    import glob
    import re
    
    # æŸ¥æ‰¾æœ€æ–°çš„ checkpoint
    checkpoint_root = os.path.join(model_dir, "checkpoints")
    
    latest_file = None
    max_iter = 0
    
    # ä¼˜å…ˆä» checkpoints ç›®å½•åŠ è½½
    if os.path.exists(checkpoint_root):
        # å°è¯•æ–°çš„ç›®å½•ç»“æ„: checkpoints/iter_X/prefix_policy_iterX.pt
        iter_dirs = glob.glob(os.path.join(checkpoint_root, "iter_*"))
        for d in iter_dirs:
            match = re.search(r'iter_(\d+)$', d)
            if match:
                iter_num = int(match.group(1))
                policy_file = os.path.join(d, f"{save_prefix}_policy_network_iter{iter_num}.pt")
                if os.path.exists(policy_file) and iter_num > max_iter:
                    max_iter = iter_num
                    latest_file = policy_file
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼ˆæˆ–è€…æ˜¯æ—§ç»“æ„ï¼‰ï¼Œå°è¯•æ—§çš„æ‰å¹³ç»“æ„: checkpoints/prefix_policy_iterX.pt
        if latest_file is None:
            policy_files = glob.glob(os.path.join(checkpoint_root, f"{save_prefix}_policy_network_iter*.pt"))
            for f in policy_files:
                match = re.search(r'_iter(\d+)\.pt$', f)
                if match:
                    iter_num = int(match.group(1))
                    if iter_num > max_iter:
                        max_iter = iter_num
                        latest_file = f
            
        if latest_file:
            print(f"  æ‰¾åˆ° checkpoint: è¿­ä»£ {max_iter}")
            policy_path = latest_file
            start_iteration = max_iter
        else:
            policy_path = None
            start_iteration = 0
    else:
        # ... (åç»­é€»è¾‘ä¸å˜)
        policy_path = None
        start_iteration = 0

    if policy_path is None: # å¦‚æœ checkpoint æ²¡æ‰¾åˆ°ï¼Œå°è¯•åŠ è½½æœ€ç»ˆæ¨¡å‹
        policy_path = os.path.join(model_dir, f"{save_prefix}_policy_network.pt")
        if os.path.exists(policy_path):
            print(f"  æ‰¾åˆ°æœ€ç»ˆæ¨¡å‹")
            start_iteration = 0  # æœ€ç»ˆæ¨¡å‹æ²¡æœ‰è¿­ä»£ä¿¡æ¯
        else:
            policy_path = None
            start_iteration = 0
    
    if policy_path is None or not os.path.exists(policy_path):
        print(f"  âœ— æœªæ‰¾åˆ°å¯åŠ è½½çš„æ¨¡å‹")
        return 0
    
    # åŠ è½½ç­–ç•¥ç½‘ç»œ
    print(f"  åŠ è½½ç­–ç•¥ç½‘ç»œ: {policy_path}")
    policy_state = torch.load(policy_path, map_location=solver.device)
    policy_net = solver._policy_network
    if isinstance(policy_net, nn.DataParallel):
        policy_net.module.load_state_dict(policy_state)
    else:
        policy_net.load_state_dict(policy_state)
    print(f"  âœ“ ç­–ç•¥ç½‘ç»œå·²åŠ è½½")
    
    # åŠ è½½ä¼˜åŠ¿ç½‘ç»œ
    # ç¡®å®šä¼˜åŠ¿ç½‘ç»œæ‰€åœ¨çš„ç›®å½•
    if start_iteration > 0:
        # æ£€æŸ¥æ˜¯æ–°ç»“æ„è¿˜æ˜¯æ—§ç»“æ„
        if "iter_" in os.path.dirname(policy_path):
            # æ–°ç»“æ„: checkpoints/iter_X/
            adv_dir = os.path.dirname(policy_path)
        else:
            # æ—§ç»“æ„: checkpoints/
            adv_dir = checkpoint_root
    else:
        adv_dir = model_dir

    for player in range(game.num_players()):
        if start_iteration > 0:
            adv_path = os.path.join(adv_dir, f"{save_prefix}_advantage_player_{player}_iter{start_iteration}.pt")
        else:
            adv_path = os.path.join(adv_dir, f"{save_prefix}_advantage_player_{player}.pt")
        
        if os.path.exists(adv_path):
            # ... (åŠ è½½é€»è¾‘ä¸å˜)
            adv_state = torch.load(adv_path, map_location=solver.device)
            adv_net = solver._advantage_networks[player]
            if isinstance(adv_net, nn.DataParallel):
                adv_net.module.load_state_dict(adv_state)
            else:
                adv_net.load_state_dict(adv_state)
            print(f"  âœ“ ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œå·²åŠ è½½")
        else:
            print(f"  âš ï¸ ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œæœªæ‰¾åˆ°: {adv_path}")
    
    return start_iteration


def create_save_directory(save_prefix, save_dir="models"):
    """åˆ›å»ºä¿å­˜ç›®å½•"""
    import time as time_module
    base_dir = save_dir
    if not os.path.exists(base_dir):
        os.makedirs(base_dir)
    
    model_dir = os.path.join(base_dir, save_prefix)
    if os.path.exists(model_dir):
        timestamp = time_module.strftime("%Y%m%d_%H%M%S")
        model_dir = f"{model_dir}_{timestamp}"
    
    os.makedirs(model_dir, exist_ok=True)
    return model_dir


def save_checkpoint(solver, game, model_dir, save_prefix, iteration, is_final=False):
    """ä¿å­˜ checkpoint"""
    if is_final:
        suffix = ""
        checkpoint_dir = model_dir
    else:
        suffix = f"_iter{iteration}"
        # å°†æ¯ä¸ª iteration çš„ checkpoint æ”¾å…¥ç‹¬ç«‹å­ç›®å½•
        checkpoint_dir = os.path.join(model_dir, "checkpoints", f"iter_{iteration}")
        os.makedirs(checkpoint_dir, exist_ok=True)
    
    # ä¿å­˜ç­–ç•¥ç½‘ç»œï¼ˆå¤„ç† DataParallelï¼‰
    policy_path = os.path.join(checkpoint_dir, f"{save_prefix}_policy_network{suffix}.pt")
    policy_net = solver._policy_network
    if isinstance(policy_net, nn.DataParallel):
        torch.save(policy_net.module.state_dict(), policy_path)
    else:
        torch.save(policy_net.state_dict(), policy_path)
    
    # ä¿å­˜ä¼˜åŠ¿ç½‘ç»œï¼ˆå¤„ç† DataParallelï¼‰
    for player in range(game.num_players()):
        advantage_path = os.path.join(checkpoint_dir, f"{save_prefix}_advantage_player_{player}{suffix}.pt")
        adv_net = solver._advantage_networks[player]
        if isinstance(adv_net, nn.DataParallel):
            torch.save(adv_net.module.state_dict(), advantage_path)
        else:
            torch.save(adv_net.state_dict(), advantage_path)
    
    return checkpoint_dir


def main():
    # æ³¨å†Œä¿¡å·å¤„ç†ï¼Œç¡®ä¿è¢« kill æ—¶ä¹Ÿèƒ½æ¸…ç†å­è¿›ç¨‹
    def signal_handler(signum, frame):
        print(f"\næ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨æ¸…ç†å¹¶é€€å‡º...")
        # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ç›´æ¥è°ƒç”¨ solver._stop_workers() å› ä¸º solver ä¸åœ¨ä½œç”¨åŸŸå†…
        # ä½†ç”±äº worker è¿›ç¨‹å·²è®¾ç½®ä¸º daemon=Trueï¼Œä¸»è¿›ç¨‹é€€å‡ºæ—¶å®ƒä»¬ä¼šè‡ªåŠ¨è¢«ç³»ç»Ÿæ¸…ç†
        sys.exit(0)
        
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)

    parser = argparse.ArgumentParser(description="å¤šè¿›ç¨‹å¹¶è¡Œ DeepCFR è®­ç»ƒ")
    parser.add_argument("--num_players", type=int, default=2, help="ç©å®¶æ•°é‡")
    parser.add_argument("--num_workers", type=int, default=4, help="Worker è¿›ç¨‹æ•°é‡")
    parser.add_argument("--num_iterations", type=int, default=100, help="è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--num_traversals", type=int, default=40, help="æ¯æ¬¡è¿­ä»£éå†æ¬¡æ•°")
    parser.add_argument("--policy_layers", type=int, nargs="+", default=[128, 128])
    parser.add_argument("--advantage_layers", type=int, nargs="+", default=[128, 128])
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--batch_size", type=int, default=2048)
    parser.add_argument("--memory_capacity", type=int, default=1000000,
                        help="ç»éªŒå›æ”¾ç¼“å†²åŒºå®¹é‡ï¼ˆé»˜è®¤: 1000000ï¼‰")
    parser.add_argument("--max_memory_gb", type=float, default=None,
                        help="æœ€å¤§å†…å­˜é™åˆ¶ï¼ˆGBï¼‰ï¼Œè¶…è¿‡æ­¤é™åˆ¶ä¼šè‡ªåŠ¨æ¸…ç†æ—§æ ·æœ¬ï¼ˆé»˜è®¤: ä¸é™åˆ¶ï¼‰")
    parser.add_argument("--queue_maxsize", type=int, default=50000,
                        help="é˜Ÿåˆ—æœ€å¤§å¤§å°ï¼Œé™ä½å¯å‡å°‘å†…å­˜å ç”¨ï¼ˆé»˜è®¤: 50000ï¼‰")
    parser.add_argument("--betting_abstraction", type=str, default="fcpa")
    parser.add_argument("--save_prefix", type=str, default="deepcfr_parallel")
    parser.add_argument("--save_dir", type=str, default="models")
    parser.add_argument("--eval_interval", type=int, default=10)
    parser.add_argument("--checkpoint_interval", type=int, default=0, 
                        help="Checkpoint ä¿å­˜é—´éš”ï¼ˆ0=ä¸ä¿å­˜ä¸­é—´checkpointï¼‰")
    parser.add_argument("--skip_nashconv", action="store_true", 
                        help="è·³è¿‡ NashConv è®¡ç®—ï¼ˆ6äººå±€å¼ºçƒˆå»ºè®®ï¼‰")
    parser.add_argument("--use_gpu", action="store_true", default=True,
                        help="ä½¿ç”¨ GPU è®­ç»ƒ")
    parser.add_argument("--gpu_ids", type=int, nargs="+", default=None,
                        help="ä½¿ç”¨çš„ GPU ID åˆ—è¡¨ï¼ˆä¾‹å¦‚ --gpu_ids 0 1 2 3ï¼‰")
    parser.add_argument("--resume", type=str, default=None,
                        help="ä»æŒ‡å®šç›®å½•æ¢å¤è®­ç»ƒï¼ˆä¾‹å¦‚ --resume models/deepcfr_parallel_6pï¼‰")
    parser.add_argument("--eval_with_games", action="store_true",
                        help="è¯„ä¼°æ—¶è¿è¡Œæµ‹è¯•å¯¹å±€")
    parser.add_argument("--num_test_games", type=int, default=50,
                        help="è¯„ä¼°æ—¶çš„æµ‹è¯•å¯¹å±€æ•°é‡ï¼ˆé»˜è®¤: 50ï¼‰")
    
    args = parser.parse_args()
    
    # å¤„ç†æ¢å¤è®­ç»ƒé…ç½®ï¼ˆå¿…é¡»åœ¨åˆ›å»ºæ¸¸æˆä¹‹å‰ï¼‰
    if args.resume:
        model_dir = args.resume
        if not os.path.exists(model_dir):
            print(f"âœ— æ¢å¤ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            import sys
            sys.exit(1)
        
        # å°è¯•ä» config.json è¯»å–é…ç½®
        config_path = os.path.join(model_dir, "config.json")
        if os.path.exists(config_path):
            import json
            with open(config_path, 'r') as f:
                resume_config = json.load(f)
            
            print(f"ä» {model_dir} æ¢å¤è®­ç»ƒ")
            
            # è‡ªåŠ¨è¦†ç›–å…³é”®å‚æ•°ï¼Œç¡®ä¿ç½‘ç»œç»“æ„ä¸€è‡´
            # æ³¨æ„ï¼šå‘½ä»¤è¡Œæ˜¾å¼æŒ‡å®šçš„å‚æ•°ä¼˜å…ˆçº§åº”è¯¥æ›´é«˜ï¼Œä½†ä¸ºäº†ç®€åŒ–ç»­è®­ï¼Œè¿™é‡Œé»˜è®¤ä½¿ç”¨ config ä¸­çš„å€¼
            # é™¤éç”¨æˆ·æƒ³è¦æ”¹å˜æŸäº›è®­ç»ƒè¶…å‚æ•°ï¼ˆå¦‚ batch_size, learning_rateï¼‰
            
            if 'num_players' in resume_config:
                args.num_players = resume_config['num_players']
            if 'policy_layers' in resume_config:
                args.policy_layers = resume_config['policy_layers']
            if 'advantage_layers' in resume_config:
                args.advantage_layers = resume_config['advantage_layers']
            if 'betting_abstraction' in resume_config:
                args.betting_abstraction = resume_config['betting_abstraction']
            if 'save_prefix' in resume_config:
                args.save_prefix = resume_config['save_prefix']
            if 'num_traversals' in resume_config:
                args.num_traversals = resume_config['num_traversals']
                
            print(f"  è‡ªåŠ¨åŠ è½½é…ç½®: {args.num_players}äººå±€, ç­–ç•¥å±‚{args.policy_layers}, ä¼˜åŠ¿å±‚{args.advantage_layers}")
            print(f"  save_prefix: {args.save_prefix}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° config.jsonï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°")

    # åˆ›å»ºæ¸¸æˆ
    num_players = args.num_players
    if num_players == 2:
        blinds_str = "100 50"
        first_player_str = "2 1 1 1"
    else:
        blinds_list = ["50", "100"] + ["0"] * (num_players - 2)
        blinds_str = " ".join(blinds_list)
        first_player_str = " ".join(["3"] + ["1"] * 3)
    
    stacks_str = " ".join(["2000"] * num_players)
    
    game_string = (
        f"universal_poker("
        f"betting=nolimit,"
        f"numPlayers={num_players},"
        f"numRounds=4,"
        f"blind={blinds_str},"
        f"stack={stacks_str},"
        f"numHoleCards=2,"
        f"numBoardCards=0 3 1 1,"
        f"firstPlayer={first_player_str},"
        f"numSuits=4,"
        f"numRanks=13,"
        f"bettingAbstraction={args.betting_abstraction}"
        f")"
    )
    
    # è®¾ç½®è®¾å¤‡
    gpu_ids = None
    if args.use_gpu and torch.cuda.is_available():
        if args.gpu_ids is not None and len(args.gpu_ids) > 0:
            gpu_ids = args.gpu_ids
            device = f"cuda:{gpu_ids[0]}"
            if len(gpu_ids) > 1:
                print(f"ä½¿ç”¨å¤š GPU: {gpu_ids}")
                for gid in gpu_ids:
                    print(f"  GPU {gid}: {torch.cuda.get_device_name(gid)}")
            else:
                print(f"ä½¿ç”¨ GPU: {torch.cuda.get_device_name(gpu_ids[0])}")
        else:
            device = "cuda:0"
            print(f"ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        print("ä½¿ç”¨ CPU")
    
    print(f"åˆ›å»ºæ¸¸æˆ: {game_string}")
    game = pyspiel.load_game(game_string)
    
    # å¤„ç†æ¢å¤è®­ç»ƒ
    start_iteration = 0
    if args.resume:
        # ç›®å½•æ£€æŸ¥å·²åœ¨å‰é¢å®Œæˆ
        model_dir = args.resume
    else:
        # åˆ›å»ºæ–°çš„ä¿å­˜ç›®å½•
        model_dir = create_save_directory(args.save_prefix, args.save_dir)
    
    print(f"æ¨¡å‹ä¿å­˜ç›®å½•: {model_dir}")
    if args.checkpoint_interval > 0:
        print(f"Checkpoint ä¿å­˜é—´éš”: æ¯ {args.checkpoint_interval} æ¬¡è¿­ä»£")
    
    # åˆ›å»ºæ±‚è§£å™¨
    solver = ParallelDeepCFRSolver(
        game,
        num_workers=args.num_workers,
        policy_network_layers=tuple(args.policy_layers),
        advantage_network_layers=tuple(args.advantage_layers),
        num_iterations=args.num_iterations,
        num_traversals=args.num_traversals,
        learning_rate=args.learning_rate,
        batch_size_advantage=args.batch_size,
        batch_size_strategy=args.batch_size,
        memory_capacity=args.memory_capacity,
        device=device,
        gpu_ids=gpu_ids,
        max_memory_gb=args.max_memory_gb,
        queue_maxsize=args.queue_maxsize,
    )
    
    # æ˜¾ç¤ºå†…å­˜é…ç½®
    if args.max_memory_gb:
        print(f"  å†…å­˜é™åˆ¶: {args.max_memory_gb}GB")
    print(f"  é˜Ÿåˆ—å¤§å°: {args.queue_maxsize}")
    
    # ç«‹å³ä¿å­˜é…ç½®ï¼ˆæ–¹ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æŸ¥çœ‹æˆ–æ¢å¤ï¼‰
    if not args.resume:
        import json
        config_path = os.path.join(model_dir, "config.json")
        config = {
            'num_players': num_players,
            'num_workers': args.num_workers,
            'num_iterations': args.num_iterations,
            'num_traversals': args.num_traversals,
            'policy_layers': args.policy_layers,
            'advantage_layers': args.advantage_layers,
            'learning_rate': args.learning_rate,
            'batch_size': args.batch_size,
            'memory_capacity': args.memory_capacity,
            'max_memory_gb': args.max_memory_gb,
            'queue_maxsize': args.queue_maxsize,
            'betting_abstraction': args.betting_abstraction,
            'device': device,
            'gpu_ids': gpu_ids,
            'game_string': game_string,
            'multi_gpu': gpu_ids is not None and len(gpu_ids) > 1,
            'parallel': True,
            'use_feature_transform': True,
            'use_simple_feature': True,
            'save_prefix': args.save_prefix,
        }
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        print(f"  âœ“ é…ç½®å·²ä¿å­˜: {config_path}")
    
    # å¦‚æœæ˜¯æ¢å¤è®­ç»ƒï¼ŒåŠ è½½ checkpoint
    if args.resume:
        print(f"\nåŠ è½½ checkpoint...")
        start_iteration = load_checkpoint(solver, model_dir, args.save_prefix, game)
        if start_iteration > 0:
            print(f"  âœ“ å°†ä»è¿­ä»£ {start_iteration + 1} ç»§ç»­è®­ç»ƒ")
        else:
            print(f"  âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆ checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # è®­ç»ƒï¼ˆå¸¦ checkpoint æ”¯æŒï¼‰
    policy_network, advantage_losses, policy_loss = solver.solve(
        verbose=True,
        eval_interval=args.eval_interval,
        checkpoint_interval=args.checkpoint_interval,
        model_dir=model_dir,
        save_prefix=args.save_prefix,
        game=game,
        start_iteration=start_iteration,
        eval_with_games=args.eval_with_games,
        num_test_games=args.num_test_games,
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    save_checkpoint(solver, game, model_dir, args.save_prefix, args.num_iterations, is_final=True)
    print(f"  âœ“ ç­–ç•¥ç½‘ç»œå·²ä¿å­˜: {os.path.join(model_dir, f'{args.save_prefix}_policy_network.pt')}")
    for player in range(num_players):
        print(f"  âœ“ ç©å®¶ {player} ä¼˜åŠ¿ç½‘ç»œå·²ä¿å­˜")
    
    # ä¿å­˜é…ç½®
    import json
    config_path = os.path.join(model_dir, "config.json")
    config = {
        'num_players': num_players,
        'num_workers': args.num_workers,
        'num_iterations': args.num_iterations,
        'num_traversals': args.num_traversals,
        'policy_layers': args.policy_layers,
        'advantage_layers': args.advantage_layers,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size,
        'memory_capacity': args.memory_capacity,
        'max_memory_gb': args.max_memory_gb,
        'queue_maxsize': args.queue_maxsize,
        'betting_abstraction': args.betting_abstraction,
        'device': device,
        'gpu_ids': gpu_ids,
        'game_string': game_string,
        'multi_gpu': gpu_ids is not None and len(gpu_ids) > 1,
        'parallel': True,
        'use_feature_transform': True,
        'use_simple_feature': True,
        'save_prefix': args.save_prefix,
    }
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"  âœ“ é…ç½®å·²ä¿å­˜: {config_path}")
    
    # NashConv è®¡ç®—ï¼ˆå¯é€‰ï¼‰
    if not args.skip_nashconv:
        print(f"\nè®¡ç®— NashConv...")
        if num_players > 2:
            print(f"  âš ï¸ è­¦å‘Š: {num_players} äººæ¸¸æˆçš„ NashConv è®¡ç®—å¯èƒ½éå¸¸æ…¢æˆ–ä¸å¯è¡Œ")
            print(f"  å»ºè®®: ä½¿ç”¨ --skip_nashconv è·³è¿‡")
        try:
            from open_spiel.python import policy
            
            average_policy = policy.tabular_policy_from_callable(
                game, solver.action_probabilities
            )
            pyspiel_policy = policy.python_policy_to_pyspiel_policy(average_policy)
            conv = pyspiel.nash_conv(game, pyspiel_policy, use_cpp_br=True)
            print(f"  âœ“ NashConv: {conv:.6f}")
        except Exception as e:
            print(f"  âœ— NashConv è®¡ç®—å¤±è´¥: {e}")
            print(f"  å»ºè®®: ä½¿ç”¨ --skip_nashconv è·³è¿‡")
    else:
        print(f"\n  â­ï¸ è·³è¿‡ NashConv è®¡ç®—")
    
    print("\n" + "=" * 70)
    print("âœ“ å¹¶è¡Œ DeepCFR è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)


if __name__ == "__main__":
    main()

