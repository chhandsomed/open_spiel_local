# DeepCFR ç®—æ³•åŸç†ä¸å¾·å·æ‰‘å…‹è®­ç»ƒè¯¦è§£

## ğŸ“š ç›®å½•

1. [DeepCFR ç®—æ³•æ¦‚è¿°](#deepcfr-ç®—æ³•æ¦‚è¿°)
2. [æ ¸å¿ƒåŸç†](#æ ¸å¿ƒåŸç†)
3. [ç½‘ç»œæ¶æ„](#ç½‘ç»œæ¶æ„)
4. [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
5. [ä»£ç å®ç°è¯¦è§£](#ä»£ç å®ç°è¯¦è§£)
6. [åœ¨å¾·å·æ‰‘å…‹ä¸­çš„åº”ç”¨](#åœ¨å¾·å·æ‰‘å…‹ä¸­çš„åº”ç”¨)

---

## DeepCFR ç®—æ³•æ¦‚è¿°

**DeepCFR (Deep Counterfactual Regret Minimization)** æ˜¯ CFR (Counterfactual Regret Minimization) ç®—æ³•çš„æ·±åº¦å­¦ä¹ ç‰ˆæœ¬ï¼Œç”¨äºæ±‚è§£å¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯åšå¼ˆçš„çº³ä»€å‡è¡¡ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦ DeepCFRï¼Ÿ

ä¼ ç»Ÿ CFR ç®—æ³•éœ€è¦ï¼š
- âŒ å­˜å‚¨æ‰€æœ‰ä¿¡æ¯é›†ï¼ˆinformation setsï¼‰çš„ç­–ç•¥
- âŒ å¯¹äºå¾·å·æ‰‘å…‹è¿™æ ·çš„æ¸¸æˆï¼Œä¿¡æ¯é›†æ•°é‡å·¨å¤§ï¼ˆ10^18+ï¼‰
- âŒ å†…å­˜å’Œè®¡ç®—èµ„æºéœ€æ±‚å·¨å¤§

DeepCFR é€šè¿‡ç¥ç»ç½‘ç»œï¼š
- âœ… ç”¨ç¥ç»ç½‘ç»œè¿‘ä¼¼ç­–ç•¥å’Œä¼˜åŠ¿å€¼
- âœ… åªéœ€è¦å­˜å‚¨è®­ç»ƒæ ·æœ¬ï¼Œä¸éœ€è¦å­˜å‚¨æ‰€æœ‰ä¿¡æ¯é›†
- âœ… å¯ä»¥å¤„ç†å¤§è§„æ¨¡æ¸¸æˆ

### æ ¸å¿ƒæ€æƒ³

DeepCFR å°† CFR çš„ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ç”¨ç¥ç»ç½‘ç»œæ›¿ä»£ï¼š
1. **ä¼˜åŠ¿ç½‘ç»œ (Advantage Networks)**ï¼šè¿‘ä¼¼æ¯ä¸ªç©å®¶çš„åæ‚”å€¼ï¼ˆregretï¼‰
2. **ç­–ç•¥ç½‘ç»œ (Policy Network)**ï¼šè¿‘ä¼¼å¹³å‡ç­–ç•¥ï¼ˆaverage strategyï¼‰

---

## æ ¸å¿ƒåŸç†

### 1. CFR åŸºç¡€å›é¡¾

CFR ç®—æ³•çš„æ ¸å¿ƒæ˜¯**åæ‚”å€¼åŒ¹é… (Regret Matching)**ï¼š

```
åæ‚”å€¼ R^T(a) = Î£_t (u(a) - u(Ïƒ^t))
ç­–ç•¥ Ïƒ^{T+1}(a) = R^T_+(a) / Î£_b R^T_+(b)
```

å…¶ä¸­ï¼š
- `R^T_+(a) = max(0, R^T(a))` æ˜¯æ­£åæ‚”å€¼
- `u(a)` æ˜¯é€‰æ‹©åŠ¨ä½œ a çš„æœŸæœ›æ”¶ç›Š
- `u(Ïƒ^t)` æ˜¯å½“å‰ç­–ç•¥çš„æœŸæœ›æ”¶ç›Š

### 2. DeepCFR çš„æ”¹è¿›

DeepCFR ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ï¼š

1. **ä¼˜åŠ¿ç½‘ç»œ**ï¼š`A^Î¸(s, a) â‰ˆ R^T(s, a)` ï¼ˆåæ‚”å€¼ï¼‰
2. **ç­–ç•¥ç½‘ç»œ**ï¼š`Ï€^Ï†(s, a) â‰ˆ Ïƒ^T(s, a)` ï¼ˆå¹³å‡ç­–ç•¥ï¼‰

### 3. è®­ç»ƒè¿‡ç¨‹

```
å¯¹äºæ¯æ¬¡è¿­ä»£ t:
    å¯¹äºæ¯ä¸ªç©å®¶ p:
        è¿›è¡Œå¤šæ¬¡æ¸¸æˆæ ‘éå†ï¼Œæ”¶é›†æ ·æœ¬
        ç”¨ä¼˜åŠ¿ç½‘ç»œè®¡ç®—åæ‚”å€¼
        æ›´æ–°ä¼˜åŠ¿ç½‘ç»œ
    æ›´æ–°ç­–ç•¥ç½‘ç»œï¼ˆä½¿ç”¨å¹³å‡ç­–ç•¥ï¼‰
```

---

## ç½‘ç»œæ¶æ„

### 1. ä¼˜åŠ¿ç½‘ç»œ (Advantage Networks)

**ä½œç”¨**ï¼šä¸ºæ¯ä¸ªç©å®¶å­¦ä¹ åæ‚”å€¼ï¼Œç”¨äºç­–ç•¥é€‰æ‹©

```python
# æ¯ä¸ªç©å®¶ä¸€ä¸ªç‹¬ç«‹çš„ä¼˜åŠ¿ç½‘ç»œ
self._advantage_networks = [
    MLP(embedding_size, [128, 128], num_actions) 
    for _ in range(num_players)
]
```

**è¾“å…¥**ï¼šä¿¡æ¯çŠ¶æ€å‘é‡ (information state tensor)
- å¾·å·æ‰‘å…‹ä¸­ï¼šåŒ…å«æ‰‹ç‰Œã€å…¬å…±ç‰Œã€ä¸‹æ³¨å†å²ç­‰ä¿¡æ¯
- å¤§å°ï¼š266 ç»´ï¼ˆ6äººåœºï¼‰

**è¾“å‡º**ï¼šæ¯ä¸ªåŠ¨ä½œçš„ä¼˜åŠ¿å€¼ï¼ˆåæ‚”å€¼ï¼‰
- 4ä¸ªåŠ¨ä½œï¼šFold, Call/Check, Bet/Raise, All-in

### 2. ç­–ç•¥ç½‘ç»œ (Policy Network)

**ä½œç”¨**ï¼šå­¦ä¹ å¹³å‡ç­–ç•¥ï¼Œç”¨äºæœ€ç»ˆå†³ç­–

```python
# æ‰€æœ‰ç©å®¶å…±äº«ä¸€ä¸ªç­–ç•¥ç½‘ç»œ
self._policy_network = MLP(
    embedding_size, 
    [256, 256, 128], 
    num_actions
)
```

**è¾“å…¥**ï¼šä¿¡æ¯çŠ¶æ€å‘é‡
**è¾“å‡º**ï¼šæ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ

### 3. ç½‘ç»œç»“æ„ (MLP)

```python
class MLP(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        # è¾“å…¥å±‚ -> éšè—å±‚1 -> éšè—å±‚2 -> ... -> è¾“å‡ºå±‚
        self._layers = [
            SonnetLinear(input_size, hidden_sizes[0]),
            SonnetLinear(hidden_sizes[0], hidden_sizes[1]),
            ...
            SonnetLinear(hidden_sizes[-1], output_size)
        ]
```

**æ¿€æ´»å‡½æ•°**ï¼šReLUï¼ˆé™¤äº†è¾“å‡ºå±‚ï¼‰

---

## è®­ç»ƒæµç¨‹

### æ•´ä½“è®­ç»ƒå¾ªç¯

```python
for iteration in range(num_iterations):  # ä¾‹å¦‚ï¼š500æ¬¡è¿­ä»£
    for player in range(num_players):     # å¯¹æ¯ä¸ªç©å®¶
        # 1. æ¸¸æˆæ ‘éå†ï¼ˆæ”¶é›†æ ·æœ¬ï¼‰
        for _ in range(num_traversals):   # ä¾‹å¦‚ï¼š50æ¬¡éå†
            traverse_game_tree(root, player)
        
        # 2. æ›´æ–°ä¼˜åŠ¿ç½‘ç»œ
        learn_advantage_network(player)
    
    # 3. æ›´æ–°ç­–ç•¥ç½‘ç»œ
    learn_strategy_network()
```

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤ 1: æ¸¸æˆæ ‘éå† (`_traverse_game_tree`)

è¿™æ˜¯ DeepCFR çš„æ ¸å¿ƒï¼Œç”¨äºæ”¶é›†è®­ç»ƒæ ·æœ¬ï¼š

```python
def _traverse_game_tree(self, state, player):
    if state.is_terminal():
        return state.returns()[player]  # æ¸¸æˆç»“æŸï¼Œè¿”å›æ”¶ç›Š
    
    elif state.is_chance_node():
        # æœºä¼šèŠ‚ç‚¹ï¼ˆå‘ç‰Œï¼‰ï¼šéšæœºé€‰æ‹©
        action = sample_from_chance_outcomes()
        return traverse_game_tree(state.child(action), player)
    
    elif state.current_player() == player:
        # å½“å‰ç©å®¶ï¼šè®¡ç®—åæ‚”å€¼
        _, strategy = sample_action_from_advantage(state, player)
        
        # é€’å½’è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„æœŸæœ›æ”¶ç›Š
        for action in legal_actions:
            expected_payoff[action] = traverse_game_tree(
                state.child(action), player
            )
        
        # è®¡ç®—å½“å‰ç­–ç•¥çš„æœŸæœ›æ”¶ç›Šï¼ˆCFVï¼‰
        cfv = Î£ strategy[a] * expected_payoff[a]
        
        # è®¡ç®—åæ‚”å€¼ï¼ˆä¼˜åŠ¿å€¼ï¼‰
        for action in legal_actions:
            regret[action] = expected_payoff[action] - cfv
        
        # å­˜å‚¨åˆ°ä¼˜åŠ¿ç¼“å†²åŒº
        advantage_memories[player].add(
            info_state, iteration, regret, action
        )
        
        return cfv
    
    else:
        # å…¶ä»–ç©å®¶ï¼šä½¿ç”¨ç­–ç•¥é‡‡æ ·åŠ¨ä½œ
        _, strategy = sample_action_from_advantage(state, other_player)
        action = sample_from_strategy(strategy)
        
        # å­˜å‚¨åˆ°ç­–ç•¥ç¼“å†²åŒº
        strategy_memories.add(info_state, iteration, strategy)
        
        return traverse_game_tree(state.child(action), player)
```

**å…³é”®ç‚¹**ï¼š
- å¯¹å½“å‰ç©å®¶ï¼šè®¡ç®—æ‰€æœ‰åŠ¨ä½œçš„æœŸæœ›æ”¶ç›Šï¼Œå¾—åˆ°åæ‚”å€¼
- å¯¹å…¶ä»–ç©å®¶ï¼šä½¿ç”¨ç­–ç•¥é‡‡æ ·åŠ¨ä½œï¼Œè®°å½•ç­–ç•¥åˆ†å¸ƒ

#### æ­¥éª¤ 2: åæ‚”å€¼åŒ¹é… (`_sample_action_from_advantage`)

å°†ä¼˜åŠ¿å€¼è½¬æ¢ä¸ºç­–ç•¥ï¼š

```python
def _sample_action_from_advantage(self, state, player):
    # 1. ç”¨ä¼˜åŠ¿ç½‘ç»œè·å–ä¼˜åŠ¿å€¼
    info_state = state.information_state_tensor(player)
    raw_advantages = advantage_networks[player](info_state)
    
    # 2. åªä¿ç•™æ­£ä¼˜åŠ¿å€¼ï¼ˆæ­£åæ‚”å€¼ï¼‰
    advantages = [max(0, a) for a in raw_advantages]
    
    # 3. å½’ä¸€åŒ–å¾—åˆ°ç­–ç•¥ï¼ˆåæ‚”å€¼åŒ¹é…ï¼‰
    cumulative_regret = sum(advantages[action] for action in legal_actions)
    if cumulative_regret > 0:
        strategy[action] = advantages[action] / cumulative_regret
    else:
        # å¦‚æœæ‰€æœ‰åæ‚”å€¼éƒ½æ˜¯è´Ÿçš„ï¼Œå‡åŒ€åˆ†å¸ƒ
        strategy[action] = 1.0 / len(legal_actions)
    
    return advantages, strategy
```

#### æ­¥éª¤ 3: æ›´æ–°ä¼˜åŠ¿ç½‘ç»œ (`_learn_advantage_network`)

```python
def _learn_advantage_network(self, player):
    # 1. ä»ç¼“å†²åŒºé‡‡æ ·
    samples = advantage_memories[player].sample(batch_size)
    
    # 2. å‡†å¤‡æ•°æ®
    info_states = [s.info_state for s in samples]
    advantages = [s.advantage for s in samples]
    iterations = [sqrt(s.iteration) for s in samples]  # åŠ æƒ
    
    # 3. å‰å‘ä¼ æ’­
    outputs = advantage_networks[player](info_states)
    
    # 4. è®¡ç®—æŸå¤±ï¼ˆåŠ æƒ MSEï¼‰
    loss = MSE(iterations * outputs, iterations * advantages)
    
    # 5. åå‘ä¼ æ’­
    loss.backward()
    optimizer.step()
    
    return loss
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `sqrt(iteration)` åŠ æƒï¼Œæ—©æœŸè¿­ä»£æƒé‡è¾ƒå°
- è¿™æ˜¯ DeepCFR è®ºæ–‡ä¸­çš„æŠ€å·§ï¼Œå¸®åŠ©ç½‘ç»œå­¦ä¹ 

#### æ­¥éª¤ 4: æ›´æ–°ç­–ç•¥ç½‘ç»œ (`_learn_strategy_network`)

```python
def _learn_strategy_network(self):
    # 1. ä»ç­–ç•¥ç¼“å†²åŒºé‡‡æ ·
    samples = strategy_memories.sample(batch_size)
    
    # 2. å‡†å¤‡æ•°æ®
    info_states = [s.info_state for s in samples]
    action_probs = [s.strategy_action_probs for s in samples]
    iterations = [sqrt(s.iteration) for s in samples]
    
    # 3. å‰å‘ä¼ æ’­
    logits = policy_network(info_states)
    outputs = softmax(logits)
    
    # 4. è®¡ç®—æŸå¤±ï¼ˆåŠ æƒ MSEï¼‰
    loss = MSE(iterations * outputs, iterations * action_probs)
    
    # 5. åå‘ä¼ æ’­
    loss.backward()
    optimizer.step()
    
    return loss
```

---

## ä»£ç å®ç°è¯¦è§£

### 1. åˆå§‹åŒ– (`__init__`)

```python
def __init__(self, game, policy_network_layers, advantage_network_layers, ...):
    # æ¸¸æˆä¿¡æ¯
    self._game = game
    self._root_node = game.new_initial_state()
    self._embedding_size = len(root_node.information_state_tensor(0))
    self._num_actions = game.num_distinct_actions()
    
    # ç½‘ç»œåˆå§‹åŒ–
    self._policy_network = MLP(embedding_size, policy_layers, num_actions)
    self._advantage_networks = [
        MLP(embedding_size, advantage_layers, num_actions)
        for _ in range(num_players)
    ]
    
    # ç¼“å†²åŒºï¼ˆReservoir Samplingï¼‰
    self._strategy_memories = ReservoirBuffer(memory_capacity)
    self._advantage_memories = [
        ReservoirBuffer(memory_capacity) for _ in range(num_players)
    ]
    
    # ä¼˜åŒ–å™¨
    self._optimizer_policy = Adam(policy_network.parameters(), lr=learning_rate)
    self._optimizer_advantages = [
        Adam(advantage_networks[p].parameters(), lr=learning_rate)
        for p in range(num_players)
    ]
```

### 2. ç¼“å†²åŒº (Reservoir Buffer)

ä½¿ç”¨**æ°´åº“é‡‡æ · (Reservoir Sampling)** æ¥å‡åŒ€é‡‡æ ·ï¼š

```python
class ReservoirBuffer:
    def add(self, element):
        if len(self._data) < capacity:
            self._data.append(element)
        else:
            # éšæœºæ›¿æ¢
            idx = random.randint(0, self._add_calls)
            if idx < capacity:
                self._data[idx] = element
        self._add_calls += 1
```

**ä¼˜ç‚¹**ï¼š
- å†…å­˜å›ºå®šï¼ˆä¸ä¼šæ— é™å¢é•¿ï¼‰
- ä¿è¯å‡åŒ€é‡‡æ ·ï¼ˆæ¯ä¸ªæ ·æœ¬è¢«ä¿ç•™çš„æ¦‚ç‡ç›¸ç­‰ï¼‰

### 3. è®­ç»ƒè„šæœ¬ (`train_deep_cfr_texas.py`)

```python
def train_deep_cfr(...):
    # 1. åˆ›å»ºæ¸¸æˆ
    game = pyspiel.load_game("universal_poker(...)")
    
    # 2. åˆ›å»ºæ±‚è§£å™¨
    solver = DeepCFRSolver(
        game,
        policy_network_layers=(256, 256, 128),
        advantage_network_layers=(128, 128, 64),
        num_iterations=500,
        num_traversals=50,
        ...
    )
    
    # 3. è®­ç»ƒå¾ªç¯
    for iteration in range(num_iterations):
        for player in range(num_players):
            # éå†æ¸¸æˆæ ‘
            for _ in range(num_traversals):
                solver._traverse_game_tree(root, player)
            
            # æ›´æ–°ä¼˜åŠ¿ç½‘ç»œ
            loss = solver._learn_advantage_network(player)
        
        # æ›´æ–°ç­–ç•¥ç½‘ç»œ
        policy_loss = solver._learn_strategy_network()
```

---

## åœ¨å¾·å·æ‰‘å…‹ä¸­çš„åº”ç”¨

### 1. æ¸¸æˆé…ç½®

```python
game_string = (
    f"universal_poker("
    f"betting=nolimit,"
    f"numPlayers=6,"
    f"numRounds=4,"           # Preflop, Flop, Turn, River
    f"numBoardCards=0 3 1 1," # æ¯è½®å…¬å…±ç‰Œæ•°
    f"numHoleCards=2,"        # æ¯äºº2å¼ æ‰‹ç‰Œ
    f"stack=2000 2000 ...,"   # åˆå§‹ç­¹ç 
    f"blind=100 100 ..."      # ç›²æ³¨
    f")"
)
```

### 2. ä¿¡æ¯çŠ¶æ€ (Information State)

å¾·å·æ‰‘å…‹çš„ä¿¡æ¯çŠ¶æ€åŒ…å«ï¼š
- **ç©å®¶ä½ç½®**ï¼š6ä¸ªç©å®¶ï¼Œç”¨6ç»´ one-hot ç¼–ç 
- **æ‰‹ç‰Œ**ï¼š52å¼ ç‰Œï¼Œç”¨52ç»´å‘é‡ï¼ˆ1è¡¨ç¤ºæœ‰è¿™å¼ ç‰Œï¼‰
- **å…¬å…±ç‰Œ**ï¼š52ç»´å‘é‡ï¼ˆ1è¡¨ç¤ºå…¬å…±ç‰Œä¸­æœ‰è¿™å¼ ç‰Œï¼‰
- **ä¸‹æ³¨å†å²**ï¼šåŠ¨ä½œåºåˆ—ç¼–ç 
- **æŠ•å…¥é‡‘é¢**ï¼šæ¯ä¸ªç©å®¶çš„æŠ•å…¥

**æ€»å¤§å°**ï¼š266ç»´ï¼ˆ6äººåœºï¼‰

### 3. åŠ¨ä½œç©ºé—´

4ä¸ªåŠ¨ä½œï¼š
- `0`: Foldï¼ˆå¼ƒç‰Œï¼‰
- `1`: Call/Checkï¼ˆè·Ÿæ³¨/è¿‡ç‰Œï¼‰
- `2`: Bet/Raiseï¼ˆä¸‹æ³¨/åŠ æ³¨ï¼‰
- `3`: All-inï¼ˆå…¨æŠ¼ï¼‰

### 4. è®­ç»ƒå‚æ•°ï¼ˆ6äººåœºå¤§è§„æ¨¡è®­ç»ƒï¼‰

```python
num_iterations = 500      # è¿­ä»£æ¬¡æ•°
num_traversals = 50       # æ¯æ¬¡è¿­ä»£çš„éå†æ¬¡æ•°
policy_layers = (256, 256, 128)      # ç­–ç•¥ç½‘ç»œ
advantage_layers = (128, 128, 64)    # ä¼˜åŠ¿ç½‘ç»œ
learning_rate = 0.001
memory_capacity = 10,000,000
```

**è®¡ç®—é‡**ï¼š
- æ¯æ¬¡è¿­ä»£ï¼š6ä¸ªç©å®¶ Ã— 50æ¬¡éå† = 300æ¬¡æ¸¸æˆæ ‘éå†
- æ€»éå†æ¬¡æ•°ï¼š500 Ã— 300 = 150,000æ¬¡
- æ¯æ¬¡éå†å¯èƒ½æ¢ç´¢æ•°ç™¾åˆ°æ•°åƒä¸ªèŠ‚ç‚¹

### 5. è®­ç»ƒè¿‡ç¨‹ç¤ºä¾‹

```
è¿­ä»£ 1/500...
  éå†æ¸¸æˆæ ‘ï¼ˆç©å®¶0ï¼Œ50æ¬¡ï¼‰...
  éå†æ¸¸æˆæ ‘ï¼ˆç©å®¶1ï¼Œ50æ¬¡ï¼‰...
  ...
  æ›´æ–°ä¼˜åŠ¿ç½‘ç»œï¼ˆç©å®¶0ï¼‰... æŸå¤±: 3.66M
  æ›´æ–°ä¼˜åŠ¿ç½‘ç»œï¼ˆç©å®¶1ï¼‰... æŸå¤±: 2.14M
  ...
  æ›´æ–°ç­–ç•¥ç½‘ç»œ... æŸå¤±: 37.13

è¿­ä»£ 20/500...
  ç­–ç•¥ç†µ: 0.0000
  ç­–ç•¥ç¼“å†²åŒº: 85,722
  ä¼˜åŠ¿æ ·æœ¬: 10,917
  æµ‹è¯•å¯¹å±€: ç©å®¶0å¹³å‡æ”¶ç›Š=67.39, èƒœç‡=16.0%

...

è¿­ä»£ 500/500...
  ç­–ç•¥ç¼“å†²åŒº: 2,258,501
  ä¼˜åŠ¿æ ·æœ¬: 281,087
  æœ€ç»ˆæŸå¤±: 1,144.36M (ç©å®¶0)
```

### 6. ä¸ºä»€ä¹ˆæŸå¤±å€¼ä¼šå¢é•¿ï¼Ÿ

è¿™æ˜¯**æ­£å¸¸çš„**ï¼åŸå› ï¼š
1. éšç€è®­ç»ƒæ·±å…¥ï¼Œæ¢ç´¢çš„æ¸¸æˆæ ‘æ›´æ·±
2. ä¼˜åŠ¿å€¼çš„èŒƒå›´æ‰©å¤§ï¼ˆåæ‚”å€¼å¯èƒ½å¾ˆå¤§ï¼‰
3. ç½‘ç»œéœ€è¦å­¦ä¹ æ›´å¤§çš„æ•°å€¼èŒƒå›´

**å…³é”®æŒ‡æ ‡**ï¼š
- âœ… ç¼“å†²åŒºæŒç»­å¢é•¿ï¼ˆè¯´æ˜åœ¨æ¢ç´¢ï¼‰
- âœ… æ‰€æœ‰ç©å®¶éƒ½åœ¨è®­ç»ƒï¼ˆæŸå¤±éƒ½åœ¨å¢é•¿ï¼‰
- âš ï¸ ç­–ç•¥ç†µä¸º0ï¼ˆå¯èƒ½ç­–ç•¥è¿‡äºç¡®å®šï¼‰

---

## å…³é”®æ¦‚å¿µæ€»ç»“

### 1. åæ‚”å€¼ (Regret)

```
åæ‚”å€¼ = é€‰æ‹©åŠ¨ä½œaçš„æ”¶ç›Š - å½“å‰ç­–ç•¥çš„æœŸæœ›æ”¶ç›Š
```

å¦‚æœåæ‚”å€¼ä¸ºæ­£ï¼Œè¯´æ˜è¿™ä¸ªåŠ¨ä½œæ¯”å½“å‰ç­–ç•¥å¥½ã€‚

### 2. åæ‚”å€¼åŒ¹é… (Regret Matching)

```
ç­–ç•¥æ¦‚ç‡ = æ­£åæ‚”å€¼ / æ‰€æœ‰æ­£åæ‚”å€¼çš„å’Œ
```

åªè€ƒè™‘æ­£åæ‚”å€¼ï¼Œè´Ÿåæ‚”å€¼è®¾ä¸º0ã€‚

### 3. å¹³å‡ç­–ç•¥ (Average Strategy)

```
å¹³å‡ç­–ç•¥ = (1/T) * Î£_t ç­–ç•¥^t
```

æ‰€æœ‰è¿­ä»£çš„ç­–ç•¥çš„å¹³å‡å€¼ï¼Œæ”¶æ•›åˆ°çº³ä»€å‡è¡¡ã€‚

### 4. ä¿¡æ¯é›† (Information Set)

åœ¨å¾·å·æ‰‘å…‹ä¸­ï¼Œä¿¡æ¯é›†æ˜¯ç©å®¶èƒ½çœ‹åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼š
- è‡ªå·±çš„æ‰‹ç‰Œ
- å…¬å…±ç‰Œ
- ä¸‹æ³¨å†å²
- å…¶ä»–ç©å®¶çš„æŠ•å…¥

**å…³é”®**ï¼šç›¸åŒä¿¡æ¯é›†çš„çŠ¶æ€ï¼Œç©å®¶åº”è¯¥é‡‡ç”¨ç›¸åŒç­–ç•¥ã€‚

---

## è®­ç»ƒæŠ€å·§

### 1. ä¼˜åŠ¿ç½‘ç»œé‡æ–°åˆå§‹åŒ–

```python
if reinitialize_advantage_networks:
    advantage_networks[player].reset()
```

**åŸå› **ï¼šæ¯è½®é‡æ–°å­¦ä¹ ï¼Œé¿å…è¿‡æ‹Ÿåˆæ—©æœŸæ ·æœ¬ã€‚

### 2. è¿­ä»£åŠ æƒ

```python
weight = sqrt(iteration)
loss = MSE(weight * prediction, weight * target)
```

**åŸå› **ï¼šæ—©æœŸè¿­ä»£çš„æ ·æœ¬è´¨é‡è¾ƒä½ï¼Œæƒé‡è¾ƒå°ã€‚

### 3. æ°´åº“é‡‡æ ·

ä½¿ç”¨å›ºå®šå¤§å°çš„ç¼“å†²åŒºï¼Œä¿è¯å‡åŒ€é‡‡æ ·ï¼Œé¿å…å†…å­˜çˆ†ç‚¸ã€‚

### 4. å¤–éƒ¨é‡‡æ · (External Sampling)

åœ¨éå†æ—¶ï¼Œå¯¹å…¶ä»–ç©å®¶ä½¿ç”¨ç­–ç•¥é‡‡æ ·ï¼Œè€Œä¸æ˜¯éå†æ‰€æœ‰åŠ¨ä½œï¼Œå¤§å¤§å‡å°‘è®¡ç®—é‡ã€‚

---

## è¯„ä¼°æŒ‡æ ‡

### 1. ç­–ç•¥ç†µ

è¡¡é‡ç­–ç•¥çš„éšæœºæ€§ï¼š
```
ç†µ = -Î£ p(a) * log(p(a))
```

- ç†µ=0ï¼šç­–ç•¥å®Œå…¨ç¡®å®šï¼ˆæ€»æ˜¯é€‰æ‹©åŒä¸€ä¸ªåŠ¨ä½œï¼‰
- ç†µå¤§ï¼šç­–ç•¥éšæœºæ€§å¼º

### 2. ç¼“å†²åŒºå¤§å°

- ç­–ç•¥ç¼“å†²åŒºï¼šå·²æ¢ç´¢çš„ä¿¡æ¯é›†æ•°é‡
- ä¼˜åŠ¿ç¼“å†²åŒºï¼šå·²æ”¶é›†çš„ä¼˜åŠ¿æ ·æœ¬æ•°é‡

### 3. æµ‹è¯•å¯¹å±€

ä¸éšæœºç­–ç•¥å¯¹å±€ï¼Œç»Ÿè®¡ï¼š
- å¹³å‡æ”¶ç›Š
- èƒœç‡

---

## å‚è€ƒèµ„æ–™

- DeepCFR è®ºæ–‡ï¼šhttps://arxiv.org/abs/1811.00164
- OpenSpiel æ–‡æ¡£ï¼šhttps://github.com/deepmind/open_spiel
- CFR ç®—æ³•ï¼šhttps://en.wikipedia.org/wiki/Counterfactual_regret_minimization

---

## æ€»ç»“

DeepCFR é€šè¿‡ç¥ç»ç½‘ç»œè¿‘ä¼¼ CFR ç®—æ³•ï¼Œä½¿å¾—å¯ä»¥å¤„ç†åƒå¾·å·æ‰‘å…‹è¿™æ ·çš„å¤§è§„æ¨¡ä¸å®Œå…¨ä¿¡æ¯åšå¼ˆï¼š

1. **ä¼˜åŠ¿ç½‘ç»œ**ï¼šå­¦ä¹ æ¯ä¸ªç©å®¶çš„åæ‚”å€¼ï¼Œç”¨äºç­–ç•¥é€‰æ‹©
2. **ç­–ç•¥ç½‘ç»œ**ï¼šå­¦ä¹ å¹³å‡ç­–ç•¥ï¼Œç”¨äºæœ€ç»ˆå†³ç­–
3. **æ¸¸æˆæ ‘éå†**ï¼šæ”¶é›†è®­ç»ƒæ ·æœ¬
4. **è¿­ä»£è®­ç»ƒ**ï¼šé€æ­¥æ”¹è¿›ç­–ç•¥ï¼Œæ”¶æ•›åˆ°çº³ä»€å‡è¡¡

åœ¨å¾·å·æ‰‘å…‹ä¸­ï¼ŒDeepCFR å¯ä»¥å­¦ä¹ åˆ°æ¥è¿‘æœ€ä¼˜çš„ç­–ç•¥ï¼Œå³ä½¿æ¸¸æˆçŠ¶æ€ç©ºé—´å·¨å¤§ï¼ˆ10^18+ ä¿¡æ¯é›†ï¼‰ã€‚


