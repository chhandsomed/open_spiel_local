#!/usr/bin/env python3
"""æ‰¹é‡è¯„ä¼°æ‰€æœ‰ checkpointï¼Œæ‰¾å‡ºæœ€ä½³æ¨¡å‹

ä½¿ç”¨æ–¹æ³•:
    python evaluate_all_checkpoints.py --model_dir models/deepcfr_parallel_6p --num_games 500 --use_gpu
"""

import os
import sys
import argparse
import json
import glob
import re
import subprocess
from pathlib import Path

def find_all_checkpoints(model_dir):
    """æŸ¥æ‰¾æ‰€æœ‰ checkpoint ç›®å½•"""
    checkpoints = []
    
    checkpoint_root = os.path.join(model_dir, "checkpoints")
    if not os.path.exists(checkpoint_root):
        print(f"  âœ— æœªæ‰¾åˆ° checkpoints ç›®å½•: {checkpoint_root}")
        return checkpoints
    
    # æŸ¥æ‰¾æ‰€æœ‰ iter_* ç›®å½•
    iter_dirs = glob.glob(os.path.join(checkpoint_root, "iter_*"))
    for d in iter_dirs:
        match = re.search(r'iter_(\d+)$', d)
        if match:
            iter_num = int(match.group(1))
            # æ£€æŸ¥æ˜¯å¦æœ‰ç­–ç•¥ç½‘ç»œæ–‡ä»¶
            policy_files = glob.glob(os.path.join(d, "*_policy_network_iter*.pt"))
            if policy_files:
                checkpoints.append({
                    'iter': iter_num,
                    'dir': d,
                    'path': d
                })
    
    # æŒ‰è¿­ä»£å·æ’åº
    checkpoints.sort(key=lambda x: x['iter'])
    return checkpoints


def evaluate_checkpoint(checkpoint_path, num_games=500, use_gpu=True):
    """è¯„ä¼°å•ä¸ª checkpoint"""
    print(f"\nè¯„ä¼° checkpoint: {os.path.basename(checkpoint_path)}")
    
    # è°ƒç”¨ inference_simple.py è¿›è¡Œè¯„ä¼°
    cmd = [
        sys.executable, "inference_simple.py",
        "--model_dir", checkpoint_path,
        "--num_games", str(num_games),
    ]
    if use_gpu:
        cmd.append("--use_gpu")
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=3600  # 1å°æ—¶è¶…æ—¶
        )
        
        if result.returncode != 0:
            print(f"  âœ— è¯„ä¼°å¤±è´¥: {result.stderr[:200]}")
            return None
        
        # è§£æè¾“å‡ºï¼Œæå–å…³é”®æŒ‡æ ‡
        output = result.stdout
        metrics = {}
        
        # æå–å¹³å‡æ”¶ç›Šå’Œèƒœç‡
        # æ ¼å¼: "ç©å®¶ 0: å¹³å‡æ”¶ç›Š: X.XXXX èƒœç‡: XX.X%"
        for line in output.split('\n'):
            if 'å¹³å‡æ”¶ç›Š:' in line and 'ç©å®¶ 0' in line:
                # æå–å¹³å‡æ”¶ç›Š
                match = re.search(r'å¹³å‡æ”¶ç›Š:\s*([-\d.]+)', line)
                if match:
                    metrics['player0_avg_return'] = float(match.group(1))
            
            if 'èƒœç‡:' in line and 'ç©å®¶ 0' in line:
                # æå–èƒœç‡
                match = re.search(r'èƒœç‡:\s*([\d.]+)%', line)
                if match:
                    metrics['player0_win_rate'] = float(match.group(1))
        
        # æå–æ‰€æœ‰ç©å®¶çš„æ”¶ç›Šï¼ˆç”¨äºè®¡ç®—æ€»ä½“è¡¨ç°ï¼‰
        player_returns = []
        for i in range(6):  # 6äººå±€
            pattern = f'ç©å®¶ {i}:.*?å¹³å‡æ”¶ç›Š:\s*([-\d.]+)'
            match = re.search(pattern, output)
            if match:
                player_returns.append(float(match.group(1)))
        
        if player_returns:
            metrics['all_players_returns'] = player_returns
            metrics['avg_return_all'] = sum(player_returns) / len(player_returns)
            metrics['max_return'] = max(player_returns)
            metrics['min_return'] = min(player_returns)
            # è®¡ç®—æ”¶ç›Šæ–¹å·®ï¼ˆè¶Šå°è¶Šå¥½ï¼Œè¯´æ˜ç­–ç•¥æ›´å¹³è¡¡ï¼‰
            metrics['return_variance'] = sum((r - metrics['avg_return_all'])**2 for r in player_returns) / len(player_returns)
        
        return metrics
        
    except subprocess.TimeoutExpired:
        print(f"  âœ— è¯„ä¼°è¶…æ—¶")
        return None
    except Exception as e:
        print(f"  âœ— è¯„ä¼°å‡ºé”™: {e}")
        return None


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡è¯„ä¼°æ‰€æœ‰ checkpoint")
    parser.add_argument("--model_dir", type=str, required=True,
                       help="æ¨¡å‹ç›®å½•ï¼ˆä¾‹å¦‚: models/deepcfr_parallel_6pï¼‰")
    parser.add_argument("--num_games", type=int, default=500,
                       help="æ¯ä¸ª checkpoint çš„æµ‹è¯•å±€æ•°ï¼ˆé»˜è®¤: 500ï¼‰")
    parser.add_argument("--use_gpu", action="store_true", default=True,
                       help="ä½¿ç”¨ GPU")
    parser.add_argument("--top_k", type=int, default=5,
                       help="æ˜¾ç¤ºå‰ K ä¸ªæœ€ä½³æ¨¡å‹ï¼ˆé»˜è®¤: 5ï¼‰")
    parser.add_argument("--output", type=str, default=None,
                       help="ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶")
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("æ‰¹é‡è¯„ä¼°æ‰€æœ‰ Checkpoint")
    print("=" * 70)
    print(f"æ¨¡å‹ç›®å½•: {args.model_dir}")
    print(f"æµ‹è¯•å±€æ•°: {args.num_games} å±€/checkpoint")
    print(f"ä½¿ç”¨ GPU: {args.use_gpu}")
    
    # æŸ¥æ‰¾æ‰€æœ‰ checkpoint
    print(f"\n[1/3] æŸ¥æ‰¾æ‰€æœ‰ checkpoint...")
    checkpoints = find_all_checkpoints(args.model_dir)
    
    if not checkpoints:
        print("  âœ— æœªæ‰¾åˆ°ä»»ä½• checkpoint")
        return
    
    print(f"  âœ“ æ‰¾åˆ° {len(checkpoints)} ä¸ª checkpoint")
    print(f"  è¿­ä»£èŒƒå›´: {checkpoints[0]['iter']} - {checkpoints[-1]['iter']}")
    
    # è¯„ä¼°æ¯ä¸ª checkpoint
    print(f"\n[2/3] è¯„ä¼° checkpointï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
    results = []
    
    for i, ckpt in enumerate(checkpoints):
        print(f"\nè¿›åº¦: {i+1}/{len(checkpoints)}")
        metrics = evaluate_checkpoint(ckpt['path'], args.num_games, args.use_gpu)
        
        if metrics:
            result = {
                'iter': ckpt['iter'],
                'path': ckpt['path'],
                **metrics
            }
            results.append(result)
            print(f"  âœ“ è¿­ä»£ {ckpt['iter']}: ç©å®¶0å¹³å‡æ”¶ç›Š={metrics.get('player0_avg_return', 'N/A'):.2f}, "
                  f"èƒœç‡={metrics.get('player0_win_rate', 'N/A'):.1f}%")
        else:
            print(f"  âœ— è¿­ä»£ {ckpt['iter']}: è¯„ä¼°å¤±è´¥")
    
    if not results:
        print("\n  âœ— æ‰€æœ‰ checkpoint è¯„ä¼°å¤±è´¥")
        return
    
    # æ’åºå’Œæ˜¾ç¤ºç»“æœ
    print(f"\n[3/3] åˆ†æç»“æœ...")
    
    # æŒ‰ç©å®¶0å¹³å‡æ”¶ç›Šæ’åºï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
    results_sorted = sorted(results, key=lambda x: x.get('player0_avg_return', -999), reverse=True)
    
    print("\n" + "=" * 70)
    print(f"è¯„ä¼°ç»“æœï¼ˆæŒ‰ç©å®¶0å¹³å‡æ”¶ç›Šæ’åºï¼Œå‰ {args.top_k} åï¼‰")
    print("=" * 70)
    print(f"{'æ’å':<6} {'è¿­ä»£':<8} {'ç©å®¶0æ”¶ç›Š':<12} {'ç©å®¶0èƒœç‡':<12} {'æ”¶ç›Šæ–¹å·®':<12} {'è·¯å¾„'}")
    print("-" * 70)
    
    for i, r in enumerate(results_sorted[:args.top_k], 1):
        iter_num = r['iter']
        avg_return = r.get('player0_avg_return', 0)
        win_rate = r.get('player0_win_rate', 0)
        variance = r.get('return_variance', 0)
        path = os.path.basename(r['path'])
        
        print(f"{i:<6} {iter_num:<8} {avg_return:>10.2f}    {win_rate:>9.1f}%    {variance:>10.2f}    {path}")
    
    # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹
    if results_sorted:
        best = results_sorted[0]
        print("\n" + "=" * 70)
        print("ğŸ† æœ€ä½³æ¨¡å‹")
        print("=" * 70)
        print(f"è¿­ä»£: {best['iter']}")
        print(f"è·¯å¾„: {best['path']}")
        print(f"ç©å®¶0å¹³å‡æ”¶ç›Š: {best.get('player0_avg_return', 0):.2f}")
        print(f"ç©å®¶0èƒœç‡: {best.get('player0_win_rate', 0):.1f}%")
        if 'all_players_returns' in best:
            print(f"æ‰€æœ‰ç©å®¶æ”¶ç›Š: {[f'{r:.2f}' for r in best['all_players_returns']]}")
            print(f"æ”¶ç›Šæ–¹å·®: {best.get('return_variance', 0):.2f} (è¶Šå°è¶Šå¥½ï¼Œè¯´æ˜ç­–ç•¥æ›´å¹³è¡¡)")
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = args.output
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    print("\n" + "=" * 70)
    print("è¯„ä¼°å®Œæˆ")
    print("=" * 70)
    print("\nä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæ¨ç†:")
    if results_sorted:
        best_path = results_sorted[0]['path']
        print(f"  python inference_simple.py --model_dir {best_path} --num_games 1000 --use_gpu")


if __name__ == "__main__":
    main()

